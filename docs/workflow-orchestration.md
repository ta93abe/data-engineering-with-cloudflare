# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œå…¨ã‚¬ã‚¤ãƒ‰

Cloudflareãƒ‡ãƒ¼ã‚¿åŸºç›¤ã«ãŠã‘ã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®é¸æŠè‚¢ã€æ¯”è¼ƒã€å®Ÿè£…ã‚¬ã‚¤ãƒ‰ã€‚

## ğŸ“‹ ç›®æ¬¡

1. [ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é¸æŠè‚¢ã®å…¨ä½“åƒ](#ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é¸æŠè‚¢ã®å…¨ä½“åƒ)
2. [Cloudflareãƒã‚¤ãƒ†ã‚£ãƒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ](#cloudflareãƒã‚¤ãƒ†ã‚£ãƒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ)
3. [OSS ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ„ãƒ¼ãƒ«æ¯”è¼ƒ](#ossã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ„ãƒ¼ãƒ«æ¯”è¼ƒ)
4. [æ¨å¥¨æ§‹æˆã¨ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹åˆ¥é¸æŠ](#æ¨å¥¨æ§‹æˆã¨ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹åˆ¥é¸æŠ)
5. [å®Ÿè£…ä¾‹](#å®Ÿè£…ä¾‹)

---

## ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é¸æŠè‚¢ã®å…¨ä½“åƒ

### æ¯”è¼ƒãƒãƒˆãƒªã‚¯ã‚¹

| ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ | è¤‡é›‘åº¦ | ã‚³ã‚¹ãƒˆ | Cloudflareçµ±åˆ | ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ | ãŠã™ã™ã‚åº¦ |
|-----------|--------|--------|----------------|-----------------|-----------|
| **Cloudflare Pipelines** | ä½ | $ | âœ… ãƒã‚¤ãƒ†ã‚£ãƒ– | é«˜ | â­â­â­â­â­ ã‚¤ãƒ™ãƒ³ãƒˆETL |
| **Workers Cron + Queues** | ä½ | $ | âœ… ãƒã‚¤ãƒ†ã‚£ãƒ– | é«˜ | â­â­â­â­â­ ãƒãƒƒãƒå‡¦ç† |
| **Cloudflare Workflows (Beta)** | ä½-ä¸­ | $ | âœ… ãƒã‚¤ãƒ†ã‚£ãƒ– | é«˜ | â­â­â­â­ ä»Šå¾Œæœ‰æœ› |
| **Prefect (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ)** | ä¸­ | $ | ğŸŸ¡ å¯èƒ½ | é«˜ | â­â­â­â­â­ ãƒãƒ©ãƒ³ã‚¹è‰¯ |
| **Dagster (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ)** | ä¸­-é«˜ | $$ | ğŸŸ¡ å¯èƒ½ | é«˜ | â­â­â­â­ ãƒ‡ãƒ¼ã‚¿é‡è¦– |
| **Apache Airflow (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ)** | é«˜ | $$ | ğŸŸ¡ å¯èƒ½ | é«˜ | â­â­â­ å®Ÿç¸¾è±Šå¯Œ |
| **Temporal (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ)** | é«˜ | $$ | ğŸŸ¡ å¯èƒ½ | éå¸¸ã«é«˜ | â­â­â­â­ ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ |
| **Kestra (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ)** | ä½-ä¸­ | $ | ğŸŸ¡ å¯èƒ½ | ä¸­-é«˜ | â­â­â­â­ YAMLå¿—å‘ |

---

## Cloudflareãƒã‚¤ãƒ†ã‚£ãƒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

### 1. Cloudflare Pipelines ğŸ†•ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ»ETLç‰¹åŒ–ï¼‰

**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¤ãƒ™ãƒ³ãƒˆå–ã‚Šè¾¼ã¿ã¨ETLå‡¦ç†ã«æœ€é©ã€‚**

#### æ¦‚è¦

Cloudflare Pipelinesï¼ˆ2025å¹´4æœˆãƒ™ãƒ¼ã‚¿ãƒªãƒªãƒ¼ã‚¹ï¼‰ã¯ã€Cloudflare Data Platformã®ä¸­æ ¸ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã€ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®å–ã‚Šè¾¼ã¿ã€å¤‰æ›ã€ä¿å­˜ã‚’çµ±åˆçš„ã«å‡¦ç†ã—ã¾ã™ã€‚

- **R2 Data Catalog**: Apache Icebergå½¢å¼ã®ãƒ†ãƒ¼ãƒ–ãƒ«ç®¡ç†
- **R2 SQL**: DuckDBãƒ™ãƒ¼ã‚¹ã®ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³
- **Pipelines**: ã‚¤ãƒ™ãƒ³ãƒˆåé›†ãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¤‰æ›

#### ç‰¹å¾´

- âœ… **é«˜é€Ÿå–ã‚Šè¾¼ã¿**: 1ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚ãŸã‚Š100 MB/ç§’
- âœ… **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ SQLå¤‰æ›**: ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãã®å ´ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ»å¤‰æ›ãƒ»ã‚¨ãƒ³ãƒªãƒƒãƒ
- âœ… **Apache Icebergå¯¾å¿œ**: ã‚ªãƒ¼ãƒ—ãƒ³ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ãƒ™ãƒ³ãƒ€ãƒ¼ãƒ­ãƒƒã‚¯ã‚¤ãƒ³å›é¿
- âœ… **Workersçµ±åˆ**: Workersã‹ã‚‰ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãŒãƒã‚¤ãƒ†ã‚£ãƒ–
- âœ… **ã‚ªãƒ¼ãƒ—ãƒ³ãƒ™ãƒ¼ã‚¿**: ç¾åœ¨ã¯ç„¡æ–™ï¼ˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ»ã‚¯ã‚¨ãƒªã¯é€šå¸¸æ–™é‡‘ï¼‰

#### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Cloudflare Workers / HTTP API                       â”‚
â”‚          - ã‚¯ãƒªãƒƒã‚¯ã‚¹ãƒˆãƒªãƒ¼ãƒ                                   â”‚
â”‚          - ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°                                â”‚
â”‚          - ãƒ¡ãƒˆãƒªã‚¯ã‚¹                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Cloudflare Pipelines                            â”‚
â”‚  - SQLãƒ™ãƒ¼ã‚¹ã®å¤‰æ›ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ»ã‚¨ãƒ³ãƒªãƒƒãƒï¼‰                      â”‚
â”‚  - è‡ªå‹•ãƒãƒƒãƒãƒ³ã‚°                                             â”‚
â”‚  - é‡è¤‡æ’é™¤ãƒ»é›†ç´„                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚Apache Icebergâ”‚      â”‚  R2 Files    â”‚
        â”‚  Tables      â”‚      â”‚  (Parquet)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   R2 SQL     â”‚
                   â”‚  (ã‚¯ã‚¨ãƒª)     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ä½¿ã„æ‰€

| ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ | èª¬æ˜ |
|-------------|------|
| **ã‚¯ãƒªãƒƒã‚¯ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿** | ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆãƒ»ã‚¢ãƒ—ãƒªã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆåé›† |
| **ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°** | Workers/APIã‹ã‚‰ã®ãƒ­ã‚°é›†ç´„ãƒ»æ¤œç´¢ |
| **ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†** | ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å–ã‚Šè¾¼ã¿ |
| **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ETL** | ã‚½ãƒ¼ã‚¹ã‹ã‚‰R2ã¸ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¤‰æ› |
| **ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ã‚¯å–ã‚Šè¾¼ã¿** | Apache Icebergå½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ã‚¯æ§‹ç¯‰ |
| **CDNã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹** | Cloudflare CDNçµŒç”±ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†æ |

#### å®Ÿè£…ä¾‹

```javascript
// workers/events/clickstream-ingestion.js

/**
 * ã‚¯ãƒªãƒƒã‚¯ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¤ãƒ™ãƒ³ãƒˆã‚’Pipelinesã«é€ä¿¡
 */
export default {
  async fetch(request, env) {
    const event = await request.json();

    // Pipelinesã«ã‚¤ãƒ™ãƒ³ãƒˆé€ä¿¡
    await fetch(`https://api.cloudflare.com/pipelines/v1/accounts/${env.ACCOUNT_ID}/pipelines/${env.PIPELINE_ID}/events`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${env.CF_API_TOKEN}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        events: [{
          timestamp: new Date().toISOString(),
          user_id: event.userId,
          page: event.page,
          action: event.action,
          properties: event.properties
        }]
      })
    });

    return new Response('Event tracked', { status: 200 });
  }
};
```

```sql
-- Pipelines SQLå¤‰æ›è¨­å®šï¼ˆä¾‹ï¼‰

-- ç”Ÿã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒ»ã‚¨ãƒ³ãƒªãƒƒãƒã—ã¦Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
SELECT
  timestamp,
  user_id,
  page,
  action,
  CASE
    WHEN action = 'purchase' THEN 'conversion'
    WHEN action = 'add_to_cart' THEN 'engagement'
    ELSE 'other'
  END as event_category,
  properties
FROM events
WHERE action IS NOT NULL
  AND user_id IS NOT NULL
```

#### R2 SQLã§ã®ã‚¯ã‚¨ãƒª

```sql
-- Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰åˆ†æã‚¯ã‚¨ãƒª
SELECT
  DATE_TRUNC('hour', timestamp) as hour,
  event_category,
  COUNT(*) as event_count,
  COUNT(DISTINCT user_id) as unique_users
FROM clickstream_events
WHERE timestamp >= NOW() - INTERVAL '24' HOUR
GROUP BY hour, event_category
ORDER BY hour DESC, event_count DESC
```

#### ã„ã¤ä½¿ã†ã¹ãã‹

- âœ… **ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°**: é«˜é »åº¦ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆã‚¯ãƒªãƒƒã‚¯ã€ãƒ­ã‚°ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
- âœ… **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¤‰æ›**: SQLã§ãã®å ´ã§å‡¦ç†ã§ãã‚‹å˜ç´”ãªå¤‰æ›
- âœ… **Apache Iceberg**: ã‚ªãƒ¼ãƒ—ãƒ³ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§é•·æœŸä¿å­˜ãƒ»åˆ†æ
- âŒ **è¤‡é›‘ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**: è¤‡é›‘ãªDAGä¾å­˜é–¢ä¿‚ãŒã‚ã‚‹å ´åˆ
- âŒ **ãƒãƒƒãƒå‡¦ç†**: å®šæœŸçš„ãªå¤§è¦æ¨¡ãƒãƒƒãƒå‡¦ç†ï¼ˆâ†’ Workers Cron + Queuesï¼‰

---

### 2. Workers Cron + Queuesï¼ˆãƒãƒƒãƒå‡¦ç†æ¨å¥¨ğŸ†ï¼‰

**å®šæœŸãƒãƒƒãƒå‡¦ç†ã«æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã§è²»ç”¨å¯¾åŠ¹æœã®é«˜ã„é¸æŠè‚¢ã€‚**

#### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Workers Cron (ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼)                    â”‚
â”‚  - cron: "0 */6 * * *"                                      â”‚
â”‚  - ãƒˆãƒªã‚¬ãƒ¼: ã‚¿ã‚¹ã‚¯ã‚’Queuesã«é€ä¿¡                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Cloudflare Queues                               â”‚
â”‚  - ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿Queue                                        â”‚
â”‚  - å¤‰æ›å‡¦ç†Queue                                             â”‚
â”‚  - é€šçŸ¥Queue                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼          â–¼          â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Worker 1 â”‚ â”‚ Worker 2 â”‚ â”‚ Worker 3 â”‚
        â”‚ (Ingest) â”‚ â”‚(Transform)â”‚ â”‚ (Notify) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### å®Ÿè£…ä¾‹

```javascript
// workers/orchestrator/cron-scheduler.js

/**
 * Cronã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ - DAGå®šç¾©ã¨ã‚¿ã‚¹ã‚¯é€ä¿¡
 */
export default {
  async scheduled(event, env, ctx) {
    const cronType = event.cron;  // "0 */6 * * *"

    // DAGå®šç¾©
    const dags = {
      'data-ingestion-dag': {
        schedule: '0 */6 * * *',
        tasks: [
          { name: 'fetch-api-data', queue: 'ingestion', params: { source: 'api' } },
          { name: 'validate-data', queue: 'validation', params: {} },
          { name: 'write-to-r2', queue: 'storage', params: { layer: 'bronze' } }
        ]
      },
      'transformation-dag': {
        schedule: '30 */6 * * *',
        tasks: [
          { name: 'dbt-run', queue: 'transformation', params: { models: 'staging' } },
          { name: 'dbt-test', queue: 'testing', params: {} }
        ]
      }
    };

    // è©²å½“ã™ã‚‹DAGã‚’å®Ÿè¡Œ
    for (const [dagName, dag] of Object.entries(dags)) {
      if (dag.schedule === cronType) {
        ctx.waitUntil(executeDag(env, dagName, dag));
      }
    }
  }
};

async function executeDag(env, dagName, dag) {
  const runId = crypto.randomUUID();
  console.log(`Starting DAG: ${dagName}, Run ID: ${runId}`);

  for (const task of dag.tasks) {
    try {
      // ã‚¿ã‚¹ã‚¯ã‚’Queueã«é€ä¿¡
      await env.TASK_QUEUE.send({
        runId,
        dagName,
        taskName: task.name,
        params: task.params,
        timestamp: new Date().toISOString()
      });

      console.log(`Task queued: ${task.name}`);

    } catch (error) {
      console.error(`Failed to queue task ${task.name}:`, error);

      // å¤±æ•—ã‚’D1ã«è¨˜éŒ²
      await env.DB.prepare(`
        INSERT INTO task_failures (run_id, dag_name, task_name, error, created_at)
        VALUES (?, ?, ?, ?, ?)
      `).bind(runId, dagName, task.name, error.message, new Date().toISOString()).run();
    }
  }
}
```

```javascript
// workers/tasks/ingestion-worker.js

/**
 * Queueã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒãƒ¼ - ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ã‚¿ã‚¹ã‚¯
 */
export default {
  async queue(batch, env) {
    for (const message of batch.messages) {
      const { runId, taskName, params } = message.body;

      try {
        await executeTask(taskName, params, env);

        // æˆåŠŸã‚’D1ã«è¨˜éŒ²
        await env.DB.prepare(`
          INSERT INTO task_runs (run_id, task_name, status, completed_at)
          VALUES (?, ?, ?, ?)
        `).bind(runId, taskName, 'success', new Date().toISOString()).run();

        message.ack();

      } catch (error) {
        console.error(`Task failed: ${taskName}`, error);

        // ãƒªãƒˆãƒ©ã‚¤ã¾ãŸã¯å¤±æ•—è¨˜éŒ²
        if (message.attempts < 3) {
          message.retry();
        } else {
          await logTaskFailure(env, runId, taskName, error);
          message.ack();
        }
      }
    }
  }
};

async function executeTask(taskName, params, env) {
  switch (taskName) {
    case 'fetch-api-data':
      return await fetchAPIData(params, env);

    case 'validate-data':
      return await validateData(params, env);

    case 'write-to-r2':
      return await writeToR2(params, env);

    default:
      throw new Error(`Unknown task: ${taskName}`);
  }
}
```

#### wranglerè¨­å®š

```toml
# wrangler-orchestrator.toml

name = "orchestrator"
main = "workers/orchestrator/cron-scheduler.js"
compatibility_date = "2024-01-01"

# Cronãƒˆãƒªã‚¬ãƒ¼
[triggers]
crons = [
  "0 */6 * * *",   # ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿
  "30 */6 * * *"   # å¤‰æ›å‡¦ç†
]

# Queues binding
[[queues.producers]]
queue = "task-queue"
binding = "TASK_QUEUE"

# D1 binding
[[d1_databases]]
binding = "DB"
database_name = "orchestration-db"
database_id = "your-d1-database-id"
```

```toml
# wrangler-task-worker.toml

name = "ingestion-worker"
main = "workers/tasks/ingestion-worker.js"
compatibility_date = "2024-01-01"

# Queues consumer
[[queues.consumers]]
queue = "task-queue"
max_batch_size = 10
max_batch_timeout = 30
```

---

### 3. Cloudflare Workflows (Beta) ğŸ†•

**2024å¹´å¾ŒåŠã«ãƒ™ãƒ¼ã‚¿ãƒªãƒªãƒ¼ã‚¹äºˆå®šã®æ–°æ©Ÿèƒ½ã€‚**

#### ç‰¹å¾´

- âœ… ã‚¹ãƒ†ãƒ¼ãƒˆãƒ•ãƒ«ã€é•·æ™‚é–“å®Ÿè¡Œãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å¯¾å¿œ
- âœ… è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- âœ… ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¨ãƒ‡ã‚£ã‚¿ï¼ˆäºˆå®šï¼‰
- âœ… Durable Objects ãƒ™ãƒ¼ã‚¹

#### æƒ³å®šã•ã‚Œã‚‹ä½¿ç”¨ä¾‹

```javascript
// workers/workflows/data-pipeline.js (ä»®æƒ³ä¾‹)

import { WorkflowEntrypoint, WorkflowStep } from 'cloudflare:workflows';

export class DataPipeline extends WorkflowEntrypoint {
  async run(event, step) {
    // ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿
    const rawData = await step.do('fetch-data', async () => {
      return await fetchFromAPI(event.source);
    });

    // ã‚¹ãƒ†ãƒƒãƒ—2: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    const validatedData = await step.do('validate', async () => {
      return await validateData(rawData);
    });

    // ã‚¹ãƒ†ãƒƒãƒ—3: å¤‰æ›
    const transformedData = await step.do('transform', async () => {
      return await transformData(validatedData);
    });

    // ã‚¹ãƒ†ãƒƒãƒ—4: R2ä¿å­˜
    await step.do('save-to-r2', async () => {
      await saveToR2(transformedData);
    });

    // ã‚¹ãƒ†ãƒƒãƒ—5: é€šçŸ¥
    await step.do('notify', async () => {
      await sendSlackNotification('Data pipeline completed');
    });

    return { success: true, recordsProcessed: transformedData.length };
  }
}
```

**æ³¨æ„:** Cloudflare Workflowsã¯ã¾ã ãƒ™ãƒ¼ã‚¿ç‰ˆã®ãŸã‚ã€æœ¬ç•ªåˆ©ç”¨ã¯æ…é‡ã«ã€‚

---

### 4. Durable Objects ã§ã‚¹ãƒ†ãƒ¼ãƒˆãƒ•ãƒ«ç®¡ç†

è¤‡é›‘ãªä¾å­˜é–¢ä¿‚ãŒã‚ã‚‹å ´åˆã®é¸æŠè‚¢ã€‚

```javascript
// workers/orchestrator/workflow-coordinator.js

export class WorkflowCoordinator {
  constructor(state, env) {
    this.state = state;
    this.env = env;
  }

  async fetch(request) {
    const url = new URL(request.url);

    if (url.pathname === '/start') {
      return await this.startWorkflow(await request.json());
    }

    if (url.pathname === '/status') {
      return await this.getStatus();
    }

    return new Response('Not Found', { status: 404 });
  }

  async startWorkflow(config) {
    const workflowState = {
      id: crypto.randomUUID(),
      status: 'running',
      steps: config.steps.map(s => ({ name: s, status: 'pending' })),
      startedAt: new Date().toISOString()
    };

    await this.state.storage.put('workflow', workflowState);

    // ã‚¹ãƒ†ãƒƒãƒ—ã‚’é †æ¬¡å®Ÿè¡Œ
    for (let i = 0; i < config.steps.length; i++) {
      await this.executeStep(i, config.steps[i]);
    }

    return new Response(JSON.stringify({ workflowId: workflowState.id }));
  }

  async executeStep(index, stepName) {
    const workflow = await this.state.storage.get('workflow');

    workflow.steps[index].status = 'running';
    workflow.steps[index].startedAt = new Date().toISOString();
    await this.state.storage.put('workflow', workflow);

    try {
      // Queueã«ã‚¿ã‚¹ã‚¯é€ä¿¡
      await this.env.TASK_QUEUE.send({ stepName });

      workflow.steps[index].status = 'completed';
    } catch (error) {
      workflow.steps[index].status = 'failed';
      workflow.steps[index].error = error.message;
    }

    await this.state.storage.put('workflow', workflow);
  }

  async getStatus() {
    const workflow = await this.state.storage.get('workflow');
    return new Response(JSON.stringify(workflow));
  }
}
```

---

## OSS ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ„ãƒ¼ãƒ«æ¯”è¼ƒ

### 1. Prefect â­â­â­â­â­ (æœ€æ¨å¥¨)

**ãƒ¢ãƒ€ãƒ³ã§Pythonicãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆï¼‰ã€‚**

#### ç‰¹å¾´

- âœ… Python-nativeã€ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
- âœ… ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆå®Ÿè¡Œæ™‚ã«DAGå¤‰æ›´å¯ï¼‰
- âœ… ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆå¯èƒ½ï¼ˆDocker/VMï¼‰
- âœ… ã‚¿ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–
- âœ… å„ªã‚ŒãŸUI

#### Cloudflareçµ±åˆä¾‹

```python
# flows/data_pipeline.py

from prefect import flow, task
import duckdb
import os

@task(retries=3, retry_delay_seconds=60)
def fetch_from_r2(bucket: str, prefix: str):
    """R2ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—"""
    conn = duckdb.connect(':memory:')
    conn.execute("INSTALL httpfs; LOAD httpfs;")
    conn.execute(f"SET s3_endpoint='{os.getenv('R2_ENDPOINT')}';")

    df = conn.execute(f"""
        SELECT * FROM read_parquet('s3://{bucket}/{prefix}/*.parquet')
    """).df()

    return df

@task
def transform_data(df):
    """ãƒ‡ãƒ¼ã‚¿å¤‰æ›"""
    df['processed_at'] = pd.Timestamp.now()
    df['total'] = df['amount'] * df['quantity']
    return df

@task(retries=2)
def write_to_r2(df, bucket: str, key: str):
    """R2ã«æ›¸ãè¾¼ã¿ï¼ˆWorkersçµŒç”±ï¼‰"""
    response = requests.post(
        'https://data-writer.yourcompany.workers.dev/write',
        json={
            'bucket': bucket,
            'key': key,
            'data': df.to_dict('records')
        },
        headers={'Authorization': f'Bearer {os.getenv("WORKERS_API_KEY")}'}
    )
    response.raise_for_status()

@flow(name="r2-data-pipeline")
def r2_data_pipeline(source_bucket: str, target_bucket: str):
    """R2ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    # ä¸¦åˆ—å®Ÿè¡Œ
    raw_data = fetch_from_r2.submit(source_bucket, 'bronze/raw')

    # å¤‰æ›
    transformed = transform_data(raw_data.result())

    # ä¿å­˜
    write_to_r2(transformed, target_bucket, 'silver/processed.parquet')

# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š
if __name__ == "__main__":
    r2_data_pipeline.serve(
        name="r2-pipeline-deployment",
        cron="0 */6 * * *"
    )
```

#### ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆï¼‰

```bash
# Prefect Serverã‚’ãƒ­ãƒ¼ã‚«ãƒ«èµ·å‹•
prefect server start

# åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆèµ·å‹•
prefect agent start -q default

# ã¾ãŸã¯ Docker Composeï¼ˆæ¨å¥¨ï¼‰
docker-compose up -d
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  prefect-server:
    image: prefecthq/prefect:2-python3.11
    command: prefect server start --host 0.0.0.0
    ports:
      - "4200:4200"
    environment:
      - PREFECT_SERVER_API_HOST=0.0.0.0
    volumes:
      - prefect-data:/root/.prefect

  prefect-agent:
    image: prefecthq/prefect:2-python3.11
    command: prefect agent start -q cloudflare-queue
    environment:
      - PREFECT_API_URL=http://prefect-server:4200/api
      - R2_ENDPOINT=${R2_ENDPOINT}
      - R2_ACCESS_KEY_ID=${R2_ACCESS_KEY_ID}
      - R2_SECRET_ACCESS_KEY=${R2_SECRET_ACCESS_KEY}
    volumes:
      - ./flows:/flows
    depends_on:
      - prefect-server

volumes:
  prefect-data:
```

---

### 2. Dagster â­â­â­â­

**ãƒ‡ãƒ¼ã‚¿è³‡ç”£ä¸­å¿ƒã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆï¼‰ã€‚**

#### ç‰¹å¾´

- âœ… Asset-orientedï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ = ç¬¬ä¸€ç´šå¸‚æ°‘ï¼‰
- âœ… ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°åŸå‰‡ï¼ˆãƒ†ã‚¹ãƒˆã€å‹ï¼‰
- âœ… ãƒ‡ãƒ¼ã‚¿ãƒªãƒãƒ¼ã‚¸è‡ªå‹•è¿½è·¡
- âœ… ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å¯¾å¿œ
- âœ… ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆå¯èƒ½ï¼ˆDocker/Kubernetesï¼‰

#### å®Ÿè£…ä¾‹

```python
# dagster_project/assets.py

from dagster import asset, AssetExecutionContext
import duckdb
import os

@asset(group_name="bronze")
def raw_api_data(context: AssetExecutionContext):
    """APIã‹ã‚‰ç”Ÿãƒ‡ãƒ¼ã‚¿å–å¾—ã—ã¦R2ã«ä¿å­˜"""
    data = fetch_api_data()

    # R2ã«ä¿å­˜ï¼ˆWorkersçµŒç”±ï¼‰
    write_to_r2('bronze/api_data.parquet', data)

    context.log.info(f"Fetched {len(data)} records")
    return data

@asset(group_name="silver", deps=[raw_api_data])
def cleaned_data(context: AssetExecutionContext):
    """ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    conn = duckdb.connect(':memory:')
    conn.execute("INSTALL httpfs; LOAD httpfs;")

    cleaned = conn.execute("""
        SELECT
          id,
          TRIM(name) as name,
          CAST(amount AS DECIMAL(10,2)) as amount
        FROM read_parquet('s3://bronze/api_data.parquet')
        WHERE amount IS NOT NULL
    """).df()

    write_to_r2('silver/cleaned.parquet', cleaned)
    return cleaned

@asset(group_name="gold", deps=[cleaned_data])
def aggregated_metrics(context: AssetExecutionContext):
    """é›†è¨ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    conn = duckdb.connect(':memory:')

    metrics = conn.execute("""
        SELECT
          DATE_TRUNC('day', created_at) as date,
          COUNT(*) as total_records,
          SUM(amount) as total_amount
        FROM read_parquet('s3://silver/cleaned.parquet')
        GROUP BY date
    """).df()

    write_to_r2('gold/metrics.parquet', metrics)
    return metrics
```

```python
# dagster_project/jobs.py

from dagster import define_asset_job, ScheduleDefinition

# ã‚¸ãƒ§ãƒ–å®šç¾©
daily_pipeline = define_asset_job(
    name="daily_pipeline",
    selection=["raw_api_data", "cleaned_data", "aggregated_metrics"]
)

# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
daily_schedule = ScheduleDefinition(
    job=daily_pipeline,
    cron_schedule="0 2 * * *"
)
```

---

### 3. Apache Airflow â­â­â­

**æ¥­ç•Œæ¨™æº–ã€æœ€ã‚‚æ™®åŠï¼ˆã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆï¼‰ã€‚**

#### ç‰¹å¾´

- âœ… æˆç†Ÿã—ãŸã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ 
- âœ… è±Šå¯Œãªã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼ˆ400+ï¼‰
- âœ… å¼·åŠ›ãªUI
- âœ… ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆï¼ˆDocker/Kubernetesï¼‰
- âŒ è¤‡é›‘ãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
- âŒ é‡ã„ï¼ˆãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»å¤§ï¼‰

#### DAGä¾‹

```python
# dags/cloudflare_r2_pipeline.py

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import duckdb

default_args = {
    'owner': 'data-team',
    'retries': 3,
    'retry_delay': timedelta(minutes=5)
}

def fetch_from_r2(**context):
    """R2ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—"""
    conn = duckdb.connect(':memory:')
    conn.execute("INSTALL httpfs; LOAD httpfs;")

    data = conn.execute("""
        SELECT * FROM read_parquet('s3://bronze/raw/*.parquet')
    """).fetchall()

    context['ti'].xcom_push(key='raw_data', value=data)

def transform(**context):
    """å¤‰æ›å‡¦ç†"""
    data = context['ti'].xcom_pull(key='raw_data')
    # å¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯
    transformed = process_data(data)
    context['ti'].xcom_push(key='transformed_data', value=transformed)

def load_to_r2(**context):
    """R2ã«ä¿å­˜"""
    data = context['ti'].xcom_pull(key='transformed_data')
    # WorkersçµŒç”±ã§ä¿å­˜
    save_via_workers(data)

with DAG(
    'r2_data_pipeline',
    default_args=default_args,
    description='R2 data processing pipeline',
    schedule_interval='0 */6 * * *',
    start_date=datetime(2025, 1, 1),
    catchup=False
) as dag:

    fetch = PythonOperator(
        task_id='fetch_from_r2',
        python_callable=fetch_from_r2
    )

    transform_task = PythonOperator(
        task_id='transform',
        python_callable=transform
    )

    load = PythonOperator(
        task_id='load_to_r2',
        python_callable=load_to_r2
    )

    fetch >> transform_task >> load
```

---

### 4. Temporal â­â­â­â­

**ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ãƒ»é•·æ™‚é–“å®Ÿè¡Œãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å‘ã‘ï¼ˆã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆï¼‰ã€‚**

#### ç‰¹å¾´

- âœ… éå¸¸ã«å …ç‰¢ï¼ˆè‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰
- âœ… é•·æ™‚é–“å®Ÿè¡Œãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆæ•°æ—¥ã€œæ•°ãƒ¶æœˆï¼‰
- âœ… è¤‡é›‘ãªçŠ¶æ…‹ç®¡ç†
- âœ… ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆå¯èƒ½ï¼ˆDocker/Kubernetesï¼‰
- âŒ å­¦ç¿’æ›²ç·šãŒæ€¥

#### ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¾‹

```typescript
// workflows/dataProcessing.ts

import { proxyActivities } from '@temporalio/workflow';
import type * as activities from './activities';

const { fetchFromR2, transformData, writeToR2 } = proxyActivities<typeof activities>({
  startToCloseTimeout: '10 minutes',
  retry: {
    initialInterval: '1s',
    maximumAttempts: 3
  }
});

export async function dataProcessingWorkflow(config: WorkflowConfig): Promise<void> {
  // ã‚¹ãƒ†ãƒƒãƒ—1
  const rawData = await fetchFromR2(config.sourceBucket);

  // ã‚¹ãƒ†ãƒƒãƒ—2ï¼ˆä¸¦åˆ—å®Ÿè¡Œå¯èƒ½ï¼‰
  const [transformed1, transformed2] = await Promise.all([
    transformData(rawData, 'model1'),
    transformData(rawData, 'model2')
  ]);

  // ã‚¹ãƒ†ãƒƒãƒ—3
  await writeToR2(config.targetBucket, [transformed1, transformed2]);
}
```

---

### 5. Kestra â­â­â­â­

**YAML-basedã€UIé‡è¦–ï¼ˆã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆï¼‰ã€‚**

#### ç‰¹å¾´

- âœ… YAMLã§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®šç¾©
- âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿè¡Œãƒ“ãƒ¥ãƒ¼
- âœ… ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ 
- âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç°¡å˜ï¼ˆDockerä¸€ç™ºï¼‰
- âœ… ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆå¯èƒ½

#### ãƒ•ãƒ­ãƒ¼å®šç¾©

```yaml
# flows/r2-data-pipeline.yml

id: r2-data-pipeline
namespace: data-engineering

inputs:
  - name: source_bucket
    type: STRING
    defaults: bronze

  - name: target_bucket
    type: STRING
    defaults: silver

tasks:
  - id: fetch_data
    type: io.kestra.plugin.scripts.python.Script
    script: |
      import duckdb
      conn = duckdb.connect(':memory:')
      conn.execute("INSTALL httpfs; LOAD httpfs;")
      data = conn.execute(f"SELECT * FROM read_parquet('s3://{{ inputs.source_bucket }}/*.parquet')").df()
      print(f"Fetched {len(data)} records")

  - id: transform
    type: io.kestra.plugin.scripts.python.Script
    script: |
      # å¤‰æ›å‡¦ç†
      transformed = data.copy()
      transformed['processed'] = True

  - id: upload_to_r2
    type: io.kestra.plugin.core.http.Request
    uri: https://data-writer.workers.dev/upload
    method: POST
    body: "{{ outputs.transform.data }}"

triggers:
  - id: daily_schedule
    type: io.kestra.core.models.triggers.types.Schedule
    cron: "0 2 * * *"
```

---

## æ¨å¥¨æ§‹æˆã¨ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹åˆ¥é¸æŠ

### ã‚·ãƒŠãƒªã‚ªåˆ¥æ¨å¥¨

| ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ | æ¨å¥¨ãƒ„ãƒ¼ãƒ« | ç†ç”± |
|-------------|-----------|------|
| **ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ»ãƒ­ã‚°åé›†** | Cloudflare Pipelines | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ETLã€Apache Icebergå¯¾å¿œ |
| **ã‚·ãƒ³ãƒ—ãƒ«ãªå®šæœŸãƒãƒƒãƒ** | Workers Cron + Queues | ã‚³ã‚¹ãƒˆæœ€å°ã€ç®¡ç†ä¸è¦ |
| **ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** | Prefect (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ) | ãƒãƒ©ãƒ³ã‚¹ã€ä½¿ã„ã‚„ã™ã„ã€è»½é‡ |
| **è¤‡é›‘ãªä¾å­˜é–¢ä¿‚ãƒ»ãƒ‡ãƒ¼ã‚¿è³‡ç”£ç®¡ç†** | Dagster (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ) | Asset-orientedã€ãƒªãƒãƒ¼ã‚¸ |
| **æ—¢å­˜Airflowåˆ©ç”¨çµ„ç¹”** | Apache Airflow (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ) | äº’æ›æ€§ã€ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ  |
| **ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹é€£æº** | Temporal (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ) | å …ç‰¢æ€§ã€é•·æ™‚é–“å®Ÿè¡Œ |
| **ãƒãƒ¼ã‚³ãƒ¼ãƒ‰å¿—å‘** | Kestra (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ) | YAMLã€UI |

### æ¨å¥¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#### ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒ•ãƒ«Cloudflareãƒã‚¤ãƒ†ã‚£ãƒ–ï¼ˆå°ã€œä¸­è¦æ¨¡ï¼‰

```
Workers Cron â†’ Queues â†’ Workers â†’ R2 â†’ D1
                  â†“
          Durable Objects (çŠ¶æ…‹ç®¡ç†)
```

**ãƒ¡ãƒªãƒƒãƒˆ:**
- é‹ç”¨ã‚³ã‚¹ãƒˆæœ€å°
- Cloudflareçµ±åˆå®Œç’§
- ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ:**
- UI/å¯è¦–åŒ–ãŒå¼±ã„
- è¤‡é›‘ãªDAGã¯å®Ÿè£…å¤§å¤‰

---

#### ãƒ‘ã‚¿ãƒ¼ãƒ³2: Prefect (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ) + Cloudflareï¼ˆä¸­ã€œå¤§è¦æ¨¡ï¼‰ğŸ†

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prefect Server       â”‚ (Docker/VM)
â”‚ - Webãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰   â”‚
â”‚ - ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prefect Agent        â”‚ (Docker/VM)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cloudflare Workers + R2        â”‚
â”‚   - ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Ÿè¡Œ               â”‚
â”‚   - R2èª­ã¿æ›¸ã                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—:**

```yaml
# docker-compose.yml

version: '3.8'
services:
  prefect-server:
    image: prefecthq/prefect:2-python3.11
    command: prefect server start --host 0.0.0.0
    ports:
      - "4200:4200"
    volumes:
      - prefect-data:/root/.prefect

  prefect-agent:
    image: prefecthq/prefect:2-python3.11
    environment:
      - PREFECT_API_URL=http://prefect-server:4200/api
      - R2_ENDPOINT=${R2_ENDPOINT}
      - R2_ACCESS_KEY_ID=${R2_ACCESS_KEY_ID}
      - R2_SECRET_ACCESS_KEY=${R2_SECRET_ACCESS_KEY}
    command: prefect agent start -q cloudflare-queue
    volumes:
      - ./flows:/flows
    depends_on:
      - prefect-server

volumes:
  prefect-data:
```

**ãƒ¡ãƒªãƒƒãƒˆ:**
- å¼·åŠ›ãªUIï¼ˆã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆã§ã‚‚åˆ©ç”¨å¯èƒ½ï¼‰
- Cloudflareçµ±åˆå¯èƒ½
- ã‚¿ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
- å¤–éƒ¨SaaSä¸è¦

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ:**
- ã‚µãƒ¼ãƒãƒ¼ï¼‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œç’°å¢ƒå¿…è¦ï¼ˆVM/Dockerï¼‰
- ã‚µãƒ¼ãƒãƒ¼ã®é‹ç”¨ãƒ»ä¿å®ˆãŒå¿…è¦

---

#### ãƒ‘ã‚¿ãƒ¼ãƒ³3: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼ˆè¶…å¤§è¦æ¨¡ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dagster    â”‚ (Assetç®¡ç†ãƒ»ãƒªãƒãƒ¼ã‚¸)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Workers Cron  â”‚ (ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Queues     â”‚ (ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Workers (å®Ÿè¡Œ)                  â”‚
â”‚   â†“                              â”‚
â”‚   R2, D1, Vectorize              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å®Ÿè£…ä¾‹: Prefectå®Œå…¨çµ±åˆ

### Flowsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
flows/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py
â”œâ”€â”€ tasks/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ r2_operations.py
â”‚   â”œâ”€â”€ transformations.py
â”‚   â””â”€â”€ notifications.py
â”œâ”€â”€ flows/
â”‚   â”œâ”€â”€ daily_ingestion.py
â”‚   â”œâ”€â”€ hourly_processing.py
â”‚   â””â”€â”€ weekly_aggregation.py
â””â”€â”€ deployments/
    â””â”€â”€ production.py
```

### R2 Operations

```python
# flows/tasks/r2_operations.py

from prefect import task
import duckdb
import os
import requests

@task(name="read-from-r2", retries=3)
def read_from_r2(bucket: str, prefix: str):
    """R2ã‹ã‚‰Parquetãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Š"""
    conn = duckdb.connect(':memory:')
    conn.execute("INSTALL httpfs; LOAD httpfs;")
    conn.execute(f"SET s3_endpoint='{os.getenv('R2_ENDPOINT')}';")
    conn.execute(f"SET s3_access_key_id='{os.getenv('R2_ACCESS_KEY_ID')}';")
    conn.execute(f"SET s3_secret_access_key='{os.getenv('R2_SECRET_ACCESS_KEY')}';")

    df = conn.execute(f"""
        SELECT * FROM read_parquet('s3://{bucket}/{prefix}/**/*.parquet')
    """).df()

    return df

@task(name="write-to-r2-via-worker", retries=2)
def write_to_r2_via_worker(data, bucket: str, key: str):
    """WorkersçµŒç”±ã§R2ã«æ›¸ãè¾¼ã¿"""
    response = requests.post(
        f"{os.getenv('WORKERS_API_URL')}/write",
        json={
            'bucket': bucket,
            'key': key,
            'data': data.to_dict('records')
        },
        headers={
            'Authorization': f"Bearer {os.getenv('WORKERS_API_KEY')}"
        },
        timeout=300
    )

    response.raise_for_status()
    return response.json()
```

### ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼

```python
# flows/flows/daily_ingestion.py

from prefect import flow
from prefect.tasks import task_input_hash
from datetime import timedelta
from tasks.r2_operations import read_from_r2, write_to_r2_via_worker
from tasks.transformations import clean_data, enrich_data
from tasks.notifications import send_slack_notification

@flow(
    name="daily-data-ingestion",
    description="Daily data ingestion and transformation pipeline",
    log_prints=True
)
def daily_ingestion_flow(date: str):
    """æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ãƒ•ãƒ­ãƒ¼"""

    # ä¸¦åˆ—ã§ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—
    api_data = read_from_r2.submit("bronze", f"api/{date}")
    db_data = read_from_r2.submit("bronze", f"database/{date}")

    # å¾…æ©Ÿ
    api_result = api_data.result()
    db_result = db_data.result()

    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆä¸¦åˆ—ï¼‰
    cleaned_api = clean_data.submit(api_result)
    cleaned_db = clean_data.submit(db_result)

    # ã‚¨ãƒ³ãƒªãƒƒãƒãƒ¡ãƒ³ãƒˆ
    enriched = enrich_data(
        cleaned_api.result(),
        cleaned_db.result()
    )

    # Silverãƒ¬ã‚¤ãƒ¤ãƒ¼ã«ä¿å­˜
    write_result = write_to_r2_via_worker(
        enriched,
        "silver",
        f"enriched/{date}/data.parquet"
    )

    # é€šçŸ¥
    send_slack_notification(
        f"âœ… Daily ingestion completed: {len(enriched)} records processed"
    )

    return write_result
```

### ãƒ‡ãƒ—ãƒ­ã‚¤

```python
# flows/deployments/production.py

from flows.daily_ingestion import daily_ingestion_flow
from prefect.deployments import Deployment
from prefect.server.schemas.schedules import CronSchedule

deployment = Deployment.build_from_flow(
    flow=daily_ingestion_flow,
    name="production-daily-ingestion",
    work_queue_name="cloudflare-queue",
    schedule=CronSchedule(cron="0 2 * * *", timezone="UTC"),
    parameters={"date": "{{ ds }}"},
    tags=["production", "daily", "r2"]
)

if __name__ == "__main__":
    deployment.apply()
```

```bash
# ãƒ‡ãƒ—ãƒ­ã‚¤å®Ÿè¡Œ
python flows/deployments/production.py

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆèµ·å‹•
prefect agent start -q cloudflare-queue
```

---

## ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»å¯è¦–åŒ–

### Prefect UI

- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿè¡ŒçŠ¶æ³
- ã‚¿ã‚¹ã‚¯ä¾å­˜é–¢ä¿‚ã‚°ãƒ©ãƒ•
- å¤±æ•—ã‚¢ãƒ©ãƒ¼ãƒˆ
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹

### ã‚«ã‚¹ã‚¿ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆEvidenceï¼‰

```sql
-- flows/monitoring/task_metrics.sql

SELECT
  flow_name,
  DATE_TRUNC('day', start_time) as date,
  COUNT(*) as total_runs,
  SUM(CASE WHEN status = 'COMPLETED' THEN 1 ELSE 0 END) as successful,
  SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) as failed,
  AVG(DATEDIFF('second', start_time, end_time)) as avg_duration_seconds
FROM prefect_flow_runs
WHERE start_time >= CURRENT_DATE - INTERVAL '30' DAY
GROUP BY flow_name, date
ORDER BY date DESC, flow_name
```

---

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. ã¹ãç­‰æ€§

```python
@task
def write_with_idempotency(data, partition_key):
    """ã¹ãç­‰ãªæ›¸ãè¾¼ã¿"""
    # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚­ãƒ¼ã§æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¸Šæ›¸ã
    key = f"silver/data/{partition_key}/data.parquet"
    write_to_r2(data, key)  # åŒã˜ã‚­ãƒ¼ãªã‚‰ä¸Šæ›¸ã
```

### 2. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
@task(retries=3, retry_delay_seconds=[10, 60, 300])
def resilient_task():
    """æ®µéšçš„ãªãƒªãƒˆãƒ©ã‚¤é…å»¶"""
    try:
        return risky_operation()
    except TemporaryError:
        raise  # ãƒªãƒˆãƒ©ã‚¤
    except PermanentError as e:
        log_to_slack(f"Permanent failure: {e}")
        raise  # å¤±æ•—ã¨ã—ã¦è¨˜éŒ²
```

### 3. ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†

```python
@task(tags=["heavy-compute"])
def cpu_intensive_task():
    """é‡ã„å‡¦ç†ã¯ã‚¿ã‚°ä»˜ã‘"""
    # å°‚ç”¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã§å®Ÿè¡Œ
    pass
```

---

## ã‚³ã‚¹ãƒˆæ¯”è¼ƒï¼ˆã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆå‰æï¼‰

| æ§‹æˆ | æœˆé–“ã‚³ã‚¹ãƒˆï¼ˆæ¦‚ç®—ï¼‰ | å†…è¨³ |
|------|------------------|------|
| **Cloudflare Pipelines** | $0-10 | ãƒ™ãƒ¼ã‚¿æœŸé–“ä¸­ç„¡æ–™ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ»ã‚¯ã‚¨ãƒªã®ã¿èª²é‡‘ |
| **Workers Cron + Queues** | $5-20 | Workerså®Ÿè¡Œè²»ã®ã¿ |
| **Prefect (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ)** | $30-100 | VM/Docker (2-4 vCPU, 4-8GB RAM) |
| **Dagster (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ)** | $50-150 | VM/Docker (4-8 vCPU, 8-16GB RAM) |
| **Airflow (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ)** | $50-200 | VM/K8s (4-8 vCPU, 8-16GB RAM) |
| **Temporal (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ)** | $100-300 | VM/K8s (8+ vCPU, 16+ GB RAM) |
| **Kestra (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ)** | $30-80 | VM/Docker (2-4 vCPU, 4-8GB RAM) |

**æ³¨æ„:**
- ã™ã¹ã¦å¤–éƒ¨SaaSä¸è¦ã®ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆæ§‹æˆ
- ã‚³ã‚¹ãƒˆã¯VMã¾ãŸã¯Dockerãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°è²»ç”¨ï¼ˆAWS/GCP/Azure EC2/Compute Engineç­‰ï¼‰
- Cloudflare Workersã®ã‚³ã‚¹ãƒˆã¯åˆ¥é€”

---

## ã¾ã¨ã‚

### æ¨å¥¨æ§‹æˆï¼ˆã™ã¹ã¦ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆï¼‰

#### ğŸ’š ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ETL
â†’ **Cloudflare Pipelines**ï¼ˆãƒ™ãƒ¼ã‚¿æœŸé–“ä¸­ç„¡æ–™ï¼‰

#### ğŸ’™ ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãƒ»ä¸­å°è¦æ¨¡ãƒãƒƒãƒå‡¦ç†
â†’ **Workers Cron + Queues**ï¼ˆæœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ï¼‰

#### ğŸ’œ æˆé•·ä¼æ¥­ãƒ»ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
â†’ **Prefect (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ) + Cloudflare Workers**

#### ğŸ§¡ å¤§ä¼æ¥­ãƒ»è¤‡é›‘ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
â†’ **Dagster (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ)** ã¾ãŸã¯ **Airflow (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ)** + Cloudflare

**ã™ã¹ã¦ã®æ§‹æˆã§å¤–éƒ¨SaaSã¯ä¸è¦ã€‚ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆã§å®Œå…¨ã«ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«å¯èƒ½ã€‚**

---

## å‚è€ƒãƒªãƒ³ã‚¯

### Cloudflareãƒã‚¤ãƒ†ã‚£ãƒ–
- [Cloudflare Pipelines](https://developers.cloudflare.com/pipelines/)
- [Cloudflare Data Platform](https://blog.cloudflare.com/cloudflare-data-platform/)
- [Cloudflare Queues](https://developers.cloudflare.com/queues/)
- [Cloudflare Workflows](https://developers.cloudflare.com/workflows/)
- [Durable Objects](https://developers.cloudflare.com/durable-objects/)

### OSSãƒ„ãƒ¼ãƒ«ï¼ˆã™ã¹ã¦ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆå¯èƒ½ï¼‰
- [Prefect Documentation](https://docs.prefect.io/)
- [Dagster Documentation](https://docs.dagster.io/)
- [Apache Airflow](https://airflow.apache.org/)
- [Temporal](https://temporal.io/)
- [Kestra](https://kestra.io/)

---

æœ€çµ‚æ›´æ–°: 2025-12-26
