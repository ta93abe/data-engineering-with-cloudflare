# GitHub Activity Pipeline è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

## 1. æ¦‚è¦

### 1.1 ç›®çš„

GitHubã®æ´»å‹•ãƒ­ã‚°ï¼ˆãƒªãƒã‚¸ãƒˆãƒªã‚¤ãƒ™ãƒ³ãƒˆã€ã‚³ãƒŸãƒƒãƒˆã€ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€Issueç­‰ï¼‰ã‚’å®šæœŸçš„ã«åé›†ã—ã€Cloudflare R2ã«DuckDB/Parquetå½¢å¼ã§ä¿å­˜ã™ã‚‹ã“ã¨ã§ã€é–‹ç™ºãƒãƒ¼ãƒ ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚’åˆ†æãƒ»å¯è¦–åŒ–ã™ã‚‹åŸºç›¤ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

### 1.2 ä¸»ãªæ©Ÿèƒ½

- **è‡ªå‹•åé›†**: Workers Cronã«ã‚ˆã‚‹å®šæœŸçš„ãªGitHub APIå‘¼ã³å‡ºã—
- **å¢—åˆ†åŒæœŸ**: å‰å›å–å¾—ä»¥é™ã®å·®åˆ†ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’åŠ¹ç‡çš„ã«å–å¾—
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«**: è¤‡æ•°ãƒªãƒã‚¸ãƒˆãƒªãƒ»çµ„ç¹”ã«å¯¾å¿œ
- **åˆ†ææœ€é©åŒ–**: DuckDB/Parquetå½¢å¼ã§ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€é©åŒ–
- **ã‚³ã‚¹ãƒˆåŠ¹ç‡**: R2ã®ã‚¨ã‚°ãƒ¬ã‚¹ç„¡æ–™ã‚’æ´»ç”¨

### 1.3 ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹

- é–‹ç™ºè€…ã®ç”Ÿç”£æ€§åˆ†æ
- ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼æ™‚é–“ã®å¯è¦–åŒ–
- ãƒªãƒã‚¸ãƒˆãƒªã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã®ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
- ãƒãƒ¼ãƒ ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æˆåŠŸç‡è¿½è·¡

## 2. GitHubãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹

### 2.1 GitHub Events API

**å¯¾è±¡ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—**:

| ã‚¤ãƒ™ãƒ³ãƒˆ | API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ | åé›†é »åº¦ | ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º (æ¦‚ç®—) |
|---------|------------------|---------|-------------------|
| **Repository Events** | `/repos/{owner}/{repo}/events` | 15åˆ† | 10-100 KB/ãƒªãƒã‚¸ãƒˆãƒª |
| **Pull Requests** | `/repos/{owner}/{repo}/pulls` | 30åˆ† | 50-500 KB/ãƒªãƒã‚¸ãƒˆãƒª |
| **Issues** | `/repos/{owner}/{repo}/issues` | 30åˆ† | 20-200 KB/ãƒªãƒã‚¸ãƒˆãƒª |
| **Commits** | `/repos/{owner}/{repo}/commits` | 1æ™‚é–“ | 100 KB-2 MB/ãƒªãƒã‚¸ãƒˆãƒª |
| **Workflows** | `/repos/{owner}/{repo}/actions/runs` | 30åˆ† | 50-500 KB/ãƒªãƒã‚¸ãƒˆãƒª |
| **Code Reviews** | `/repos/{owner}/{repo}/pulls/{number}/reviews` | 1æ™‚é–“ | 10-100 KB/PR |
| **Contributors** | `/repos/{owner}/{repo}/contributors` | 1æ—¥ | 10-50 KB/ãƒªãƒã‚¸ãƒˆãƒª |

### 2.2 GitHub APIåˆ¶é™

| ãƒ—ãƒ©ãƒ³ | ãƒ¬ãƒ¼ãƒˆåˆ¶é™ | èªè¨¼ | å‚™è€ƒ |
|-------|----------|------|------|
| **èªè¨¼æ¸ˆã¿** | 5,000 ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/æ™‚ | Personal Access Token | æ¨å¥¨ |
| **GitHub App** | 15,000 ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/æ™‚ | ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ | å¤§è¦æ¨¡çµ„ç¹”å‘ã‘ |
| **æœªèªè¨¼** | 60 ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/æ™‚ | ãªã— | éæ¨å¥¨ |

**ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ˜ãƒƒãƒ€ãƒ¼**:
- `X-RateLimit-Limit`: åˆ¶é™å€¤
- `X-RateLimit-Remaining`: æ®‹ã‚Šãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
- `X-RateLimit-Reset`: ãƒªã‚»ãƒƒãƒˆæ™‚åˆ»ï¼ˆUNIX timestampï¼‰

### 2.3 ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒä¾‹

#### Repository Events

```json
{
  "id": "12345678901",
  "type": "PushEvent",
  "actor": {
    "id": 123456,
    "login": "octocat",
    "avatar_url": "https://avatars.githubusercontent.com/u/123456"
  },
  "repo": {
    "id": 78910,
    "name": "octocat/Hello-World",
    "url": "https://api.github.com/repos/octocat/Hello-World"
  },
  "payload": {
    "push_id": 1234567890,
    "size": 1,
    "ref": "refs/heads/main",
    "commits": [...]
  },
  "created_at": "2025-01-01T12:00:00Z"
}
```

#### Pull Request

```json
{
  "id": 987654321,
  "number": 42,
  "state": "open",
  "title": "Add new feature",
  "user": {
    "login": "developer",
    "id": 111222
  },
  "created_at": "2025-01-01T10:00:00Z",
  "updated_at": "2025-01-01T14:00:00Z",
  "merged_at": null,
  "head": {
    "ref": "feature-branch",
    "sha": "abc123def456"
  },
  "base": {
    "ref": "main",
    "sha": "def456abc789"
  },
  "additions": 150,
  "deletions": 20,
  "changed_files": 5
}
```

## 3. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ

### 3.1 å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        GitHub API                               â”‚
â”‚  - Repository Events      - Pull Requests     - Actions         â”‚
â”‚  - Issues                 - Commits           - Contributors    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Cloudflare Workers Cron                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  github-events-collector (15åˆ†ã”ã¨)                      â”‚   â”‚
â”‚  â”‚  github-pr-collector (30åˆ†ã”ã¨)                          â”‚   â”‚
â”‚  â”‚  github-commits-collector (1æ™‚é–“ã”ã¨)                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Data Processing Workers                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  - Rate Limit Management                                 â”‚   â”‚
â”‚  â”‚  - Data Transformation                                   â”‚   â”‚
â”‚  â”‚  - Deduplication                                         â”‚   â”‚
â”‚  â”‚  - Schema Validation                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Workers KV     â”‚  â”‚    D1    â”‚  â”‚     R2 Storage          â”‚
â”‚                  â”‚  â”‚          â”‚  â”‚                         â”‚
â”‚ - API Tokens     â”‚  â”‚- State   â”‚  â”‚ Medallion Architecture: â”‚
â”‚ - Rate Limits    â”‚  â”‚- Metadataâ”‚  â”‚                         â”‚
â”‚ - Last Sync Time â”‚  â”‚- Config  â”‚  â”‚ bronze/                 â”‚
â”‚                  â”‚  â”‚          â”‚  â”‚   github_events/        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     year=2025/          â”‚
                                    â”‚       month=01/         â”‚
                                    â”‚         day=01/         â”‚
                                    â”‚   github_pulls/         â”‚
                                    â”‚   github_commits/       â”‚
                                    â”‚                         â”‚
                                    â”‚ silver/                 â”‚
                                    â”‚   github_events_clean/  â”‚
                                    â”‚   pr_metrics/           â”‚
                                    â”‚                         â”‚
                                    â”‚ gold/                   â”‚
                                    â”‚   developer_activity/   â”‚
                                    â”‚   repo_metrics/         â”‚
                                    â”‚   team_stats/           â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DuckDB Query Layer                            â”‚
â”‚  - R2 SQL (Native DuckDB integration)                           â”‚
â”‚  - Parquet file scanning                                        â”‚
â”‚  - Partitioned queries (year/month/day)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Analytics & Visualization                      â”‚
â”‚  - Evidence Dashboard (Team Metrics)                            â”‚
â”‚  - marimo Notebooks (Ad-hoc Analysis)                           â”‚
â”‚  - dbt Transformations (Silver â†’ Gold)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

#### 3.2.1 åé›†ãƒ•ãƒ­ãƒ¼

```
1. Workers Cron Trigger (15åˆ†/30åˆ†/1æ™‚é–“ã”ã¨)
        â”‚
        â–¼
2. KV: Last Sync Timeã‚’å–å¾—
        â”‚
        â–¼
3. GitHub API: å¢—åˆ†ãƒ‡ãƒ¼ã‚¿å–å¾—
   - If-Modified-Since ãƒ˜ãƒƒãƒ€ãƒ¼ä½¿ç”¨
   - ETagsæ´»ç”¨
        â”‚
        â–¼
4. Rate Limit Check (KVã§ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°)
   - Remaining < 100 â†’ ã‚¹ã‚­ãƒƒãƒ—
   - Remaining >= 100 â†’ ç¶šè¡Œ
        â”‚
        â–¼
5. Data Transformation
   - JSON â†’ Parquetå¤‰æ›
   - ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æ­£è¦åŒ– (UTC)
   - Nullå€¤å‡¦ç†
   - ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼
        â”‚
        â–¼
6. R2ã¸æ›¸ãè¾¼ã¿
   - ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³: year/month/day
   - ãƒ•ã‚¡ã‚¤ãƒ«å‘½å: {event_type}_{timestamp}_{uuid}.parquet
        â”‚
        â–¼
7. D1: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
   - last_sync_time
   - records_count
   - file_path
        â”‚
        â–¼
8. KV: Last Sync Timeæ›´æ–°
```

#### 3.2.2 å¤‰æ›ãƒ•ãƒ­ãƒ¼ (dbt)

```
Bronze (Raw)
    â”‚
    â”œâ†’ dbt: github_events_clean
    â”‚   - é‡è¤‡é™¤å»
    â”‚   - ç„¡åŠ¹ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿
    â”‚   - ã‚«ãƒ©ãƒ æ¨™æº–åŒ–
    â”‚
    â–¼
Silver (Cleaned)
    â”‚
    â”œâ†’ dbt: pr_metrics
    â”‚   - PRä½œæˆã‹ã‚‰ãƒãƒ¼ã‚¸ã¾ã§ã®æ™‚é–“
    â”‚   - ãƒ¬ãƒ“ãƒ¥ãƒ¼æ™‚é–“
    â”‚   - ã‚³ãƒ¡ãƒ³ãƒˆæ•°
    â”‚
    â”œâ†’ dbt: commit_metrics
    â”‚   - 1æ—¥ã‚ãŸã‚Šã®ã‚³ãƒŸãƒƒãƒˆæ•°
    â”‚   - å¤‰æ›´è¡Œæ•°çµ±è¨ˆ
    â”‚   - ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥åˆ†æ
    â”‚
    â–¼
Gold (Aggregated)
    â”‚
    â”œâ†’ developer_activity (é–‹ç™ºè€…åˆ¥)
    â”œâ†’ repo_metrics (ãƒªãƒã‚¸ãƒˆãƒªåˆ¥)
    â”œâ†’ team_stats (ãƒãƒ¼ãƒ åˆ¥)
    â””â†’ weekly_summary (é€±æ¬¡ã‚µãƒãƒªãƒ¼)
```

### 3.3 ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æ§‹é€ 

#### R2ãƒã‚±ãƒƒãƒˆæ§‹æˆ

```
github-data-lake/
â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ github_events/
â”‚   â”‚   â”œâ”€â”€ year=2025/
â”‚   â”‚   â”‚   â”œâ”€â”€ month=01/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ day=01/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ push_events_20250101_120000_abc123.parquet
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pull_request_events_20250101_120000_def456.parquet
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ day=02/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚   â””â”€â”€ month=02/
â”‚   â”‚   â””â”€â”€ year=2024/
â”‚   â”œâ”€â”€ github_pulls/
â”‚   â”‚   â””â”€â”€ year=2025/...
â”‚   â”œâ”€â”€ github_commits/
â”‚   â”‚   â””â”€â”€ year=2025/...
â”‚   â””â”€â”€ github_workflows/
â”‚       â””â”€â”€ year=2025/...
â”‚
â”œâ”€â”€ silver/
â”‚   â”œâ”€â”€ github_events_clean/
â”‚   â”‚   â””â”€â”€ year=2025/...
â”‚   â”œâ”€â”€ pr_metrics/
â”‚   â”‚   â””â”€â”€ year=2025/...
â”‚   â””â”€â”€ commit_metrics/
â”‚       â””â”€â”€ year=2025/...
â”‚
â””â”€â”€ gold/
    â”œâ”€â”€ developer_activity/
    â”‚   â””â”€â”€ year=2025/...
    â”œâ”€â”€ repo_metrics/
    â”‚   â””â”€â”€ year=2025/...
    â””â”€â”€ team_stats/
        â””â”€â”€ year=2025/...
```

#### Parquet ã‚¹ã‚­ãƒ¼ãƒ (Events)

```sql
CREATE TABLE github_events (
    event_id VARCHAR PRIMARY KEY,
    event_type VARCHAR NOT NULL,
    actor_id BIGINT,
    actor_login VARCHAR,
    repo_id BIGINT,
    repo_name VARCHAR,
    payload JSON,
    created_at TIMESTAMP,
    org VARCHAR,
    -- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚­ãƒ¼
    year INTEGER,
    month INTEGER,
    day INTEGER,
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    ingested_at TIMESTAMP,
    source_file VARCHAR
);
```

#### Parquet ã‚¹ã‚­ãƒ¼ãƒ (Pull Requests)

```sql
CREATE TABLE github_pulls (
    pull_id BIGINT PRIMARY KEY,
    number INTEGER,
    repo_name VARCHAR,
    state VARCHAR,
    title VARCHAR,
    user_login VARCHAR,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    closed_at TIMESTAMP,
    merged_at TIMESTAMP,
    head_ref VARCHAR,
    base_ref VARCHAR,
    additions INTEGER,
    deletions INTEGER,
    changed_files INTEGER,
    commits INTEGER,
    comments INTEGER,
    review_comments INTEGER,
    -- æ´¾ç”Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹
    time_to_merge_hours DOUBLE,
    time_to_first_review_hours DOUBLE,
    -- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
    year INTEGER,
    month INTEGER,
    day INTEGER,
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    ingested_at TIMESTAMP
);
```

## 4. å®Ÿè£…è©³ç´°

### 4.1 Workerså®Ÿè£…

#### 4.1.1 ã‚¤ãƒ™ãƒ³ãƒˆåé›†Worker

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/workers/github-events-collector.ts`

```typescript
interface Env {
  GITHUB_TOKEN: string;
  GITHUB_DATA_BUCKET: R2Bucket;
  GITHUB_STATE_KV: KVNamespace;
  GITHUB_METADATA_DB: D1Database;
  GITHUB_ANALYTICS: AnalyticsEngineDataset;
}

export default {
  async scheduled(event: ScheduledEvent, env: Env, ctx: ExecutionContext) {
    const repos = await getTargetRepos(env);

    for (const repo of repos) {
      try {
        await collectEvents(repo, env);
      } catch (error) {
        console.error(`Failed to collect events for ${repo}:`, error);
        await logError(env, repo, error);
      }
    }
  }
};

async function collectEvents(repo: string, env: Env) {
  // 1. å‰å›åŒæœŸæ™‚åˆ»ã‚’å–å¾—
  const lastSync = await env.GITHUB_STATE_KV.get(`last_sync:${repo}`);

  // 2. ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
  const rateLimit = await checkRateLimit(env);
  if (rateLimit.remaining < 100) {
    console.log('Rate limit low, skipping');
    return;
  }

  // 3. GitHub APIã‹ã‚‰å¢—åˆ†ãƒ‡ãƒ¼ã‚¿å–å¾—
  const events = await fetchGitHubEvents(repo, lastSync, env.GITHUB_TOKEN);

  if (events.length === 0) {
    console.log('No new events');
    return;
  }

  // 4. Parquetå½¢å¼ã«å¤‰æ›
  const parquetBuffer = await convertToParquet(events);

  // 5. R2ã«ä¿å­˜ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ§‹é€ ï¼‰
  const now = new Date();
  const path = `bronze/github_events/year=${now.getUTCFullYear()}/month=${
    String(now.getUTCMonth() + 1).padStart(2, '0')
  }/day=${String(now.getUTCDate()).padStart(2, '0')}/events_${
    Date.now()
  }_${crypto.randomUUID()}.parquet`;

  await env.GITHUB_DATA_BUCKET.put(path, parquetBuffer);

  // 6. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’D1ã«ä¿å­˜
  await env.GITHUB_METADATA_DB.prepare(`
    INSERT INTO ingestion_log (repo, event_type, count, file_path, created_at)
    VALUES (?, ?, ?, ?, ?)
  `).bind(repo, 'events', events.length, path, now.toISOString()).run();

  // 7. åŒæœŸæ™‚åˆ»ã‚’æ›´æ–°
  await env.GITHUB_STATE_KV.put(`last_sync:${repo}`, now.toISOString());

  // 8. ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
  env.GITHUB_ANALYTICS.writeDataPoint({
    blobs: ['github_ingestion', repo, 'events'],
    doubles: [events.length, parquetBuffer.byteLength],
    indexes: ['ingestion_timestamp']
  });
}
```

#### 4.1.2 GitHub API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/workers/lib/github-client.ts`

```typescript
interface GitHubClientOptions {
  token: string;
  baseUrl?: string;
}

export class GitHubClient {
  private token: string;
  private baseUrl: string;

  constructor(options: GitHubClientOptions) {
    this.token = options.token;
    this.baseUrl = options.baseUrl || 'https://api.github.com';
  }

  async fetchEvents(
    repo: string,
    since?: string,
    etag?: string
  ): Promise<{ events: any[]; etag: string; rateLimit: RateLimit }> {
    const headers: Record<string, string> = {
      'Authorization': `Bearer ${this.token}`,
      'Accept': 'application/vnd.github+json',
      'X-GitHub-Api-Version': '2022-11-28',
    };

    if (since) {
      headers['If-Modified-Since'] = since;
    }

    if (etag) {
      headers['If-None-Match'] = etag;
    }

    const response = await fetch(
      `${this.baseUrl}/repos/${repo}/events`,
      { headers }
    );

    // ãƒ¬ãƒ¼ãƒˆåˆ¶é™æƒ…å ±ã‚’å–å¾—
    const rateLimit = {
      limit: parseInt(response.headers.get('X-RateLimit-Limit') || '0'),
      remaining: parseInt(response.headers.get('X-RateLimit-Remaining') || '0'),
      reset: parseInt(response.headers.get('X-RateLimit-Reset') || '0'),
    };

    // 304 Not Modified
    if (response.status === 304) {
      return { events: [], etag: etag || '', rateLimit };
    }

    if (!response.ok) {
      throw new Error(`GitHub API error: ${response.status}`);
    }

    const events = await response.json();
    const newEtag = response.headers.get('ETag') || '';

    return { events, etag: newEtag, rateLimit };
  }

  async fetchPullRequests(
    repo: string,
    state: 'open' | 'closed' | 'all' = 'all',
    since?: string
  ): Promise<any[]> {
    const params = new URLSearchParams({
      state,
      sort: 'updated',
      direction: 'desc',
      per_page: '100',
    });

    if (since) {
      params.set('since', since);
    }

    const response = await fetch(
      `${this.baseUrl}/repos/${repo}/pulls?${params}`,
      {
        headers: {
          'Authorization': `Bearer ${this.token}`,
          'Accept': 'application/vnd.github+json',
        },
      }
    );

    if (!response.ok) {
      throw new Error(`GitHub API error: ${response.status}`);
    }

    return response.json();
  }

  async fetchCommits(
    repo: string,
    since?: string,
    until?: string
  ): Promise<any[]> {
    const params = new URLSearchParams({
      per_page: '100',
    });

    if (since) params.set('since', since);
    if (until) params.set('until', until);

    const response = await fetch(
      `${this.baseUrl}/repos/${repo}/commits?${params}`,
      {
        headers: {
          'Authorization': `Bearer ${this.token}`,
          'Accept': 'application/vnd.github+json',
        },
      }
    );

    if (!response.ok) {
      throw new Error(`GitHub API error: ${response.status}`);
    }

    return response.json();
  }
}
```

#### 4.1.3 Parquetå¤‰æ›

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/workers/lib/parquet-writer.ts`

```typescript
import { Table, vectorFromArray, tableToIPC } from 'apache-arrow';

export async function convertToParquet(events: any[]): Promise<Uint8Array> {
  // Apache Arrowã‚’ä½¿ã£ã¦Parquetå½¢å¼ã«å¤‰æ›
  const schema = {
    event_id: vectorFromArray(events.map(e => e.id)),
    event_type: vectorFromArray(events.map(e => e.type)),
    actor_id: vectorFromArray(events.map(e => e.actor.id)),
    actor_login: vectorFromArray(events.map(e => e.actor.login)),
    repo_id: vectorFromArray(events.map(e => e.repo.id)),
    repo_name: vectorFromArray(events.map(e => e.repo.name)),
    payload: vectorFromArray(events.map(e => JSON.stringify(e.payload))),
    created_at: vectorFromArray(events.map(e => new Date(e.created_at))),
    year: vectorFromArray(events.map(e => new Date(e.created_at).getUTCFullYear())),
    month: vectorFromArray(events.map(e => new Date(e.created_at).getUTCMonth() + 1)),
    day: vectorFromArray(events.map(e => new Date(e.created_at).getUTCDate())),
    ingested_at: vectorFromArray(events.map(() => new Date())),
  };

  const table = new Table(schema);
  const buffer = tableToIPC(table);

  return new Uint8Array(buffer);
}
```

### 4.2 D1ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `migrations/0001_github_metadata.sql`

```sql
-- ãƒªãƒã‚¸ãƒˆãƒªè¨­å®š
CREATE TABLE repositories (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name VARCHAR(255) NOT NULL UNIQUE,
    owner VARCHAR(255) NOT NULL,
    enabled BOOLEAN DEFAULT TRUE,
    sync_events BOOLEAN DEFAULT TRUE,
    sync_pulls BOOLEAN DEFAULT TRUE,
    sync_commits BOOLEAN DEFAULT TRUE,
    sync_workflows BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ã‚¤ãƒ³ã‚¸ã‚§ã‚¹ãƒˆãƒ­ã‚°
CREATE TABLE ingestion_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    repo VARCHAR(255) NOT NULL,
    event_type VARCHAR(50) NOT NULL,
    count INTEGER NOT NULL,
    file_path VARCHAR(500) NOT NULL,
    file_size_bytes INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status VARCHAR(20) DEFAULT 'success'
);

CREATE INDEX idx_ingestion_log_repo ON ingestion_log(repo);
CREATE INDEX idx_ingestion_log_created ON ingestion_log(created_at);

-- ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
CREATE TABLE rate_limit_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    remaining INTEGER NOT NULL,
    reset_at TIMESTAMP NOT NULL,
    checked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- åŒæœŸçŠ¶æ…‹ç®¡ç†
CREATE TABLE sync_state (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    repo VARCHAR(255) NOT NULL,
    sync_type VARCHAR(50) NOT NULL,
    last_sync_at TIMESTAMP,
    last_event_id VARCHAR(100),
    etag VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(repo, sync_type)
);

CREATE INDEX idx_sync_state_repo ON sync_state(repo);

-- ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
CREATE TABLE error_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    repo VARCHAR(255),
    worker_name VARCHAR(100),
    error_message TEXT,
    error_stack TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_error_log_created ON error_log(created_at);
```

### 4.3 Workers Cronè¨­å®š

**ãƒ•ã‚¡ã‚¤ãƒ«**: `wrangler.toml`

```toml
name = "github-activity-pipeline"
main = "src/workers/github-events-collector.ts"
compatibility_date = "2025-01-01"

# ã‚¤ãƒ™ãƒ³ãƒˆåé›†ï¼ˆ15åˆ†ã”ã¨ï¼‰
[triggers]
crons = ["*/15 * * * *"]

# R2ãƒã‚±ãƒƒãƒˆ
[[r2_buckets]]
binding = "GITHUB_DATA_BUCKET"
bucket_name = "github-data-lake"

# KVï¼ˆçŠ¶æ…‹ç®¡ç†ï¼‰
[[kv_namespaces]]
binding = "GITHUB_STATE_KV"
id = "github_state_kv_prod"

# D1ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼‰
[[d1_databases]]
binding = "GITHUB_METADATA_DB"
database_name = "github_metadata"
database_id = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"

# Analytics Engine
[[analytics_engine_datasets]]
binding = "GITHUB_ANALYTICS"

# ç’°å¢ƒå¤‰æ•°
[vars]
GITHUB_REPOS = '["owner1/repo1", "owner2/repo2"]'

# ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆï¼ˆwrangler secret putã§è¨­å®šï¼‰
# GITHUB_TOKEN
```

**è¿½åŠ ã®Workerè¨­å®š** (`wrangler.github-pr-collector.toml`):

```toml
name = "github-pr-collector"
main = "src/workers/github-pr-collector.ts"
compatibility_date = "2025-01-01"

# PRãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆ30åˆ†ã”ã¨ï¼‰
[triggers]
crons = ["*/30 * * * *"]

# åŒã˜ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨
[[r2_buckets]]
binding = "GITHUB_DATA_BUCKET"
bucket_name = "github-data-lake"

[[kv_namespaces]]
binding = "GITHUB_STATE_KV"
id = "github_state_kv_prod"

[[d1_databases]]
binding = "GITHUB_METADATA_DB"
database_name = "github_metadata"
database_id = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"

[[analytics_engine_datasets]]
binding = "GITHUB_ANALYTICS"

[vars]
GITHUB_REPOS = '["owner1/repo1", "owner2/repo2"]'
```

## 5. dbtå¤‰æ›å®Ÿè£…

### 5.1 dbtãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ

```
dbt_project/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ _bronze_schema.yml
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ github_events_clean.sql
â”‚   â”‚   â”œâ”€â”€ pr_metrics.sql
â”‚   â”‚   â”œâ”€â”€ commit_metrics.sql
â”‚   â”‚   â””â”€â”€ _silver_schema.yml
â”‚   â””â”€â”€ gold/
â”‚       â”œâ”€â”€ developer_activity.sql
â”‚       â”œâ”€â”€ repo_metrics.sql
â”‚       â”œâ”€â”€ team_stats.sql
â”‚       â””â”€â”€ _gold_schema.yml
â”œâ”€â”€ macros/
â”‚   â””â”€â”€ github_helpers.sql
â””â”€â”€ dbt_project.yml
```

### 5.2 Silver Layer: PR Metrics

**ãƒ•ã‚¡ã‚¤ãƒ«**: `models/silver/pr_metrics.sql`

```sql
{{
  config(
    materialized='incremental',
    unique_key='pull_id',
    partition_by={
      'field': 'merged_at',
      'data_type': 'timestamp',
      'granularity': 'day'
    }
  )
}}

WITH pull_requests AS (
  SELECT
    pull_id,
    number,
    repo_name,
    state,
    title,
    user_login,
    created_at,
    updated_at,
    closed_at,
    merged_at,
    head_ref,
    base_ref,
    additions,
    deletions,
    changed_files,
    commits,
    comments,
    review_comments
  FROM read_parquet('s3://github-data-lake/bronze/github_pulls/**/*.parquet')
  {% if is_incremental() %}
  WHERE updated_at > (SELECT MAX(updated_at) FROM {{ this }})
  {% endif %}
),

enriched AS (
  SELECT
    *,
    -- ä½œæˆã‹ã‚‰ãƒãƒ¼ã‚¸ã¾ã§ã®æ™‚é–“ï¼ˆæ™‚é–“å˜ä½ï¼‰
    CASE
      WHEN merged_at IS NOT NULL THEN
        EXTRACT(EPOCH FROM (merged_at - created_at)) / 3600.0
      ELSE NULL
    END AS time_to_merge_hours,

    -- ä½œæˆã‹ã‚‰ã‚¯ãƒ­ãƒ¼ã‚ºã¾ã§ã®æ™‚é–“
    CASE
      WHEN closed_at IS NOT NULL THEN
        EXTRACT(EPOCH FROM (closed_at - created_at)) / 3600.0
      ELSE NULL
    END AS time_to_close_hours,

    -- å¤‰æ›´è¡Œæ•°ã®åˆè¨ˆ
    additions + deletions AS total_changes,

    -- ã‚³ãƒ¼ãƒ‰ãƒãƒ£ãƒ¼ãƒ³ï¼ˆå‰Šé™¤/è¿½åŠ æ¯”ç‡ï¼‰
    CASE
      WHEN additions > 0 THEN
        CAST(deletions AS DOUBLE) / additions
      ELSE 0
    END AS code_churn_ratio,

    -- PRã‚µã‚¤ã‚ºã‚«ãƒ†ã‚´ãƒª
    CASE
      WHEN (additions + deletions) < 10 THEN 'XS'
      WHEN (additions + deletions) < 100 THEN 'S'
      WHEN (additions + deletions) < 500 THEN 'M'
      WHEN (additions + deletions) < 1000 THEN 'L'
      ELSE 'XL'
    END AS pr_size_category,

    -- é€±æœ«PRåˆ¤å®š
    CASE
      WHEN EXTRACT(DOW FROM created_at) IN (0, 6) THEN TRUE
      ELSE FALSE
    END AS is_weekend_pr,

    CURRENT_TIMESTAMP AS processed_at
  FROM pull_requests
)

SELECT * FROM enriched
```

### 5.3 Gold Layer: Developer Activity

**ãƒ•ã‚¡ã‚¤ãƒ«**: `models/gold/developer_activity.sql`

```sql
{{
  config(
    materialized='table',
    partition_by={
      'field': 'date',
      'data_type': 'date',
      'granularity': 'day'
    }
  )
}}

WITH daily_commits AS (
  SELECT
    author_login AS developer,
    DATE(created_at) AS date,
    COUNT(*) AS commit_count,
    SUM(additions) AS lines_added,
    SUM(deletions) AS lines_deleted,
    COUNT(DISTINCT repo_name) AS repos_touched
  FROM read_parquet('s3://github-data-lake/bronze/github_commits/**/*.parquet')
  WHERE created_at >= CURRENT_DATE - INTERVAL '90 days'
  GROUP BY author_login, DATE(created_at)
),

daily_prs AS (
  SELECT
    user_login AS developer,
    DATE(created_at) AS date,
    COUNT(*) AS prs_created,
    COUNT(CASE WHEN merged_at IS NOT NULL THEN 1 END) AS prs_merged,
    AVG(time_to_merge_hours) AS avg_time_to_merge_hours,
    SUM(additions + deletions) AS total_code_changes
  FROM {{ ref('pr_metrics') }}
  WHERE created_at >= CURRENT_DATE - INTERVAL '90 days'
  GROUP BY user_login, DATE(created_at)
),

daily_reviews AS (
  SELECT
    reviewer_login AS developer,
    DATE(submitted_at) AS date,
    COUNT(*) AS reviews_submitted,
    COUNT(CASE WHEN state = 'APPROVED' THEN 1 END) AS reviews_approved,
    COUNT(CASE WHEN state = 'CHANGES_REQUESTED' THEN 1 END) AS reviews_changes_requested
  FROM read_parquet('s3://github-data-lake/bronze/github_reviews/**/*.parquet')
  WHERE submitted_at >= CURRENT_DATE - INTERVAL '90 days'
  GROUP BY reviewer_login, DATE(submitted_at)
),

combined AS (
  SELECT
    COALESCE(c.developer, p.developer, r.developer) AS developer,
    COALESCE(c.date, p.date, r.date) AS date,
    COALESCE(c.commit_count, 0) AS commits,
    COALESCE(c.lines_added, 0) AS lines_added,
    COALESCE(c.lines_deleted, 0) AS lines_deleted,
    COALESCE(c.repos_touched, 0) AS repos_touched,
    COALESCE(p.prs_created, 0) AS prs_created,
    COALESCE(p.prs_merged, 0) AS prs_merged,
    COALESCE(p.avg_time_to_merge_hours, 0) AS avg_pr_merge_time_hours,
    COALESCE(p.total_code_changes, 0) AS total_pr_changes,
    COALESCE(r.reviews_submitted, 0) AS reviews_submitted,
    COALESCE(r.reviews_approved, 0) AS reviews_approved,
    COALESCE(r.reviews_changes_requested, 0) AS reviews_changes_requested
  FROM daily_commits c
  FULL OUTER JOIN daily_prs p
    ON c.developer = p.developer AND c.date = p.date
  FULL OUTER JOIN daily_reviews r
    ON COALESCE(c.developer, p.developer) = r.developer
    AND COALESCE(c.date, p.date) = r.date
)

SELECT
  developer,
  date,
  commits,
  lines_added,
  lines_deleted,
  lines_added + lines_deleted AS total_lines_changed,
  repos_touched,
  prs_created,
  prs_merged,
  CASE
    WHEN prs_created > 0 THEN
      CAST(prs_merged AS DOUBLE) / prs_created
    ELSE 0
  END AS pr_merge_rate,
  avg_pr_merge_time_hours,
  total_pr_changes,
  reviews_submitted,
  reviews_approved,
  reviews_changes_requested,

  -- ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚¹ã‚³ã‚¢ï¼ˆé‡ã¿ä»˜ã‘åˆè¨ˆï¼‰
  (commits * 1.0) +
  (prs_created * 3.0) +
  (prs_merged * 5.0) +
  (reviews_submitted * 2.0) +
  (lines_added / 100.0) AS activity_score,

  CURRENT_TIMESTAMP AS processed_at
FROM combined
ORDER BY date DESC, developer
```

## 6. DuckDBåˆ†æã‚¯ã‚¨ãƒª

### 6.1 R2 SQLã§ã®åˆ†æ

```sql
-- éå»30æ—¥ã®é–‹ç™ºè€…åˆ¥ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ©ãƒ³ã‚­ãƒ³ã‚°
SELECT
  developer,
  SUM(commits) AS total_commits,
  SUM(prs_created) AS total_prs,
  SUM(prs_merged) AS total_prs_merged,
  SUM(reviews_submitted) AS total_reviews,
  SUM(lines_added + lines_deleted) AS total_lines_changed,
  AVG(activity_score) AS avg_activity_score
FROM read_parquet('s3://github-data-lake/gold/developer_activity/**/*.parquet')
WHERE date >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY developer
ORDER BY avg_activity_score DESC
LIMIT 20;

-- ãƒªãƒã‚¸ãƒˆãƒªåˆ¥PRçµ±è¨ˆï¼ˆéå»90æ—¥ï¼‰
SELECT
  repo_name,
  COUNT(*) AS total_prs,
  COUNT(CASE WHEN merged_at IS NOT NULL THEN 1 END) AS merged_prs,
  AVG(time_to_merge_hours) AS avg_merge_time_hours,
  AVG(additions + deletions) AS avg_pr_size,
  AVG(review_comments) AS avg_review_comments
FROM read_parquet('s3://github-data-lake/silver/pr_metrics/**/*.parquet')
WHERE created_at >= CURRENT_DATE - INTERVAL '90 days'
GROUP BY repo_name
ORDER BY total_prs DESC;

-- é€±æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
SELECT
  DATE_TRUNC('week', date) AS week,
  COUNT(DISTINCT developer) AS active_developers,
  SUM(commits) AS total_commits,
  SUM(prs_merged) AS total_prs_merged,
  AVG(avg_pr_merge_time_hours) AS avg_merge_time
FROM read_parquet('s3://github-data-lake/gold/developer_activity/**/*.parquet')
WHERE date >= CURRENT_DATE - INTERVAL '180 days'
GROUP BY DATE_TRUNC('week', date)
ORDER BY week DESC;

-- PRã‚µã‚¤ã‚ºåˆ¥ãƒãƒ¼ã‚¸ç‡
SELECT
  pr_size_category,
  COUNT(*) AS total_prs,
  COUNT(CASE WHEN merged_at IS NOT NULL THEN 1 END) AS merged_count,
  CAST(COUNT(CASE WHEN merged_at IS NOT NULL THEN 1 END) AS DOUBLE) / COUNT(*) * 100 AS merge_rate_pct,
  AVG(time_to_merge_hours) AS avg_merge_time_hours
FROM read_parquet('s3://github-data-lake/silver/pr_metrics/**/*.parquet')
WHERE created_at >= CURRENT_DATE - INTERVAL '90 days'
GROUP BY pr_size_category
ORDER BY
  CASE pr_size_category
    WHEN 'XS' THEN 1
    WHEN 'S' THEN 2
    WHEN 'M' THEN 3
    WHEN 'L' THEN 4
    WHEN 'XL' THEN 5
  END;
```

### 6.2 marimoãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ä¾‹

**ãƒ•ã‚¡ã‚¤ãƒ«**: `notebooks/github_activity_analysis.py`

```python
import marimo as mo
import duckdb
import pandas as pd
import plotly.express as px

app = mo.App()

@app.cell
def setup_duckdb():
    """DuckDBæ¥ç¶šã¨R2è¨­å®š"""
    conn = duckdb.connect()

    # R2èªè¨¼æƒ…å ±è¨­å®š
    conn.execute("""
        CREATE SECRET r2_secret (
            TYPE S3,
            KEY_ID 'your_r2_access_key_id',
            SECRET 'your_r2_secret_access_key',
            ENDPOINT 'https://your_account_id.r2.cloudflarestorage.com'
        );
    """)

    return conn

@app.cell
def load_developer_activity(conn):
    """é–‹ç™ºè€…ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    query = """
    SELECT *
    FROM read_parquet('s3://github-data-lake/gold/developer_activity/**/*.parquet')
    WHERE date >= CURRENT_DATE - INTERVAL '30 days'
    ORDER BY date DESC, activity_score DESC
    """

    df = conn.execute(query).df()
    return df

@app.cell
def plot_activity_trend(df):
    """ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒˆãƒ¬ãƒ³ãƒ‰ã®å¯è¦–åŒ–"""
    daily_stats = df.groupby('date').agg({
        'commits': 'sum',
        'prs_merged': 'sum',
        'reviews_submitted': 'sum',
        'activity_score': 'mean'
    }).reset_index()

    fig = px.line(
        daily_stats,
        x='date',
        y=['commits', 'prs_merged', 'reviews_submitted'],
        title='Daily Activity Trends (Last 30 Days)',
        labels={'value': 'Count', 'variable': 'Metric'}
    )

    return mo.ui.plotly(fig)

@app.cell
def top_contributors(df):
    """ãƒˆãƒƒãƒ—ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚¿ãƒ¼"""
    top_devs = df.groupby('developer').agg({
        'commits': 'sum',
        'prs_merged': 'sum',
        'reviews_submitted': 'sum',
        'activity_score': 'mean'
    }).reset_index().sort_values('activity_score', ascending=False).head(10)

    return mo.ui.table(top_devs)

if __name__ == "__main__":
    app.run()
```

## 7. ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ

### 7.1 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç›£è¦–

#### Workers Analytics Engine

```typescript
// ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
env.GITHUB_ANALYTICS.writeDataPoint({
  blobs: [
    'github_ingestion',
    repo,
    'events',
    status // 'success' or 'error'
  ],
  doubles: [
    recordCount,
    fileSizeBytes,
    apiCallDuration,
    rateLimit.remaining
  ],
  indexes: ['ingestion_timestamp']
});
```

#### ç›£è¦–ã‚¯ã‚¨ãƒª

```sql
-- ç›´è¿‘1æ™‚é–“ã®ã‚¤ãƒ³ã‚¸ã‚§ã‚¹ãƒˆæˆåŠŸç‡
SELECT
  blob1 AS pipeline_type,
  blob2 AS repo,
  COUNT(*) AS total_runs,
  SUM(CASE WHEN blob4 = 'success' THEN 1 ELSE 0 END) AS successful_runs,
  AVG(double1) AS avg_records_ingested,
  AVG(double4) AS avg_rate_limit_remaining
FROM GITHUB_ANALYTICS
WHERE timestamp >= NOW() - INTERVAL '1 hour'
GROUP BY blob1, blob2;

-- ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¢ãƒ©ãƒ¼ãƒˆ
SELECT
  blob2 AS repo,
  MIN(double4) AS min_rate_limit_remaining,
  COUNT(*) AS api_calls
FROM GITHUB_ANALYTICS
WHERE timestamp >= NOW() - INTERVAL '1 hour'
  AND double4 < 500
GROUP BY blob2;
```

### 7.2 dbt + Elementaryç›£è¦–

**ãƒ•ã‚¡ã‚¤ãƒ«**: `models/silver/pr_metrics.yml`

```yaml
version: 2

models:
  - name: pr_metrics
    description: "Pull Request metrics and statistics"

    meta:
      owner: "data-engineering"

    tests:
      # é‡è¤‡ãƒã‚§ãƒƒã‚¯
      - dbt_utils.unique_combination_of_columns:
          combination_of_columns:
            - pull_id
            - repo_name

      # ãƒ‡ãƒ¼ã‚¿é®®åº¦ãƒã‚§ãƒƒã‚¯
      - elementary.freshness_anomalies:
          timestamp_column: updated_at
          time_bucket:
            period: hour
            count: 1

    columns:
      - name: pull_id
        description: "Unique pull request ID"
        tests:
          - not_null
          - unique

      - name: time_to_merge_hours
        description: "Time from PR creation to merge in hours"
        tests:
          - elementary.all_columns_anomalies:
              column_anomalies:
                - zero_count
                - zero_percent
          # ç•°å¸¸å€¤æ¤œå‡ºï¼ˆãƒãƒ¼ã‚¸æ™‚é–“ãŒ24æ™‚é–“æœªæº€ã§ã‚ã‚‹ã“ã¨ã‚’æœŸå¾…ï¼‰
          - dbt_expectations.expect_column_values_to_be_between:
              min_value: 0
              max_value: 168  # 1é€±é–“
              row_condition: "merged_at IS NOT NULL"

      - name: pr_size_category
        description: "PR size category (XS, S, M, L, XL)"
        tests:
          - accepted_values:
              values: ['XS', 'S', 'M', 'L', 'XL']
```

### 7.3 ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

#### Cloudflare Workers ã‚¢ãƒ©ãƒ¼ãƒˆ

```typescript
// ã‚¨ãƒ©ãƒ¼æ™‚ã«Slacké€šçŸ¥
async function sendAlert(error: Error, repo: string, env: Env) {
  const webhookUrl = env.SLACK_WEBHOOK_URL;

  const message = {
    text: `ğŸš¨ GitHub Activity Pipeline Error`,
    blocks: [
      {
        type: 'section',
        text: {
          type: 'mrkdwn',
          text: `*Repository:* ${repo}\n*Error:* ${error.message}`
        }
      },
      {
        type: 'context',
        elements: [
          {
            type: 'mrkdwn',
            text: `Timestamp: ${new Date().toISOString()}`
          }
        ]
      }
    ]
  };

  await fetch(webhookUrl, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(message)
  });
}
```

## 8. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹

### 8.1 èªè¨¼ãƒ»èªå¯

#### GitHub Tokenç®¡ç†

```bash
# Workers Secretã¨ã—ã¦è¨­å®š
wrangler secret put GITHUB_TOKEN

# GitHub Appä½¿ç”¨æ™‚
wrangler secret put GITHUB_APP_ID
wrangler secret put GITHUB_APP_PRIVATE_KEY
```

#### ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡

- **R2ãƒã‚±ãƒƒãƒˆ**: Cloudflare AccessçµŒç”±ã§ã®ã¿ã‚¢ã‚¯ã‚»ã‚¹
- **D1ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹**: Service Bindingsã§åˆ¶é™
- **Analytics Dashboard**: Cloudflare Access + IdPçµ±åˆ

### 8.2 ãƒ‡ãƒ¼ã‚¿ä¿è­·

#### å€‹äººæƒ…å ±ã®å–ã‚Šæ‰±ã„

- **Email**: ãƒãƒƒã‚·ãƒ¥åŒ–ã—ã¦ã‹ã‚‰ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- **åå‰**: å…¬é–‹æƒ…å ±ã®ã¿åé›†
- **å‰Šé™¤æ¨©**: GDPRå¯¾å¿œã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ãƒ•ãƒ­ãƒ¼å®Ÿè£…

```typescript
// EmailåŒ¿ååŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
function anonymizeEmail(email: string): string {
  const hash = crypto.subtle.digest(
    'SHA-256',
    new TextEncoder().encode(email)
  );
  return btoa(String.fromCharCode(...new Uint8Array(hash)));
}
```

### 8.3 ç›£æŸ»ãƒ­ã‚°

```sql
-- ã™ã¹ã¦ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
CREATE TABLE audit_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id VARCHAR(100),
    action VARCHAR(50),
    resource VARCHAR(200),
    ip_address VARCHAR(45),
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_audit_log_timestamp ON audit_log(timestamp);
CREATE INDEX idx_audit_log_user ON audit_log(user_id);
```

## 9. ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Š

### 9.1 Cloudflareã‚µãƒ¼ãƒ“ã‚¹ã‚³ã‚¹ãƒˆ

**å‰ææ¡ä»¶**:
- ãƒªãƒã‚¸ãƒˆãƒªæ•°: 10
- ã‚¤ãƒ™ãƒ³ãƒˆåé›†é »åº¦: 15åˆ†ã”ã¨
- PRãƒ‡ãƒ¼ã‚¿åé›†é »åº¦: 30åˆ†ã”ã¨
- æœˆé–“ãƒ‡ãƒ¼ã‚¿é‡: ç´„50 GBï¼ˆåœ§ç¸®å¾Œï¼‰

| ã‚µãƒ¼ãƒ“ã‚¹ | ä½¿ç”¨é‡ | ã‚³ã‚¹ãƒˆï¼ˆæœˆé¡ï¼‰ | å‚™è€ƒ |
|---------|--------|--------------|------|
| **Workers** | ç´„14,000ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/æœˆ | $0 | Free tierã§ååˆ† |
| **Workers CPU Time** | ç´„50ç§’/æœˆ | $0 | Free tierã§ååˆ† |
| **R2 Storage** | 50 GB | $0.75 | $0.015/GB |
| **R2 Class A Operations** | ç´„14,000å›/æœˆ | $0.63 | $0.0045/1000 |
| **R2 Class B Operations** | ç´„1,000å›/æœˆ | $0.00 | $0.00036/1000 |
| **D1 Storage** | 100 MB | $0 | Free tier 5GB |
| **D1 Reads** | ç´„10,000å›/æœˆ | $0 | Free tier 500ä¸‡/æœˆ |
| **KV Reads** | ç´„14,000å›/æœˆ | $0 | Free tier 10ä¸‡/æœˆ |
| **KV Writes** | ç´„14,000å›/æœˆ | $0.70 | $0.05/1000 |
| **Analytics Engine** | ç´„14,000 data points/æœˆ | $0 | Free tier 1000ä¸‡/æœˆ |
| **åˆè¨ˆ** | - | **ç´„$2.08/æœˆ** | - |

### 9.2 GitHub APIã‚³ã‚¹ãƒˆ

- **ç„¡æ–™**: GitHub APIã¯ç„¡æ–™ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å†…ï¼‰
- **GitHub Enterprise**: è¿½åŠ ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãŒå¿…è¦ãªå ´åˆ

### 9.3 ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ™‚ã®ã‚³ã‚¹ãƒˆ

**100ãƒªãƒã‚¸ãƒˆãƒªã®å ´åˆ**:
- R2 Storage (500 GB): $7.50/æœˆ
- R2 Operations: $6.30/æœˆ
- KV Writes: $7.00/æœˆ
- **åˆè¨ˆ**: ç´„$21/æœˆ

**1000ãƒªãƒã‚¸ãƒˆãƒªã®å ´åˆ**:
- R2 Storage (5 TB): $75/æœˆ
- R2 Operations: $63/æœˆ
- KV Writes: $70/æœˆ
- Workers (æœ‰æ–™ãƒ—ãƒ©ãƒ³): $5/æœˆ
- **åˆè¨ˆ**: ç´„$213/æœˆ

### 9.4 å¾“æ¥ã®ã‚¯ãƒ©ã‚¦ãƒ‰ã¨ã®æ¯”è¼ƒ

**AWSåŒç­‰æ§‹æˆ**:
- S3 (50 GB + ãƒªã‚¯ã‚¨ã‚¹ãƒˆ): $5/æœˆ
- **S3 Egress (50 GB)**: $4.5/æœˆ
- Lambda: $3/æœˆ
- DynamoDB: $5/æœˆ
- CloudWatch: $3/æœˆ
- **åˆè¨ˆ**: ç´„$20.5/æœˆ

**Cloudflareã®å„ªä½æ€§**:
- **90%ã‚³ã‚¹ãƒˆå‰Šæ¸›**: $2 vs $20.5
- ã‚¨ã‚°ãƒ¬ã‚¹ç„¡æ–™ã§ã•ã‚‰ã«æ‹¡å¤§æ™‚ã«å·®ãŒé–‹ã

## 10. å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### Phase 1: åŸºæœ¬å®Ÿè£…ï¼ˆ2é€±é–“ï¼‰

- [x] **Week 1**: åŸºç›¤ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
  - [ ] Wranglerãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
  - [ ] D1ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆãƒ»ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
  - [ ] R2ãƒã‚±ãƒƒãƒˆä½œæˆ
  - [ ] KVãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹ä½œæˆ
  - [ ] GitHub Tokenè¨­å®š

- [ ] **Week 2**: ã‚³ã‚¢æ©Ÿèƒ½å®Ÿè£…
  - [ ] GitHub APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå®Ÿè£…
  - [ ] ã‚¤ãƒ™ãƒ³ãƒˆåé›†Workerå®Ÿè£…
  - [ ] Parquetå¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…
  - [ ] R2æ›¸ãè¾¼ã¿å®Ÿè£…
  - [ ] Workers Cronè¨­å®š

### Phase 2: ãƒ‡ãƒ¼ã‚¿å¤‰æ›ï¼ˆ1é€±é–“ï¼‰

- [ ] **Week 3**: dbtå®Ÿè£…
  - [ ] dbtãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
  - [ ] Silverãƒ¢ãƒ‡ãƒ«å®Ÿè£… (pr_metrics, commit_metrics)
  - [ ] Goldãƒ¢ãƒ‡ãƒ«å®Ÿè£… (developer_activity, repo_metrics)
  - [ ] Elementaryçµ±åˆ

### Phase 3: è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆ1é€±é–“ï¼‰

- [ ] **Week 4**: æ‹¡å¼µ
  - [ ] PRåé›†Workerå®Ÿè£…
  - [ ] Commitsåé›†Workerå®Ÿè£…
  - [ ] Workflowsåé›†Workerå®Ÿè£…
  - [ ] Code Reviewsåé›†Workerå®Ÿè£…

### Phase 4: åˆ†æãƒ»å¯è¦–åŒ–ï¼ˆ1é€±é–“ï¼‰

- [ ] **Week 5**: åˆ†æåŸºç›¤
  - [ ] marimoãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ä½œæˆ
  - [ ] Evidenceãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å®Ÿè£…
  - [ ] DuckDBã‚¯ã‚¨ãƒªæœ€é©åŒ–

### Phase 5: ç›£è¦–ãƒ»æœ€é©åŒ–ï¼ˆ1é€±é–“ï¼‰

- [ ] **Week 6**: æœ¬ç•ªåŒ–
  - [ ] Analytics Engineçµ±åˆ
  - [ ] ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
  - [ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–
  - [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
  - [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™

## 11. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 11.1 ã‚ˆãã‚ã‚‹å•é¡Œ

#### GitHub APIãƒ¬ãƒ¼ãƒˆåˆ¶é™è¶…é

**ç—‡çŠ¶**:
```
Error: API rate limit exceeded (403)
```

**è§£æ±ºç­–**:
1. KVã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™çŠ¶æ…‹ã‚’ç¢ºèª
2. ãƒªã‚¯ã‚¨ã‚¹ãƒˆé »åº¦ã‚’èª¿æ•´
3. GitHub Appã«ç§»è¡Œï¼ˆ15,000ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/æ™‚ï¼‰
4. è¤‡æ•°ã®ãƒˆãƒ¼ã‚¯ãƒ³ã§ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚¹

```typescript
// ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
async function shouldSkipDueToRateLimit(env: Env): Promise<boolean> {
  const rateLimitStr = await env.GITHUB_STATE_KV.get('rate_limit');
  if (!rateLimitStr) return false;

  const rateLimit = JSON.parse(rateLimitStr);
  if (rateLimit.remaining < 100) {
    const resetTime = new Date(rateLimit.reset * 1000);
    if (resetTime > new Date()) {
      console.log(`Rate limit low, waiting until ${resetTime}`);
      return true;
    }
  }

  return false;
}
```

#### Parquetå¤‰æ›ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**:
```
Error: Failed to convert to Parquet
```

**è§£æ±ºç­–**:
1. ã‚¹ã‚­ãƒ¼ãƒã®å‹ä¸ä¸€è‡´ã‚’ç¢ºèª
2. Nullå€¤å‡¦ç†ã‚’è¿½åŠ 
3. ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’åˆ¶é™ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰

```typescript
// Nullå€¤å‡¦ç†
function sanitizeEvent(event: any) {
  return {
    ...event,
    actor: event.actor || { id: 0, login: 'unknown' },
    repo: event.repo || { id: 0, name: 'unknown' },
    payload: event.payload || {},
  };
}
```

#### R2æ›¸ãè¾¼ã¿å¤±æ•—

**ç—‡çŠ¶**:
```
Error: R2 PUT failed (500)
```

**è§£æ±ºç­–**:
1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ç¢ºèªï¼ˆæœ€å¤§5TBï¼‰
2. ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…
3. ãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä½¿ç”¨ï¼ˆå¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ï¼‰

```typescript
// ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯
async function putWithRetry(
  bucket: R2Bucket,
  key: string,
  value: Uint8Array,
  maxRetries = 3
): Promise<void> {
  for (let i = 0; i < maxRetries; i++) {
    try {
      await bucket.put(key, value);
      return;
    } catch (error) {
      if (i === maxRetries - 1) throw error;
      await new Promise(resolve => setTimeout(resolve, 1000 * (i + 1)));
    }
  }
}
```

### 11.2 ãƒ‡ãƒãƒƒã‚°æ–¹æ³•

#### Workers Logsã®ç¢ºèª

```bash
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°
wrangler tail github-events-collector

# ç‰¹å®šã®æœŸé–“ã®ãƒ­ã‚°
wrangler tail github-events-collector --since 1h
```

#### D1ãƒ‡ãƒ¼ã‚¿ç¢ºèª

```bash
# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç¢ºèª
wrangler d1 execute github_metadata --command "SELECT * FROM ingestion_log ORDER BY created_at DESC LIMIT 10"

# ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç¢ºèª
wrangler d1 execute github_metadata --command "SELECT * FROM error_log ORDER BY created_at DESC LIMIT 10"
```

#### R2ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª

```bash
# ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
wrangler r2 object list github-data-lake --prefix "bronze/github_events/"

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
wrangler r2 object get github-data-lake "bronze/github_events/year=2025/month=01/day=01/events_xxx.parquet" --file ./test.parquet

# DuckDBã§æ¤œè¨¼
duckdb
D SELECT COUNT(*) FROM read_parquet('test.parquet');
```

## 12. ä»Šå¾Œã®æ‹¡å¼µ

### 12.1 çŸ­æœŸï¼ˆ3ãƒ¶æœˆï¼‰

- [ ] **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¤ãƒ™ãƒ³ãƒˆ**: Webhooksã«ã‚ˆã‚‹å³æ™‚ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿
- [ ] **ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†æ**: è©³ç´°ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- [ ] **CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆ†æ**: Actionså®Ÿè¡Œæ™‚é–“ãƒ»æˆåŠŸç‡
- [ ] **ä¾å­˜é–¢ä¿‚åˆ†æ**: ãƒªãƒã‚¸ãƒˆãƒªé–“ã®ä¾å­˜é–¢ä¿‚ã‚°ãƒ©ãƒ•

### 12.2 ä¸­æœŸï¼ˆ6ãƒ¶æœˆï¼‰

- [ ] **æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬**: PR ãƒãƒ¼ã‚¸æ™‚é–“äºˆæ¸¬
- [ ] **ç•°å¸¸æ¤œçŸ¥**: é–‹ç™ºãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç•°å¸¸æ¤œå‡º
- [ ] **ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³**: ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 
- [ ] **ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•**: ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹çŸ¥è­˜ãƒãƒƒãƒ—

### 12.3 é•·æœŸï¼ˆ12ãƒ¶æœˆï¼‰

- [ ] **ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹çµ±åˆ**: Jiraã€Slackã€Linearç­‰ã¨ã®é€£æº
- [ ] **ç”ŸæˆAIçµ±åˆ**: ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ†æãƒ»è¦ç´„
- [ ] **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: Durable Objectsã«ã‚ˆã‚‹å³æ™‚æ›´æ–°
- [ ] **å¤–éƒ¨å…¬é–‹API**: åˆ†æãƒ‡ãƒ¼ã‚¿ã®APIæä¾›

## 13. å‚è€ƒè³‡æ–™

### 13.1 GitHub API

- [GitHub REST API Documentation](https://docs.github.com/en/rest)
- [GitHub Events API](https://docs.github.com/en/rest/activity/events)
- [Rate Limiting](https://docs.github.com/en/rest/overview/resources-in-the-rest-api#rate-limiting)

### 13.2 Cloudflare

- [Workers Documentation](https://developers.cloudflare.com/workers/)
- [R2 Storage](https://developers.cloudflare.com/r2/)
- [D1 Database](https://developers.cloudflare.com/d1/)
- [Workers KV](https://developers.cloudflare.com/kv/)
- [Analytics Engine](https://developers.cloudflare.com/analytics/analytics-engine/)

### 13.3 DuckDB & Parquet

- [DuckDB Documentation](https://duckdb.org/docs/)
- [Parquet Format](https://parquet.apache.org/docs/)
- [Apache Arrow](https://arrow.apache.org/docs/)

### 13.4 dbt

- [dbt Documentation](https://docs.getdbt.com/)
- [Elementary Data Observability](https://docs.elementary-data.com/)

## 14. ã¾ã¨ã‚

### 14.1 ä¸»ãªç‰¹å¾´

- **ä½ã‚³ã‚¹ãƒˆ**: æœˆé¡$2ã€œã§é‹ç”¨å¯èƒ½ï¼ˆ10ãƒªãƒã‚¸ãƒˆãƒªï¼‰
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«**: 1000ãƒªãƒã‚¸ãƒˆãƒªã§ã‚‚æœˆé¡$213
- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ **: 15åˆ†ã”ã¨ã®è‡ªå‹•æ›´æ–°
- **åˆ†ææœ€é©åŒ–**: DuckDB/Parquetå½¢å¼ã§é«˜é€Ÿã‚¯ã‚¨ãƒª
- **ä¿å®ˆæ€§**: ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ã§é‹ç”¨è² è·æœ€å°

### 14.2 æ¨å¥¨æ§‹æˆ

| è¦æ¨¡ | ãƒªãƒã‚¸ãƒˆãƒªæ•° | åé›†é »åº¦ | æœˆé¡ã‚³ã‚¹ãƒˆ | å‚™è€ƒ |
|------|------------|---------|----------|------|
| **Small** | 1-10 | 15åˆ† | $2-5 | Free tieræ´»ç”¨ |
| **Medium** | 10-100 | 15åˆ† | $5-25 | Workersæœ‰æ–™ãƒ—ãƒ©ãƒ³æ¨å¥¨ |
| **Large** | 100-1000 | 15åˆ† | $25-250 | GitHub Appæ¨å¥¨ |
| **Enterprise** | 1000+ | 5-15åˆ† | $250+ | å°‚ç”¨ã‚¤ãƒ³ãƒ•ãƒ©æ¤œè¨ |

### 14.3 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **Phase 1å®Ÿè£…**: Workers + R2ã®åŸºæœ¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
2. **ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼**: 1é€±é–“ã®ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»æ¤œè¨¼
3. **dbtçµ±åˆ**: Silver/Goldãƒ¬ã‚¤ãƒ¤ãƒ¼å®Ÿè£…
4. **ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: Evidence/marimoã§å¯è¦–åŒ–
5. **æœ¬ç•ªåŒ–**: ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

---

**æœ€çµ‚æ›´æ–°**: 2026-01-01
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
**è‘—è€…**: Data Engineering Team
