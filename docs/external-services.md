# å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹çµ±åˆã‚¬ã‚¤ãƒ‰

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€Cloudflareãƒ‡ãƒ¼ã‚¿åŸºç›¤ã¨é€£æºã™ã‚‹å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ã«ã¤ã„ã¦è§£èª¬ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨åˆ†æç’°å¢ƒã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚

## ç›®æ¬¡

- [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦](#ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦)
- [ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ»å‡¦ç†](#ãƒ‡ãƒ¼ã‚¿å¤‰æ›å‡¦ç†)
- [ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ](#ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ)
- [ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–](#ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–)
- [CI/CDãƒ»é–‹ç™ºåŸºç›¤](#cicdé–‹ç™ºåŸºç›¤)
- [é€šçŸ¥ãƒ»ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³](#é€šçŸ¥ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³)
- [çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä¾‹](#çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä¾‹)

---

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Data Sources                             â”‚
â”‚              (APIs, Databases, Files, Events)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ingestion & Processing                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   dlt    â”‚  Cloudflare  â”‚  Cloudflare â”‚   GitHub     â”‚      â”‚
â”‚  â”‚(Extract) â”‚   Workers    â”‚   Pipelines â”‚   Actions    â”‚      â”‚
â”‚  â”‚          â”‚  (Transform) â”‚  (Stream)   â”‚  (Orchestrate)â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Storage Layer                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Cloudflare R2    â”‚  Cloudflare D1  â”‚  Cloudflare KV   â”‚     â”‚
â”‚  â”‚ (Apache Iceberg) â”‚     (SQLite)    â”‚   (Cache)        â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Transformation Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚       dbt        â”‚     DuckDB      â”‚  Cloudflare      â”‚     â”‚
â”‚  â”‚  (SQL Transform) â”‚   (Analytics)   â”‚   Workers        â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Visualization & BI                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Evidence.dev   â”‚    Cloudflare   â”‚      Slack       â”‚     â”‚
â”‚  â”‚  (Dashboard)     â”‚     Pages       â”‚  (Notifications) â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ»å‡¦ç†

### Elementary

**å…¬å¼ã‚µã‚¤ãƒˆ**: https://www.elementary-data.com/
**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: https://docs.elementary-data.com/

#### æ¦‚è¦
Elementaryã¯ã€dbtãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–ãŠã‚ˆã³ã‚ªãƒ–ã‚¶ãƒ¼ãƒãƒ“ãƒªãƒ†ã‚£ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚dbtãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã—ã¦çµ±åˆã•ã‚Œã€ãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸æ¤œçŸ¥ã€ã‚¹ã‚­ãƒ¼ãƒå¤‰æ›´ã®è¿½è·¡ã€ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ†ã‚¹ãƒˆã®ç›£è¦–ã‚’è‡ªå‹•åŒ–ã—ã¾ã™ã€‚

#### ãƒ‡ãƒ¼ã‚¿åŸºç›¤ã§ã®å½¹å‰²
- **ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–**: dbtãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œçµæœã‚’è¿½è·¡ãƒ»å¯è¦–åŒ–
- **ç•°å¸¸æ¤œçŸ¥**: æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ç•°å¸¸æ¤œçŸ¥
- **ã‚¹ã‚­ãƒ¼ãƒç›£è¦–**: ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ¼ãƒã®å¤‰æ›´ã‚’è‡ªå‹•æ¤œçŸ¥
- **ãƒ‡ãƒ¼ã‚¿ãƒªãƒãƒ¼ã‚¸ãƒ¥**: ãƒ¢ãƒ‡ãƒ«é–“ã®ä¾å­˜é–¢ä¿‚ã‚’å¯è¦–åŒ–
- **ã‚¢ãƒ©ãƒ¼ãƒˆ**: Slackã¸ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šçŸ¥
- **ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: ãƒ‡ãƒ¼ã‚¿å“è³ªã®çµ±åˆãƒ“ãƒ¥ãƒ¼

#### Cloudflareã¨ã®çµ±åˆ

##### dbtãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®è¿½åŠ 
```yaml
# dbt/packages.yml
packages:
  - package: elementary-data/elementary
    version: 0.15.1  # æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèª
```

##### profiles.ymlè¨­å®šï¼ˆDuckDB + R2ï¼‰
```yaml
# dbt/profiles.yml
elementary_project:
  target: prod
  outputs:
    prod:
      type: duckdb
      path: ':memory:'
      extensions:
        - httpfs
      settings:
        s3_endpoint: '<account-id>.r2.cloudflarestorage.com'
        s3_access_key_id: '{{ env_var("R2_ACCESS_KEY_ID") }}'
        s3_secret_access_key: '{{ env_var("R2_SECRET_ACCESS_KEY") }}'
        # Elementaryç”¨ã®ãƒ­ãƒ¼ã‚«ãƒ«DBï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼‰
        elementary_database_path: 'elementary.duckdb'
```

##### dbtãƒ¢ãƒ‡ãƒ«ã§ã®Elementaryãƒ†ã‚¹ãƒˆ
```sql
-- models/staging/stg_events.sql
{{
  config(
    materialized='incremental',
    unique_key='event_id',
    # Elementaryã®ç•°å¸¸æ¤œçŸ¥ãƒ†ã‚¹ãƒˆã‚’æœ‰åŠ¹åŒ–
    elementary_enabled=true
  )
}}

SELECT
  event_id,
  user_id,
  event_type,
  event_timestamp,
  event_count
FROM read_parquet('s3://my-bucket/events/*.parquet')
{% if is_incremental() %}
WHERE event_timestamp > (SELECT MAX(event_timestamp) FROM {{ this }})
{% endif %}
```

```yaml
# models/staging/schema.yml
version: 2

models:
  - name: stg_events
    description: Staging layer for event data
    columns:
      - name: event_id
        description: Unique event identifier
        tests:
          - unique
          - not_null
          # Elementaryç•°å¸¸æ¤œçŸ¥ãƒ†ã‚¹ãƒˆ
          - elementary.volume_anomalies:
              timestamp_column: event_timestamp
              sensitivity: 3
          - elementary.dimension_anomalies:
              dimensions:
                - event_type
              timestamp_column: event_timestamp

      - name: event_count
        description: Event count value
        tests:
          # æ•°å€¤ç¯„å›²ã®ç•°å¸¸æ¤œçŸ¥
          - elementary.all_columns_anomalies:
              column_anomalies:
                - event_count
              timestamp_column: event_timestamp
```

##### ã‚¹ã‚­ãƒ¼ãƒå¤‰æ›´ç›£è¦–
```yaml
# dbt_project.yml
models:
  my_project:
    staging:
      +elementary_enabled: true
      # ã‚¹ã‚­ãƒ¼ãƒå¤‰æ›´ã‚’è‡ªå‹•æ¤œçŸ¥
      +elementary_schema_changes: true
```

##### GitHub Actionsã§ã®å®Ÿè¡Œ
```yaml
# .github/workflows/elementary-monitor.yml
name: Elementary Data Quality Monitor

on:
  schedule:
    - cron: '0 */6 * * *'  # 6æ™‚é–“ã”ã¨
  workflow_dispatch:
  push:
    branches: [main]
    paths:
      - 'dbt/**'

jobs:
  dbt-test-and-monitor:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install dbt-duckdb
          pip install elementary-data

      - name: Install dbt packages
        working-directory: dbt
        run: dbt deps

      - name: Run dbt models
        working-directory: dbt
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: |
          dbt run
          dbt test

      - name: Run Elementary monitoring
        working-directory: dbt
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: |
          # Elementary ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åé›†ï¼‰
          dbt run --select elementary

          # Elementary ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
          edr monitor --slack-webhook ${{ secrets.SLACK_WEBHOOK_URL }}

      - name: Generate Elementary Report
        working-directory: dbt
        run: |
          # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
          edr report

      - name: Upload Elementary Report
        uses: actions/upload-artifact@v3
        with:
          name: elementary-report
          path: dbt/elementary_report.html

      - name: Deploy Report to Cloudflare Pages
        uses: cloudflare/wrangler-action@v3
        with:
          apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          accountId: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          command: pages deploy dbt/elementary_output --project-name=data-quality-dashboard
```

##### Slackã¨ã®çµ±åˆ
```bash
# Elementary CLIã§Slacké€šçŸ¥ã‚’è¨­å®š
edr monitor \
  --slack-webhook $SLACK_WEBHOOK_URL \
  --slack-channel data-quality \
  --timezone UTC

# ã¾ãŸã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†
# dbt/elementary_config.yml
slack:
  webhook_url: ${SLACK_WEBHOOK_URL}
  channel: data-quality
  workflows:
    - name: daily_monitor
      schedule: '0 8 * * *'
      alerts:
        - test_failures
        - schema_changes
        - volume_anomalies
```

##### Cloudflare Workersã§ã®é€šçŸ¥
```javascript
// workers/elementary-webhook.js
// Elementaryã‹ã‚‰ã®Webhookã‚’å—ã‘ã¦Cloudflareç’°å¢ƒã§å‡¦ç†

export default {
  async fetch(request, env) {
    if (request.method !== 'POST') {
      return new Response('Method not allowed', { status: 405 });
    }

    const alert = await request.json();

    // D1ã«ã‚¢ãƒ©ãƒ¼ãƒˆå±¥æ­´ã‚’ä¿å­˜
    await env.DB.prepare(`
      INSERT INTO data_quality_alerts (
        alert_type,
        model_name,
        test_name,
        status,
        severity,
        message,
        timestamp
      ) VALUES (?, ?, ?, ?, ?, ?, ?)
    `).bind(
      alert.type,
      alert.model,
      alert.test,
      alert.status,
      alert.severity,
      alert.message,
      new Date().toISOString()
    ).run();

    // Slackã«é€šçŸ¥
    await fetch(env.SLACK_WEBHOOK_URL, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        text: `ğŸš¨ Data Quality Alert: ${alert.message}`,
        blocks: [
          {
            type: 'header',
            text: {
              type: 'plain_text',
              text: `${alert.severity === 'high' ? 'ğŸ”´' : 'âš ï¸'} ${alert.type}`
            }
          },
          {
            type: 'section',
            fields: [
              {
                type: 'mrkdwn',
                text: `*Model:*\n${alert.model}`
              },
              {
                type: 'mrkdwn',
                text: `*Test:*\n${alert.test}`
              },
              {
                type: 'mrkdwn',
                text: `*Status:*\n${alert.status}`
              },
              {
                type: 'mrkdwn',
                text: `*Severity:*\n${alert.severity}`
              }
            ]
          },
          {
            type: 'section',
            text: {
              type: 'mrkdwn',
              text: `*Message:*\n${alert.message}`
            }
          },
          {
            type: 'actions',
            elements: [
              {
                type: 'button',
                text: {
                  type: 'plain_text',
                  text: 'View Report'
                },
                url: 'https://data-quality-dashboard.pages.dev'
              }
            ]
          }
        ]
      })
    });

    return new Response('Alert processed', { status: 200 });
  }
}
```

##### R2ã§ã®Elementaryãƒ¬ãƒãƒ¼ãƒˆæ°¸ç¶šåŒ–
```python
# scripts/upload_elementary_report.py
import boto3
from datetime import datetime

def upload_report_to_r2(report_path: str):
    """
    Elementaryãƒ¬ãƒãƒ¼ãƒˆã‚’R2ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    s3_client = boto3.client(
        's3',
        endpoint_url=f"https://{R2_ACCOUNT_ID}.r2.cloudflarestorage.com",
        aws_access_key_id=R2_ACCESS_KEY_ID,
        aws_secret_access_key=R2_SECRET_ACCESS_KEY,
        region_name='auto'
    )

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    s3_key = f"elementary/reports/{timestamp}/report.html"

    s3_client.upload_file(
        report_path,
        'data-lake-gold',  # Gold Layer
        s3_key,
        ExtraArgs={'ContentType': 'text/html'}
    )

    print(f"Report uploaded to: s3://data-lake-gold/{s3_key}")
```

#### Elementary CLI ä¸»è¦ã‚³ãƒãƒ³ãƒ‰

```bash
# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
edr report

# ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œï¼ˆSlacké€šçŸ¥ä»˜ãï¼‰
edr monitor --slack-webhook <webhook-url>

# ãƒ¬ãƒãƒ¼ãƒˆã‚’Webã‚µãƒ¼ãƒãƒ¼ã§èµ·å‹•
edr report --serve

# ç‰¹å®šæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ
edr report --days-back 7

# ãƒ¬ãƒãƒ¼ãƒˆã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
edr report \
  --project-dir ./dbt \
  --profiles-dir ./dbt \
  --profile-target prod \
  --output ./reports
```

#### Elementary ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®æ©Ÿèƒ½

1. **ãƒ†ã‚¹ãƒˆçµæœ**: ã™ã¹ã¦ã®dbtãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œå±¥æ­´ã¨çµæœ
2. **ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ**: ãƒ¢ãƒ‡ãƒ«ã®ãƒ“ãƒ«ãƒ‰æ™‚é–“ã€æˆåŠŸç‡ã€ã‚¨ãƒ©ãƒ¼
3. **ã‚¹ã‚­ãƒ¼ãƒå¤‰æ›´**: ã‚«ãƒ©ãƒ ã®è¿½åŠ ãƒ»å‰Šé™¤ãƒ»å‹å¤‰æ›´ã®å±¥æ­´
4. **ãƒ‡ãƒ¼ã‚¿ãƒªãƒãƒ¼ã‚¸ãƒ¥**: ãƒ¢ãƒ‡ãƒ«é–“ã®ä¾å­˜é–¢ä¿‚ã‚°ãƒ©ãƒ•
5. **ç•°å¸¸æ¤œçŸ¥**: æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥çµæœ
6. **ã‚«ãƒãƒ¬ãƒƒã‚¸**: ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã®å¯è¦–åŒ–

#### Cloudflare Pagesã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ä¾‹

```yaml
# .github/workflows/deploy-elementary-ui.yml
name: Deploy Elementary UI

on:
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install dbt-duckdb elementary-data

      - name: Generate Elementary report
        working-directory: dbt
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: |
          dbt deps
          dbt run --select elementary
          edr report --file-path ../elementary_report.html

      - name: Deploy to Cloudflare Pages
        uses: cloudflare/wrangler-action@v3
        with:
          apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          accountId: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          command: pages deploy elementary_output --project-name=elementary-dashboard
```

#### æ¨å¥¨åº¦
â­â­â­â­â­ - dbtã‚’ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿åŸºç›¤ã§ã¯å¿…é ˆã®ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–ãƒ„ãƒ¼ãƒ«

---

### Great Expectations

**å…¬å¼ã‚µã‚¤ãƒˆ**: https://greatexpectations.io/
**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: https://docs.greatexpectations.io/

#### æ¦‚è¦
Great Expectationsã¯ã€ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’æ¤œè¨¼ã—ã€ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã—ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–ã™ã‚‹Pythonãƒ™ãƒ¼ã‚¹ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚ã€ŒæœŸå¾…å€¤ï¼ˆExpectationsï¼‰ã€ã‚’å®šç¾©ã™ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿ãŒæœŸå¾…é€šã‚Šã§ã‚ã‚‹ã“ã¨ã‚’ç¶™ç¶šçš„ã«æ¤œè¨¼ã—ã¾ã™ã€‚

#### ãƒ‡ãƒ¼ã‚¿åŸºç›¤ã§ã®å½¹å‰²
- **ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼**: ãƒ‡ãƒ¼ã‚¿ãŒå®šç¾©ã•ã‚ŒãŸæœŸå¾…å€¤ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹æ¤œè¨¼
- **ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°**: ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±ã‚’è‡ªå‹•ç”Ÿæˆ
- **Data Docs**: HTMLãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè‡ªå‹•ç”Ÿæˆ
- **ã‚¢ãƒ©ãƒ¼ãƒˆ**: æ¤œè¨¼å¤±æ•—æ™‚ã®Slack/ãƒ¡ãƒ¼ãƒ«é€šçŸ¥
- **ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†**: æœŸå¾…å€¤å®šç¾©ã‚’Gitã§ç®¡ç†
- **R2çµ±åˆ**: DuckDBçµŒç”±ã§R2ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥æ¤œè¨¼

#### Cloudflareã¨ã®çµ±åˆ

##### R2 + DuckDB ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š

```yaml
# great_expectations/great_expectations.yml
datasources:
  r2_bronze:
    class_name: Datasource
    execution_engine:
      class_name: SqlAlchemyExecutionEngine
      connection_string: duckdb:///:memory:
    data_connectors:
      r2_parquet_connector:
        class_name: InferredAssetFilesystemDataConnector
        base_directory: /tmp/gx_data/bronze/
        default_regex:
          pattern: (.+)/(.+)\.parquet
```

##### R2ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ï¼ˆPythonï¼‰

```python
import os
import duckdb
import great_expectations as gx

# DuckDB + R2æ¥ç¶š
conn = duckdb.connect(":memory:")
conn.execute("INSTALL httpfs; LOAD httpfs;")
conn.execute(f"SET s3_endpoint='{os.getenv('R2_ENDPOINT')}';")
conn.execute(f"SET s3_access_key_id='{os.getenv('R2_ACCESS_KEY_ID')}';")
conn.execute(f"SET s3_secret_access_key='{os.getenv('R2_SECRET_ACCESS_KEY')}';")

# R2ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = conn.execute("""
    SELECT * FROM read_parquet('s3://my-bucket/data/**/*.parquet')
""").fetchdf()

# Great Expectations Context
context = gx.get_context()

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ¤œè¨¼
validator = context.sources.pandas_default.read_dataframe(df)

# Expectationsã‚’å®šç¾©
validator.expect_table_row_count_to_be_between(min_value=1, max_value=1000000)
validator.expect_column_values_to_not_be_null(column="user_id")
validator.expect_column_values_to_be_unique(column="email")
validator.expect_column_values_to_match_regex(
    column="email",
    regex="^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$"
)

# æ¤œè¨¼å®Ÿè¡Œ
results = validator.validate()

if not results["success"]:
    print("âŒ Validation failed!")
else:
    print("âœ… All validations passed!")
```

##### Expectation Suiteï¼ˆJSONå®šç¾©ï¼‰

```json
{
  "expectation_suite_name": "api_users_suite",
  "expectations": [
    {
      "expectation_type": "expect_column_values_to_not_be_null",
      "kwargs": {
        "column": "user_id"
      }
    },
    {
      "expectation_type": "expect_column_values_to_be_unique",
      "kwargs": {
        "column": "user_id"
      }
    },
    {
      "expectation_type": "expect_column_values_to_match_regex",
      "kwargs": {
        "column": "email",
        "regex": "^[a-zA-Z0-9._%+-]+@.*"
      }
    },
    {
      "expectation_type": "expect_column_values_to_be_between",
      "kwargs": {
        "column": "age",
        "min_value": 0,
        "max_value": 120
      }
    }
  ]
}
```

##### Checkpointè¨­å®šï¼ˆè¤‡æ•°æ¤œè¨¼ã®å®Ÿè¡Œï¼‰

```yaml
# great_expectations/checkpoints/daily_checkpoint.yml
name: daily_data_quality_checkpoint
config_version: 1.0
class_name: Checkpoint

validations:
  - batch_request:
      datasource_name: r2_bronze
      data_asset_name: api_posts
    expectation_suite_name: api_posts_suite

  - batch_request:
      datasource_name: r2_bronze
      data_asset_name: api_users
    expectation_suite_name: api_users_suite

action_list:
  - name: store_validation_result
    action:
      class_name: StoreValidationResultAction
  - name: update_data_docs
    action:
      class_name: UpdateDataDocsAction
  - name: send_slack_notification
    action:
      class_name: SlackNotificationAction
      slack_webhook: ${SLACK_WEBHOOK_URL}
      notify_on: failure
```

##### GitHub Actionsã§ã®è‡ªå‹•æ¤œè¨¼

```yaml
# .github/workflows/great-expectations.yml
name: Great Expectations Data Validation

on:
  schedule:
    - cron: '0 2 * * *'  # æ¯æ—¥2:00 UTC
  workflow_dispatch:

jobs:
  validate-data:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Great Expectations
        run: |
          pip install great-expectations==0.18.12
          pip install duckdb

      - name: Run validation
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: |
          python scripts/run_great_expectations.py

      - name: Deploy Data Docs to Cloudflare Pages
        uses: cloudflare/wrangler-action@v3
        with:
          apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          command: pages deploy great_expectations/uncommitted/data_docs --project-name=gx-data-docs
```

##### Cloudflare Pagesã¸ã®Data Docsãƒ‡ãƒ—ãƒ­ã‚¤

```bash
# Data Docsç”Ÿæˆ
great_expectations docs build

# Cloudflare Pagesã«ãƒ‡ãƒ—ãƒ­ã‚¤
wrangler pages deploy great_expectations/uncommitted/data_docs/cloudflare_pages_site \
  --project-name=gx-data-docs \
  --branch=main

# ã‚¢ã‚¯ã‚»ã‚¹: https://gx-data-docs.pages.dev
```

##### Cloudflare Workersçµ±åˆï¼ˆã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ©ãƒ¼ãƒˆï¼‰

```javascript
// workers/gx-alert-handler.js
// Great Expectationsã‹ã‚‰ã®Webhookã‚’å‡¦ç†

export default {
  async fetch(request, env) {
    if (request.method !== 'POST') {
      return new Response('Method not allowed', { status: 405 });
    }

    const validation = await request.json();

    // D1ã«æ¤œè¨¼çµæœã‚’ä¿å­˜
    await env.DB.prepare(`
      INSERT INTO data_quality_validations (
        suite_name,
        success,
        statistics,
        timestamp
      ) VALUES (?, ?, ?, ?)
    `).bind(
      validation.meta.expectation_suite_name,
      validation.success,
      JSON.stringify(validation.statistics),
      new Date().toISOString()
    ).run();

    // å¤±æ•—æ™‚ã¯Slacké€šçŸ¥
    if (!validation.success) {
      await fetch(env.SLACK_WEBHOOK_URL, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          text: `ğŸš¨ Data Quality Alert: ${validation.meta.expectation_suite_name}`,
          blocks: [
            {
              type: 'section',
              text: {
                type: 'mrkdwn',
                text: `*Failed Expectations:* ${validation.statistics.unsuccessful_expectations}\n*Success Rate:* ${validation.statistics.success_percent}%`
              }
            }
          ]
        })
      });
    }

    return new Response('Validation processed', { status: 200 });
  }
}
```

#### ä¸»è¦ãªExpectation Types

```python
# ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ¬ãƒ™ãƒ«
expect_table_row_count_to_be_between(min_value=1, max_value=1000000)
expect_table_columns_to_match_ordered_list(column_list=["id", "name", "email"])

# ã‚«ãƒ©ãƒ å­˜åœ¨ç¢ºèª
expect_column_to_exist(column="user_id")

# NULLå€¤
expect_column_values_to_not_be_null(column="email")

# ãƒ¦ãƒ‹ãƒ¼ã‚¯æ€§
expect_column_values_to_be_unique(column="email")

# ãƒ‡ãƒ¼ã‚¿å‹
expect_column_values_to_be_in_type_list(column="age", type_list=["INTEGER"])

# å€¤ã®ç¯„å›²
expect_column_values_to_be_between(column="price", min_value=0, max_value=10000)
expect_column_values_to_be_in_set(column="status", value_set=["active", "inactive"])

# æ–‡å­—åˆ—
expect_column_value_lengths_to_be_between(column="title", min_value=1, max_value=500)
expect_column_values_to_match_regex(column="phone", regex="^\\d{3}-\\d{4}-\\d{4}$")

# çµ±è¨ˆ
expect_column_mean_to_be_between(column="score", min_value=50, max_value=100)
expect_column_median_to_be_between(column="age", min_value=20, max_value=60)
expect_column_stdev_to_be_between(column="price", min_value=0, max_value=1000)

# æ—¥ä»˜
expect_column_values_to_be_dateutil_parseable(column="created_at")
expect_column_values_to_be_increasing(column="timestamp")
```

#### Data Docsï¼ˆHTMLãƒ¬ãƒãƒ¼ãƒˆï¼‰

Great Expectationsã¯è‡ªå‹•çš„ã«HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ï¼š

**ç‰¹å¾´:**
- æ¤œè¨¼çµæœã®å¯è¦–åŒ–
- ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆçµ±è¨ˆæƒ…å ±ï¼‰
- Expectation Suiteä¸€è¦§
- å¤±æ•—ã—ãŸExpectationsã®è©³ç´°
- ã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒ™ãƒ«ï¼ˆéå»ã®æ¤œè¨¼çµæœï¼‰

**æ§‹é€ :**
```
data_docs/
â”œâ”€â”€ index.html                    # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
â”œâ”€â”€ expectations/                 # Expectation Suiteè©³ç´°
â”‚   â”œâ”€â”€ api_posts_suite.html
â”‚   â””â”€â”€ api_users_suite.html
â”œâ”€â”€ validations/                  # æ¤œè¨¼çµæœ
â”‚   â”œâ”€â”€ api_posts_suite/
â”‚   â”‚   â””â”€â”€ 20251226T020000.html
â”‚   â””â”€â”€ api_users_suite/
â”‚       â””â”€â”€ 20251226T020000.html
â””â”€â”€ static/                       # CSS/JS
```

#### Elementaryã¨Great Expectationsã®ä½¿ã„åˆ†ã‘

| è¦³ç‚¹ | Elementary | Great Expectations |
|------|-----------|-------------------|
| **ç„¦ç‚¹** | dbtç‰¹åŒ–ã€ãƒªãƒãƒ¼ã‚¸ãƒ¥ | æ±ç”¨ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ |
| **çµ±åˆ** | dbtãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ | Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒª |
| **ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ—** | ç•°å¸¸æ¤œçŸ¥ã€ã‚¹ã‚­ãƒ¼ãƒå¤‰æ›´ | æœŸå¾…å€¤ãƒ™ãƒ¼ã‚¹æ¤œè¨¼ |
| **ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°** | é™å®šçš„ | åŒ…æ‹¬çš„ |
| **å­¦ç¿’æ›²ç·š** | ä½ï¼ˆdbtçŸ¥è­˜ã§å¯ï¼‰ | ä¸­ï¼ˆPythonãŒå¿…è¦ï¼‰ |
| **ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹** | dbtãƒ¢ãƒ‡ãƒ«ã®ç›£è¦– | ç”Ÿãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã€EDA |

**æ¨å¥¨ã®çµ„ã¿åˆã‚ã›:**
- **Great Expectations**: Bronzeå±¤ï¼ˆç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰ã®æ¤œè¨¼
- **Elementary**: Silver/Goldå±¤ï¼ˆdbtãƒ¢ãƒ‡ãƒ«ï¼‰ã®ç›£è¦–

```mermaid
graph LR
    Raw[Raw Data in R2] -->|Great Expectations| Bronze[Bronze Layer]
    Bronze -->|dlt| Silver[Silver Layer]
    Silver -->|dbt + Elementary| Gold[Gold Layer]

    style Raw fill:#fdd
    style Bronze fill:#ffd
    style Silver fill:#dfd
    style Gold fill:#ddf
```

#### æ¨å¥¨åº¦
â­â­â­â­â­ - ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã«å¿…é ˆ

---

### dbt (data build tool)

**å…¬å¼ã‚µã‚¤ãƒˆ**: https://www.getdbt.com/
**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: https://docs.getdbt.com/

#### æ¦‚è¦
dbtã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚¦ã‚§ã‚¢ãƒã‚¦ã‚¹å†…ã§ã®ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚’SQLä¸­å¿ƒã«å®Ÿè¡Œã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ãƒ†ã‚¹ãƒˆã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–ï¼‰ã‚’ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã«é©ç”¨ã—ã¾ã™ã€‚

#### ãƒ‡ãƒ¼ã‚¿åŸºç›¤ã§ã®å½¹å‰²
- **SQLå¤‰æ›**: D1ã‚„R2ï¼ˆDuckDBçµŒç”±ï¼‰ã«å¯¾ã—ã¦SQLãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚’å®Ÿè¡Œ
- **ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒªãƒ³ã‚°**: ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ã€ä¸­é–“ã€ãƒãƒ¼ãƒˆå±¤ã‚’æ§‹é€ åŒ–
- **ãƒ‡ãƒ¼ã‚¿å“è³ª**: ãƒ†ã‚¹ãƒˆã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼
- **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ**: è‡ªå‹•çš„ã«ãƒ‡ãƒ¼ã‚¿ãƒªãƒãƒ¼ã‚¸ãƒ¥ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆ
- **ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«å‡¦ç†**: åŠ¹ç‡çš„ãªãƒãƒƒãƒå‡¦ç†

#### Cloudflareã¨ã®çµ±åˆ

##### DuckDB + dbt + R2
```yaml
# profiles.yml
default:
  target: prod
  outputs:
    prod:
      type: duckdb
      path: ':memory:'
      extensions:
        - httpfs  # R2ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Š
      settings:
        s3_endpoint: '<account-id>.r2.cloudflarestorage.com'
        s3_access_key_id: '{{ env_var("R2_ACCESS_KEY_ID") }}'
        s3_secret_access_key: '{{ env_var("R2_SECRET_ACCESS_KEY") }}'
```

##### dbtãƒ¢ãƒ‡ãƒ«ä¾‹
```sql
-- models/staging/stg_events.sql
{{
  config(
    materialized='incremental',
    unique_key='event_id'
  )
}}

SELECT
  event_id,
  user_id,
  event_type,
  event_timestamp,
  properties
FROM read_parquet('s3://my-bucket/raw/events/*.parquet')
{% if is_incremental() %}
WHERE event_timestamp > (SELECT MAX(event_timestamp) FROM {{ this }})
{% endif %}
```

#### GitHub Actionsã§ã®å®Ÿè¡Œ
```yaml
# .github/workflows/dbt.yml
name: dbt Run

on:
  schedule:
    - cron: '0 */6 * * *'  # 6æ™‚é–“ã”ã¨
  workflow_dispatch:

jobs:
  dbt-run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dbt
        run: pip install dbt-duckdb
      - name: Run dbt
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: |
          dbt deps
          dbt run
          dbt test
```

#### æ¨å¥¨åº¦
â­â­â­â­â­ - SQLä¸­å¿ƒã®ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã«å¿…é ˆ

---

### dlt (data load tool)

**å…¬å¼ã‚µã‚¤ãƒˆ**: https://dlthub.com/
**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: https://dlthub.com/docs/

#### æ¦‚è¦
dltã¯ã€Pythonãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®æŠ½å‡ºã¨ãƒ­ãƒ¼ãƒ‰ã‚’ç°¡ç´ åŒ–ã—ã€ã‚¹ã‚­ãƒ¼ãƒæ¨è«–ã¨é€²åŒ–ã‚’è‡ªå‹•åŒ–ã—ã¾ã™ã€‚

#### ãƒ‡ãƒ¼ã‚¿åŸºç›¤ã§ã®å½¹å‰²
- **ãƒ‡ãƒ¼ã‚¿æŠ½å‡º**: æ§˜ã€…ãªAPIã‚„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
- **ã‚¹ã‚­ãƒ¼ãƒç®¡ç†**: è‡ªå‹•ã‚¹ã‚­ãƒ¼ãƒæ¨è«–ã¨é€²åŒ–
- **ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰**: R2ã€D1ã¸ã®ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
- **çŠ¶æ…‹ç®¡ç†**: å¢—åˆ†ãƒ­ãƒ¼ãƒ‰ã®çŠ¶æ…‹ç®¡ç†
- **å‹å®‰å…¨**: Pythonã®å‹ãƒ’ãƒ³ãƒˆã«ã‚ˆã‚‹å®‰å…¨æ€§

#### Cloudflareã¨ã®çµ±åˆ

##### R2ã¸ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
```python
import dlt
from dlt.destinations import filesystem

# R2ã‚’S3äº’æ›ã§ä½¿ç”¨
pipeline = dlt.pipeline(
    pipeline_name="my_pipeline",
    destination=filesystem(
        bucket_url="s3://my-bucket",
        credentials={
            "aws_access_key_id": "your-r2-access-key",
            "aws_secret_access_key": "your-r2-secret-key",
            "endpoint_url": "https://<account-id>.r2.cloudflarestorage.com",
            "region_name": "auto"
        }
    ),
    dataset_name="raw_data"
)

# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
@dlt.resource
def get_api_data():
    # APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    response = requests.get("https://api.example.com/data")
    yield response.json()

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
pipeline.run(get_api_data())
```

##### Cloudflare Workersã§ã®å®Ÿè¡Œ
```python
# dltã‚’Workers Python Runtimeã§å®Ÿè¡Œ
# workers/etl_pipeline.py
from js import Response
import dlt

async def on_fetch(request):
    # dltãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
    pipeline = dlt.pipeline(
        pipeline_name="worker_pipeline",
        destination="duckdb",
        dataset_name="extracted_data"
    )

    info = pipeline.run(my_data_source())

    return Response.new(
        f"Pipeline completed: {info}",
        headers={"Content-Type": "text/plain"}
    )
```

##### GitHub Actionsã§ã®å®šæœŸå®Ÿè¡Œ
```yaml
# .github/workflows/dlt-extract.yml
name: dlt Extract

on:
  schedule:
    - cron: '0 * * * *'  # æ¯æ™‚å®Ÿè¡Œ
  workflow_dispatch:

jobs:
  extract:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dlt
        run: pip install dlt[filesystem]
      - name: Run dlt pipeline
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: python pipelines/extract_data.py
```

#### æ¨å¥¨åº¦
â­â­â­â­â­ - ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»ãƒ­ãƒ¼ãƒ‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«å¿…é ˆ

---

## ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

### Apache Iceberg

**å…¬å¼ã‚µã‚¤ãƒˆ**: https://iceberg.apache.org/
**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: https://iceberg.apache.org/docs/latest/

#### æ¦‚è¦
Apache Icebergã¯ã€å¤§è¦æ¨¡ãªåˆ†æç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã™ã€‚ACIDãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã€ã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒ™ãƒ«ã€ã‚¹ã‚­ãƒ¼ãƒé€²åŒ–ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

#### ãƒ‡ãƒ¼ã‚¿åŸºç›¤ã§ã®å½¹å‰²
- **ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: R2ä¸Šã§ã®ACIDãƒ†ãƒ¼ãƒ–ãƒ«ç®¡ç†
- **ã‚¹ã‚­ãƒ¼ãƒé€²åŒ–**: å¾Œæ–¹äº’æ›æ€§ã®ã‚ã‚‹ã‚¹ã‚­ãƒ¼ãƒå¤‰æ›´
- **ã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒ™ãƒ«**: éå»ã®ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹
- **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ç®¡ç†**: åŠ¹ç‡çš„ãªã‚¯ã‚¨ãƒªã®ãŸã‚ã®è‡ªå‹•ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°
- **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†**: R2 Data Catalogã¨ã®çµ±åˆ

#### Cloudflareã¨ã®çµ±åˆ

##### R2 Data Catalog + Iceberg
Cloudflare R2 Data Catalogã¯ã€Apache Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒãƒãƒ¼ã‚¸ãƒ‰ã‚«ã‚¿ãƒ­ã‚°ã§ã™ã€‚

```python
# PyIcebergã§R2 Data Catalogã‚’ä½¿ç”¨
from pyiceberg.catalog import load_catalog

catalog = load_catalog(
    "r2_catalog",
    **{
        "type": "rest",
        "uri": "https://api.cloudflare.com/client/v4/accounts/{account_id}/r2/buckets/{bucket_name}/catalog",
        "credential": "your-api-token"
    }
)

# ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
from pyiceberg.schema import Schema
from pyiceberg.types import NestedField, StringType, TimestampType, IntegerType

schema = Schema(
    NestedField(1, "event_id", StringType(), required=True),
    NestedField(2, "user_id", StringType(), required=True),
    NestedField(3, "event_type", StringType(), required=True),
    NestedField(4, "event_timestamp", TimestampType(), required=True),
    NestedField(5, "value", IntegerType(), required=False)
)

table = catalog.create_table(
    "analytics.events",
    schema=schema,
    location="s3://my-bucket/iceberg/events"
)
```

##### R2 SQLã§ã®ã‚¯ã‚¨ãƒª
```sql
-- Cloudflare R2 SQLã§Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚¯ã‚¨ãƒª
SELECT
  event_type,
  COUNT(*) as event_count,
  AVG(value) as avg_value
FROM analytics.events
WHERE event_timestamp >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY event_type
ORDER BY event_count DESC;
```

##### dbtã§ã®Icebergãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
```sql
-- models/marts/fct_daily_events.sql
{{
  config(
    materialized='incremental',
    file_format='iceberg',
    location_root='s3://my-bucket/iceberg/marts/'
  )
}}

SELECT
  DATE(event_timestamp) as event_date,
  event_type,
  COUNT(*) as event_count
FROM {{ ref('stg_events') }}
{% if is_incremental() %}
WHERE event_timestamp > (SELECT MAX(event_timestamp) FROM {{ this }})
{% endif %}
GROUP BY 1, 2
```

##### Cloudflare Pipelinesã¨ã®çµ±åˆ
Cloudflare Pipelinesã¯ã€Apache Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ç›´æ¥æ›¸ãè¾¼ã¿ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚

```javascript
// Workers Pipelineå®šç¾©
export default {
  async fetch(request, env) {
    const data = await request.json();

    // Pipelinesã¸ã®ã‚¤ãƒ™ãƒ³ãƒˆPOST
    await env.PIPELINE.write([{
      event_id: data.id,
      user_id: data.user,
      event_type: data.type,
      event_timestamp: new Date().toISOString(),
      value: data.value
    }]);

    return new Response("Event ingested", { status: 200 });
  }
}
```

#### æ¨å¥¨åº¦
â­â­â­â­â­ - R2ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ã‚¯ã«å¿…é ˆ

---

### DuckDB

**å…¬å¼ã‚µã‚¤ãƒˆ**: https://duckdb.org/
**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: https://duckdb.org/docs/

#### æ¦‚è¦
DuckDBã¯ã€OLAPï¼ˆåˆ†æå‡¦ç†ï¼‰ã«ç‰¹åŒ–ã—ãŸçµ„ã¿è¾¼ã¿å‹SQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã™ã€‚é«˜é€Ÿã§ã€Parquetã€CSVã€JSONãªã©æ§˜ã€…ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç›´æ¥ã‚¯ã‚¨ãƒªå¯èƒ½ã§ã™ã€‚

#### ãƒ‡ãƒ¼ã‚¿åŸºç›¤ã§ã®å½¹å‰²
- **ãƒ­ãƒ¼ã‚«ãƒ«åˆ†æ**: é–‹ç™ºç’°å¢ƒã§ã®ãƒ‡ãƒ¼ã‚¿åˆ†æ
- **R2ã‚¯ã‚¨ãƒª**: R2ä¸Šã®Parquet/CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥ã‚¯ã‚¨ãƒª
- **dbtå®Ÿè¡Œ**: dbt-duckdbã§ã®ãƒ‡ãƒ¼ã‚¿å¤‰æ›
- **ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ**: D1ã‚„R2ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»åˆ†æ
- **é«˜é€Ÿé›†è¨ˆ**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®é«˜é€Ÿåˆ†æ

#### Cloudflareã¨ã®çµ±åˆ

##### R2ãƒ‡ãƒ¼ã‚¿ã®ç›´æ¥ã‚¯ã‚¨ãƒª
```sql
-- DuckDBã‹ã‚‰R2ã®Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ã‚¨ãƒª
INSTALL httpfs;
LOAD httpfs;

SET s3_endpoint='<account-id>.r2.cloudflarestorage.com';
SET s3_access_key_id='your-access-key';
SET s3_secret_access_key='your-secret-key';
SET s3_region='auto';

-- R2ä¸Šã®Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ã‚¨ãƒª
SELECT
  event_date,
  COUNT(*) as events
FROM read_parquet('s3://my-bucket/events/**/*.parquet')
WHERE event_date >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY event_date
ORDER BY event_date;
```

##### Iceberg ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¯ã‚¨ãƒª
```sql
-- DuckDB 0.10.0+ã§Icebergã‚µãƒãƒ¼ãƒˆ
INSTALL iceberg;
LOAD iceberg;

-- Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚¯ã‚¨ãƒª
SELECT * FROM iceberg_scan('s3://my-bucket/iceberg/events/metadata/v1.metadata.json');
```

##### Pythonçµ±åˆ
```python
import duckdb

# R2ã¸ã®æ¥ç¶šè¨­å®š
con = duckdb.connect()
con.execute("""
    INSTALL httpfs;
    LOAD httpfs;
    SET s3_endpoint='<account-id>.r2.cloudflarestorage.com';
    SET s3_access_key_id='your-access-key';
    SET s3_secret_access_key='your-secret-key';
""")

# R2ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ã‚¨ãƒª
result = con.execute("""
    SELECT event_type, COUNT(*) as count
    FROM read_parquet('s3://my-bucket/events/*.parquet')
    GROUP BY event_type
""").fetchdf()

print(result)
```

##### dbt-duckdbçµ±åˆ
```yaml
# profiles.yml
my_project:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: 'analytics.duckdb'
      extensions:
        - httpfs
        - parquet
      settings:
        s3_endpoint: '<account-id>.r2.cloudflarestorage.com'
        s3_access_key_id: '{{ env_var("R2_ACCESS_KEY_ID") }}'
        s3_secret_access_key: '{{ env_var("R2_SECRET_ACCESS_KEY") }}'
```

#### æ¨å¥¨åº¦
â­â­â­â­â­ - R2ãƒ‡ãƒ¼ã‚¿åˆ†æã«å¿…é ˆ

---

## ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–

### Evidence.dev

**å…¬å¼ã‚µã‚¤ãƒˆ**: https://evidence.dev/
**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: https://docs.evidence.dev/

#### æ¦‚è¦
Evidence.devã¯ã€ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹ï¼ˆBIï¼‰ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚Markdownã§ãƒ¬ãƒãƒ¼ãƒˆã‚’è¨˜è¿°ã—ã€SQLã‚¯ã‚¨ãƒªã®çµæœã‚’åŸ‹ã‚è¾¼ã‚“ã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

#### ãƒ‡ãƒ¼ã‚¿åŸºç›¤ã§ã®å½¹å‰²
- **ãƒ‡ãƒ¼ã‚¿ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: SQLãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
- **ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ**: Markdown + SQLã§ã®ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
- **ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†**: Gitã§ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
- **ã‚»ãƒ«ãƒ•ã‚µãƒ¼ãƒ“ã‚¹BI**: é–‹ç™ºè€…å‘ã‘ã®BIç’°å¢ƒ
- **é™çš„ã‚µã‚¤ãƒˆç”Ÿæˆ**: Cloudflare Pagesã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤

#### Cloudflareã¨ã®çµ±åˆ

##### DuckDBæ¥ç¶šè¨­å®š
```yaml
# sources/r2_data.yaml
name: r2_data
type: duckdb
filename: :memory:

options:
  initStatements:
    - INSTALL httpfs
    - LOAD httpfs
    - SET s3_endpoint='<account-id>.r2.cloudflarestorage.com'
    - SET s3_access_key_id='{{ env.R2_ACCESS_KEY_ID }}'
    - SET s3_secret_access_key='{{ env.R2_SECRET_ACCESS_KEY }}'
```

##### Evidenceãƒšãƒ¼ã‚¸ä¾‹
```markdown
<!-- pages/daily-metrics.md -->
# Daily Metrics Dashboard

Last updated: {new Date().toISOString()}

## Events Overview

```sql events_by_type
SELECT
  DATE(event_timestamp) as date,
  event_type,
  COUNT(*) as count
FROM read_parquet('s3://my-bucket/events/**/*.parquet')
WHERE event_timestamp >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY date, event_type
ORDER BY date DESC
\```

<LineChart
  data={events_by_type}
  x=date
  y=count
  series=event_type
/>

## Top Users

```sql top_users
SELECT
  user_id,
  COUNT(*) as event_count
FROM read_parquet('s3://my-bucket/events/**/*.parquet')
WHERE event_timestamp >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY user_id
ORDER BY event_count DESC
LIMIT 10
\```

<DataTable data={top_users} />
```

##### Cloudflare Pagesã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤
```yaml
# .github/workflows/deploy-evidence.yml
name: Deploy Evidence to Cloudflare Pages

on:
  push:
    branches: [main]
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: '18'
      - name: Install dependencies
        run: npm install
      - name: Build Evidence
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: npm run build
      - name: Deploy to Cloudflare Pages
        uses: cloudflare/wrangler-action@v3
        with:
          apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          accountId: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          command: pages deploy build --project-name=analytics-dashboard
```

##### Evidence + Workers API
```javascript
// workers/analytics-api.js
export default {
  async fetch(request, env) {
    // Evidenceãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰API
    const url = new URL(request.url);

    if (url.pathname === '/api/metrics') {
      const result = await env.DB.prepare(`
        SELECT
          date,
          SUM(count) as total
        FROM daily_metrics
        WHERE date >= date('now', '-7 days')
        GROUP BY date
      `).all();

      return Response.json(result.results);
    }

    // é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Pagesã‹ã‚‰æä¾›
    return env.ASSETS.fetch(request);
  }
}
```

#### æ¨å¥¨åº¦
â­â­â­â­â­ - ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®BIãƒ„ãƒ¼ãƒ«ã¨ã—ã¦æœ€é©

---

## CI/CDãƒ»é–‹ç™ºåŸºç›¤

### GitHub

**å…¬å¼ã‚µã‚¤ãƒˆ**: https://github.com/
**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: https://docs.github.com/

#### ãƒ‡ãƒ¼ã‚¿åŸºç›¤ã§ã®å½¹å‰²
- **ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†**: ã™ã¹ã¦ã®ã‚³ãƒ¼ãƒ‰ã€è¨­å®šã€SQLã‚’Gitç®¡ç†
- **ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**: ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€ã‚¤ã‚·ãƒ¥ãƒ¼ç®¡ç†
- **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: READMEã€Wikiã§ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç®¡ç†
- **Secretsç®¡ç†**: GitHub Secretsã§èªè¨¼æƒ…å ±ã‚’å®‰å…¨ã«ç®¡ç†
- **CI/CDãƒˆãƒªã‚¬ãƒ¼**: GitHub Actionsã¨ã®çµ±åˆ

#### æ¨å¥¨ãƒªãƒã‚¸ãƒˆãƒªæ§‹é€ 
```
data-engineering-with-cloudflare/
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ workflows/           # GitHub Actions
â”‚   â”‚   â”œâ”€â”€ deploy-workers.yml
â”‚   â”‚   â”œâ”€â”€ dbt-run.yml
â”‚   â”‚   â”œâ”€â”€ dlt-extract.yml
â”‚   â”‚   â””â”€â”€ deploy-evidence.yml
â”‚   â””â”€â”€ dependabot.yml
â”œâ”€â”€ workers/                 # Cloudflare Workers
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ transformation/
â”‚   â””â”€â”€ api/
â”œâ”€â”€ pipelines/              # dltãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
â”‚   â”œâ”€â”€ sources/
â”‚   â””â”€â”€ destinations/
â”œâ”€â”€ dbt/                    # dbtãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ macros/
â”œâ”€â”€ evidence/               # Evidenceãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
â”‚   â”œâ”€â”€ pages/
â”‚   â””â”€â”€ sources/
â”œâ”€â”€ scripts/                # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ docs/                   # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
â””â”€â”€ wrangler.toml          # Wranglerè¨­å®š
```

#### æ¨å¥¨åº¦
â­â­â­â­â­ - ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ãƒ»ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«å¿…é ˆ

---

### GitHub Actions

**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: https://docs.github.com/en/actions

#### ãƒ‡ãƒ¼ã‚¿åŸºç›¤ã§ã®å½¹å‰²
- **CI/CD**: è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤ã¨ãƒ†ã‚¹ãƒˆ
- **ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ**: Cronãƒ™ãƒ¼ã‚¹ã®ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- **ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**: è¤‡æ•°ã‚¸ãƒ§ãƒ–ã®é€£æº
- **é€šçŸ¥**: Slackã¸ã®å®Ÿè¡Œçµæœé€šçŸ¥
- **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: Secretsã«ã‚ˆã‚‹èªè¨¼æƒ…å ±ç®¡ç†

#### ä¸»è¦ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¾‹

##### 1. Workersè‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤
```yaml
# .github/workflows/deploy-workers.yml
name: Deploy Workers

on:
  push:
    branches: [main]
    paths:
      - 'workers/**'
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: '18'
      - name: Install dependencies
        run: npm install
      - name: Deploy to Cloudflare
        uses: cloudflare/wrangler-action@v3
        with:
          apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          accountId: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          command: deploy workers/ingestion/index.js
      - name: Notify Slack
        if: always()
        uses: slackapi/slack-github-action@v1
        with:
          webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          payload: |
            {
              "text": "Workers deployment ${{ job.status }}",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Workers Deployment*\nStatus: ${{ job.status }}\nCommit: ${{ github.sha }}"
                  }
                }
              ]
            }
```

##### 2. ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆdlt + dbtï¼‰
```yaml
# .github/workflows/etl-pipeline.yml
name: ETL Pipeline

on:
  schedule:
    - cron: '0 */6 * * *'  # 6æ™‚é–“ã”ã¨
  workflow_dispatch:

jobs:
  extract:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dlt
        run: pip install dlt[filesystem]
      - name: Run extraction
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          API_KEY: ${{ secrets.SOURCE_API_KEY }}
        run: python pipelines/extract.py
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: extraction-logs
          path: logs/

  transform:
    needs: extract
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dbt
        run: pip install dbt-duckdb
      - name: Run dbt
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: |
          cd dbt
          dbt deps
          dbt run
          dbt test
      - name: Generate docs
        run: |
          cd dbt
          dbt docs generate
      - name: Upload dbt docs
        uses: actions/upload-artifact@v3
        with:
          name: dbt-docs
          path: dbt/target/

  notify:
    needs: [extract, transform]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Notify Slack
        uses: slackapi/slack-github-action@v1
        with:
          webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          payload: |
            {
              "text": "ETL Pipeline completed",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*ETL Pipeline*\nExtract: ${{ needs.extract.result }}\nTransform: ${{ needs.transform.result }}"
                  }
                }
              ]
            }
```

##### 3. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
```yaml
# .github/workflows/data-quality.yml
name: Data Quality Checks

on:
  schedule:
    - cron: '0 8 * * *'  # æ¯æœ8æ™‚
  workflow_dispatch:

jobs:
  quality-checks:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          pip install duckdb great-expectations
      - name: Run quality checks
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: python scripts/quality_checks.py
      - name: Notify on failure
        if: failure()
        uses: slackapi/slack-github-action@v1
        with:
          webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          payload: |
            {
              "text": "âš ï¸ Data quality checks failed!",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Data Quality Alert*\nQuality checks have failed. Please review the logs."
                  }
                },
                {
                  "type": "actions",
                  "elements": [
                    {
                      "type": "button",
                      "text": {
                        "type": "plain_text",
                        "text": "View Logs"
                      },
                      "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                    }
                  ]
                }
              ]
            }
```

#### æ¨å¥¨åº¦
â­â­â­â­â­ - CI/CDãƒ»ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«å¿…é ˆ

---

## é€šçŸ¥ãƒ»ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

### Slack

**å…¬å¼ã‚µã‚¤ãƒˆ**: https://slack.com/
**API ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: https://api.slack.com/

#### ãƒ‡ãƒ¼ã‚¿åŸºç›¤ã§ã®å½¹å‰²
- **ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥**: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¤±æ•—ã€ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œã®é€šçŸ¥
- **é€²æ—å ±å‘Š**: ETLå®Ÿè¡Œå®Œäº†ã€ãƒ‡ãƒ—ãƒ­ã‚¤æˆåŠŸã®é€šçŸ¥
- **ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°**: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®šæœŸå ±å‘Š
- **ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**: ãƒãƒ¼ãƒ å†…ã§ã®ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹è­°è«–
- **æ‰¿èªãƒ•ãƒ­ãƒ¼**: æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤å‰ã®æ‰¿èªãƒªã‚¯ã‚¨ã‚¹ãƒˆ

#### Cloudflareã¨ã®çµ±åˆ

##### Workersã‹ã‚‰ã®é€šçŸ¥
```javascript
// workers/slack-notifier.js
export default {
  async fetch(request, env) {
    const { webhook_url } = env;

    const alert = await request.json();

    const slackMessage = {
      text: `Alert: ${alert.title}`,
      blocks: [
        {
          type: "header",
          text: {
            type: "plain_text",
            text: alert.title
          }
        },
        {
          type: "section",
          text: {
            type: "mrkdwn",
            text: alert.message
          }
        },
        {
          type: "context",
          elements: [
            {
              type: "mrkdwn",
              text: `Timestamp: ${new Date().toISOString()}`
            }
          ]
        }
      ]
    };

    await fetch(webhook_url, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(slackMessage)
    });

    return new Response('Notification sent', { status: 200 });
  }
}
```

##### Workflowsã‹ã‚‰ã®é€šçŸ¥
```typescript
// workflows/etl-with-notifications.ts
import { WorkflowEntrypoint, WorkflowStep } from 'cloudflare:workers';

export class ETLWorkflow extends WorkflowEntrypoint {
  async run(event, step: WorkflowStep) {
    try {
      // ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
      const data = await step.do('extract', async () => {
        // æŠ½å‡ºå‡¦ç†
        return extractData();
      });

      // ãƒ‡ãƒ¼ã‚¿å¤‰æ›
      const transformed = await step.do('transform', async () => {
        return transformData(data);
      });

      // ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
      await step.do('load', async () => {
        await loadToR2(transformed);
      });

      // æˆåŠŸé€šçŸ¥
      await step.do('notify-success', async () => {
        await notifySlack({
          title: 'âœ… ETL Pipeline Succeeded',
          message: `Processed ${data.length} records`,
          color: 'good'
        });
      });

    } catch (error) {
      // ã‚¨ãƒ©ãƒ¼é€šçŸ¥
      await step.do('notify-error', async () => {
        await notifySlack({
          title: 'âŒ ETL Pipeline Failed',
          message: `Error: ${error.message}`,
          color: 'danger'
        });
      });

      throw error;
    }
  }
}
```

##### å®šæœŸãƒ¬ãƒãƒ¼ãƒˆ
```javascript
// workers/daily-report.js
export default {
  async scheduled(event, env) {
    // D1ã‹ã‚‰æ—¥æ¬¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
    const metrics = await env.DB.prepare(`
      SELECT
        COUNT(*) as total_events,
        COUNT(DISTINCT user_id) as unique_users,
        AVG(session_duration) as avg_session_duration
      FROM events
      WHERE DATE(created_at) = DATE('now', '-1 day')
    `).first();

    // Slackã«é€ä¿¡
    await fetch(env.SLACK_WEBHOOK_URL, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        text: 'Daily Metrics Report',
        blocks: [
          {
            type: 'header',
            text: {
              type: 'plain_text',
              text: 'ğŸ“Š Daily Metrics Report'
            }
          },
          {
            type: 'section',
            fields: [
              {
                type: 'mrkdwn',
                text: `*Total Events:*\n${metrics.total_events.toLocaleString()}`
              },
              {
                type: 'mrkdwn',
                text: `*Unique Users:*\n${metrics.unique_users.toLocaleString()}`
              },
              {
                type: 'mrkdwn',
                text: `*Avg Session:*\n${metrics.avg_session_duration.toFixed(2)}s`
              }
            ]
          },
          {
            type: 'actions',
            elements: [
              {
                type: 'button',
                text: {
                  type: 'plain_text',
                  text: 'View Dashboard'
                },
                url: 'https://analytics-dashboard.pages.dev'
              }
            ]
          }
        ]
      })
    });
  }
}
```

```toml
# wrangler.toml
[triggers]
crons = ["0 9 * * *"]  # æ¯æœ9æ™‚ã«å®Ÿè¡Œ
```

#### æ¨å¥¨åº¦
â­â­â­â­â­ - ãƒãƒ¼ãƒ é€šçŸ¥ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆã«å¿…é ˆ

---

## çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä¾‹

### ã‚·ãƒŠãƒªã‚ª1: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¤ãƒ™ãƒ³ãƒˆåˆ†æ

```
[ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆ]
     â”‚
     â–¼
[Cloudflare Workers] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                        â”‚
     â”œâ”€> [Analytics Engine]   â”‚ (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹)
     â”‚                        â”‚
     â””â”€> [Pipelines] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
              â”‚               â”‚
              â–¼               â”‚
         [R2 (Iceberg)]       â”‚
              â”‚               â”‚
              â–¼               â”‚
      [DuckDB/dbtå¤‰æ›]        â”‚
              â”‚               â”‚
              â–¼               â”‚
    [Evidence Dashboard] â—„â”€â”€â”€â”€â”˜
    (Cloudflare Pages)
         â”‚
         â–¼
    [Slacké€šçŸ¥]
```

**GitHub Actions:**
- dbtãƒ¢ãƒ‡ãƒ«ã®å®šæœŸå®Ÿè¡Œï¼ˆ6æ™‚é–“ã”ã¨ï¼‰
- ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆæ¯æ—¥ï¼‰
- Dashboardå†ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆå¤‰æ›´æ™‚ï¼‰

---

### ã‚·ãƒŠãƒªã‚ª2: ãƒãƒƒãƒETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```
[å¤–éƒ¨API]
     â”‚
     â–¼
[GitHub Actions + dlt] â”€â”€â”
     â”‚                   â”‚ (å®šæœŸå®Ÿè¡Œ)
     â–¼                   â”‚
[R2 Raw Layer]           â”‚
     â”‚                   â”‚
     â–¼                   â”‚
[dbt + DuckDB] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚                   â”‚
     â–¼                   â”‚
[R2 Iceberg Tables]      â”‚
     â”‚                   â”‚
     â”œâ”€> [R2 SQL]        â”‚ (ã‚¢ãƒ‰ãƒ›ãƒƒã‚¯åˆ†æ)
     â”‚                   â”‚
     â””â”€> [Evidence] â—„â”€â”€â”€â”€â”˜
              â”‚
              â–¼
         [Slacké€šçŸ¥]
```

**Workflow:**
1. dltãŒAPIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡º â†’ R2 Raw
2. dbtãŒDuckDBã§ãƒ‡ãƒ¼ã‚¿å¤‰æ› â†’ R2 Iceberg
3. EvidenceãŒæœ€æ–°ãƒ‡ãƒ¼ã‚¿ã§å†ãƒ“ãƒ«ãƒ‰ â†’ Pages
4. çµæœã‚’Slackã«é€šçŸ¥

---

### ã‚·ãƒŠãƒªã‚ª3: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆ†æåŸºç›¤

```
[è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹]
     â”‚
     â”œâ”€> [Workers] â”€â”€> [D1] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                                â”‚
     â””â”€> [dlt] â”€â”€> [R2 (Iceberg)] â”€â”€â”€â”€â”¤
                                      â”‚
                   [DuckDBçµ±åˆã‚¯ã‚¨ãƒª]  â”‚
                         â”‚            â”‚
                         â–¼            â”‚
                   [dbtå¤‰æ›] â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Evidence     â”‚
              â”‚  Dashboard     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”œâ”€> Users (é–²è¦§)
                    â””â”€> Slack (ã‚¢ãƒ©ãƒ¼ãƒˆ)
```

**ç‰¹å¾´:**
- D1: ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
- R2: å¤§å®¹é‡ãƒ­ã‚°ã€å±¥æ­´ãƒ‡ãƒ¼ã‚¿
- DuckDB: D1ã¨R2ã‚’æ¨ªæ–­ã‚¯ã‚¨ãƒª
- dbt: çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ãƒˆä½œæˆ
- Evidence: çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

---

## ã¾ã¨ã‚

### æ¨å¥¨ã‚¹ã‚¿ãƒƒã‚¯

| ãƒ¬ã‚¤ãƒ¤ãƒ¼ | ãƒ„ãƒ¼ãƒ« | ç†ç”± |
|---------|--------|------|
| ãƒ‡ãƒ¼ã‚¿æŠ½å‡º | dlt | ç°¡å˜ãªè¨­å®šã€è‡ªå‹•ã‚¹ã‚­ãƒ¼ãƒç®¡ç† |
| ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ | Cloudflare R2 + Iceberg | ã‚¨ã‚°ãƒ¬ã‚¹ç„¡æ–™ã€ACIDãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ |
| å¤‰æ› | dbt + DuckDB | SQLä¸­å¿ƒã€R2ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ |
| å¯è¦–åŒ– | Evidence.dev | ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã€Pagesçµ±åˆ |
| ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ | GitHub Actions / Workflows | CI/CDçµ±åˆã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ |
| é€šçŸ¥ | Slack | ãƒãƒ¼ãƒ é€£æºã€ã‚¢ãƒ©ãƒ¼ãƒˆ |
| ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç† | GitHub | å…¨ã¦ã®ã‚³ãƒ¼ãƒ‰ã‚’ä¸€å…ƒç®¡ç† |

### é–‹ç™ºãƒ•ãƒ­ãƒ¼

1. **é–‹ç™º**: ãƒ­ãƒ¼ã‚«ãƒ«ã§DuckDB + dbtã§é–‹ç™º
2. **ãƒ†ã‚¹ãƒˆ**: GitHub PRã§dbtãƒ†ã‚¹ãƒˆè‡ªå‹•å®Ÿè¡Œ
3. **ãƒ‡ãƒ—ãƒ­ã‚¤**: ãƒãƒ¼ã‚¸å¾Œã€GitHub Actionsã§è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤
4. **é‹ç”¨**: Workflowsã¾ãŸã¯Actionsã§ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ
5. **ç›£è¦–**: Slackã§é€šçŸ¥ã€Evidenceã§å¯è¦–åŒ–

---

## å‚è€ƒãƒªãƒ³ã‚¯

### dbt
- [å…¬å¼ã‚µã‚¤ãƒˆ](https://www.getdbt.com/)
- [ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.getdbt.com/)
- [dbt-duckdb](https://github.com/duckdb/dbt-duckdb)

### dlt
- [å…¬å¼ã‚µã‚¤ãƒˆ](https://dlthub.com/)
- [ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://dlthub.com/docs/)
- [GitHub](https://github.com/dlt-hub/dlt)

### DuckDB
- [å…¬å¼ã‚µã‚¤ãƒˆ](https://duckdb.org/)
- [ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://duckdb.org/docs/)
- [S3/R2çµ±åˆ](https://duckdb.org/docs/extensions/httpfs.html)

### Apache Iceberg
- [å…¬å¼ã‚µã‚¤ãƒˆ](https://iceberg.apache.org/)
- [ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://iceberg.apache.org/docs/latest/)
- [PyIceberg](https://py.iceberg.apache.org/)

### Evidence.dev
- [å…¬å¼ã‚µã‚¤ãƒˆ](https://evidence.dev/)
- [ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.evidence.dev/)
- [GitHub](https://github.com/evidence-dev/evidence)

### GitHub Actions
- [ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.github.com/en/actions)
- [Wrangler Action](https://github.com/cloudflare/wrangler-action)
- [Slack GitHub Action](https://github.com/slackapi/slack-github-action)

### Slack
- [API ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://api.slack.com/)
- [Incoming Webhooks](https://api.slack.com/messaging/webhooks)
- [Block Kit](https://api.slack.com/block-kit)

---

æœ€çµ‚æ›´æ–°: 2025å¹´12æœˆ25æ—¥
