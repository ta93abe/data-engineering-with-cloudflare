# é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒ„ãƒ¼ãƒ«çµ±åˆã‚¬ã‚¤ãƒ‰

OpenMetadataã€OpenLineageã€ãƒªãƒãƒ¼ã‚¹ETLã€DVCã€Debeziumã®åŒ…æ‹¬çš„å°å…¥ã‚¬ã‚¤ãƒ‰ã€‚

## ğŸ“‹ ç›®æ¬¡

1. [OpenMetadata & OpenLineage (ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ»ãƒªãƒãƒ¼ã‚¸)](#openmetadata--openlineage)
2. [ãƒªãƒãƒ¼ã‚¹ETL (ãƒ‡ãƒ¼ã‚¿æ´»æ€§åŒ–)](#ãƒªãƒãƒ¼ã‚¹etl)
3. [DVC (ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†)](#dvc-data-version-control)
4. [Debezium (Change Data Capture)](#debezium-cdc)

---

# OpenMetadata & OpenLineage

## æ¦‚è¦

### OpenMetadata
ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã€‚

**æ©Ÿèƒ½:**
- ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒ†ãƒ¼ãƒ–ãƒ«ã€ã‚«ãƒ©ãƒ ï¼‰
- ãƒ‡ãƒ¼ã‚¿ãƒªãƒãƒ¼ã‚¸è¿½è·¡
- ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
- ãƒ‡ãƒ¼ã‚¿å“è³ªç®¡ç†
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»ãƒãƒ¼ãƒ ç®¡ç†
- ã‚¿ã‚°ãƒ»ç”¨èªé›†

### OpenLineage
ãƒ‡ãƒ¼ã‚¿ãƒªãƒãƒ¼ã‚¸ã®æ¨™æº–åŒ–ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã€‚

**æ©Ÿèƒ½:**
- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œè¿½è·¡
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ã®ä¾å­˜é–¢ä¿‚
- ã‚¸ãƒ§ãƒ–å®Ÿè¡Œå±¥æ­´
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åé›†æ¨™æº–åŒ–

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              OpenMetadata Server (Dockerã§å®Ÿè¡Œ)              â”‚
â”‚  - Metadata Store (PostgreSQL/MySQL)                        â”‚
â”‚  - Elasticsearch (æ¤œç´¢)                                      â”‚
â”‚  - API Server                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–²
                           â”‚ Ingestion
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ dbt Metadata â”‚  â”‚ R2 Datasets â”‚  â”‚ D1 Tables       â”‚  â”‚
â”‚  â”‚ (models,     â”‚  â”‚ (parquet,   â”‚  â”‚ (SQLite         â”‚  â”‚
â”‚  â”‚  tests,      â”‚  â”‚  JSON)      â”‚  â”‚  tables)        â”‚  â”‚
â”‚  â”‚  docs)       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         OpenLineage Events (HTTP)                    â”‚  â”‚
â”‚  â”‚  - dbt run â†’ OpenLineage emit                       â”‚  â”‚
â”‚  â”‚  - Workers execution â†’ Lineage API                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. OpenMetadata Dockerèµ·å‹•

```bash
# docker-compose.yml
version: '3.8'

services:
  openmetadata-server:
    image: openmetadata/server:1.2.0
    ports:
      - "8585:8585"
    environment:
      - OPENMETADATA_CLUSTER_NAME=openmetadata
      - DB_DRIVER_CLASS=org.postgresql.Driver
      - DB_SCHEME=postgresql
      - DB_USER=openmetadata_user
      - DB_USER_PASSWORD=openmetadata_password
      - DB_HOST=postgresql
      - DB_PORT=5432
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
    depends_on:
      - postgresql
      - elasticsearch

  postgresql:
    image: postgres:15
    environment:
      POSTGRES_USER: openmetadata_user
      POSTGRES_PASSWORD: openmetadata_password
      POSTGRES_DB: openmetadata_db
    volumes:
      - postgres-data:/var/lib/postgresql/data

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.2
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    volumes:
      - es-data:/usr/share/elasticsearch/data

volumes:
  postgres-data:
  es-data:
```

```bash
docker-compose up -d
# OpenMetadata UI: http://localhost:8585
```

### 2. dbtçµ±åˆ

```yaml
# dbt/profiles.yml

cloudflare_data_platform:
  target: prod
  outputs:
    prod:
      type: duckdb
      path: ':memory:'
      # OpenMetadataè¨­å®š
      metadata:
        openmetadata:
          server_url: "http://localhost:8585"
          api_version: "v1"
          auth_provider: "no-auth"
```

```yaml
# dbt/dbt_project.yml

meta:
  openmetadata:
    service_name: "cloudflare-dbt"
    database: "duckdb"
```

### 3. dbtãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿

```bash
# OpenMetadata Ingestionè¨­å®š
# ingestion/dbt_metadata.yaml

source:
  type: dbt
  serviceName: cloudflare-dbt
  sourceConfig:
    config:
      type: DBT
      dbtConfigSource:
        dbtManifestFilePath: /path/to/dbt/target/manifest.json
        dbtCatalogFilePath: /path/to/dbt/target/catalog.json
        dbtRunResultsFilePath: /path/to/dbt/target/run_results.json

sink:
  type: metadata-rest
  config:
    api_endpoint: http://localhost:8585/api

workflowConfig:
  openMetadataServerConfig:
    hostPort: http://localhost:8585/api
    authProvider: no-auth
```

```bash
# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿å®Ÿè¡Œ
metadata ingest -c ingestion/dbt_metadata.yaml
```

### 4. R2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç™»éŒ²

```python
# scripts/register_r2_datasets.py

from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
    OpenMetadataConnection,
)
from metadata.ingestion.ometa.ometa_api import OpenMetadata
from metadata.generated.schema.entity.data.table import Table, Column, DataType

# OpenMetadataæ¥ç¶š
server_config = OpenMetadataConnection(hostPort="http://localhost:8585/api")
metadata = OpenMetadata(server_config)

# R2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç™»éŒ²
r2_table = Table(
    name="api_posts",
    databaseSchema="r2://data-lake-raw/sources/api_jsonplaceholder",
    columns=[
        Column(name="id", dataType=DataType.INT),
        Column(name="userId", dataType=DataType.INT),
        Column(name="title", dataType=DataType.STRING),
        Column(name="body", dataType=DataType.STRING),
    ],
    tableType="External",
)

metadata.create_or_update(r2_table)
print("R2 dataset registered in OpenMetadata")
```

## OpenLineageçµ±åˆ

### 1. dbt with OpenLineage

```yaml
# dbt/profiles.yml

cloudflare_data_platform:
  target: prod
  outputs:
    prod:
      type: duckdb
      path: ':memory:'
      # OpenLineageè¨­å®š
      openlineage:
        url: "http://localhost:5000"
        api_key: "${OPENLINEAGE_API_KEY}"
        namespace: "cloudflare_dbt"
```

### 2. Workerså®Ÿè¡Œæ™‚ã®Lineageè¨˜éŒ²

```javascript
// workers/data-processor/lineage.js

/**
 * OpenLineage ã‚¤ãƒ™ãƒ³ãƒˆé€ä¿¡
 */
export async function emitLineageEvent(env, runEvent) {
  const lineageEvent = {
    eventType: runEvent.eventType,  // START, COMPLETE, FAIL
    eventTime: new Date().toISOString(),
    run: {
      runId: runEvent.runId,
      facets: {
        processing: {
          bytesRead: runEvent.bytesRead,
          bytesWritten: runEvent.bytesWritten,
          recordsRead: runEvent.recordsRead,
          recordsWritten: runEvent.recordsWritten
        }
      }
    },
    job: {
      namespace: "cloudflare_workers",
      name: runEvent.jobName,
      facets: {
        documentation: {
          description: runEvent.description
        }
      }
    },
    inputs: runEvent.inputs.map(input => ({
      namespace: "r2",
      name: input.dataset,
      facets: {
        dataSource: {
          name: "r2",
          uri: `s3://${input.bucket}/${input.key}`
        }
      }
    })),
    outputs: runEvent.outputs.map(output => ({
      namespace: "r2",
      name: output.dataset,
      facets: {
        dataSource: {
          name: "r2",
          uri: `s3://${output.bucket}/${output.key}`
        },
        schema: {
          fields: output.schema
        }
      }
    })),
    producer: "cloudflare-workers/1.0"
  };

  await fetch(env.OPENLINEAGE_URL, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${env.OPENLINEAGE_API_KEY}`
    },
    body: JSON.stringify(lineageEvent)
  });
}

// ä½¿ç”¨ä¾‹
await emitLineageEvent(env, {
  eventType: 'START',
  runId: crypto.randomUUID(),
  jobName: 'transform-api-data',
  description: 'Transform API data from bronze to silver',
  inputs: [
    { dataset: 'api_posts_bronze', bucket: 'data-lake-raw', key: 'sources/api/posts.parquet' }
  ],
  outputs: [
    {
      dataset: 'api_posts_silver',
      bucket: 'data-lake-silver',
      key: 'transformed/posts.parquet',
      schema: [
        { name: 'post_id', type: 'INTEGER' },
        { name: 'user_id', type: 'INTEGER' },
        { name: 'title', type: 'STRING' }
      ]
    }
  ],
  bytesRead: 1024000,
  bytesWritten: 512000,
  recordsRead: 1000,
  recordsWritten: 1000
});
```

---

# ãƒªãƒãƒ¼ã‚¹ETL

## æ¦‚è¦

**ãƒªãƒãƒ¼ã‚¹ETL**: ãƒ‡ãƒ¼ã‚¿ã‚¦ã‚§ã‚¢ãƒã‚¦ã‚¹ã‹ã‚‰SaaS/æ¥­å‹™ã‚·ã‚¹ãƒ†ãƒ ã¸ãƒ‡ãƒ¼ã‚¿ã‚’åŒæœŸã€‚

**ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹:**
- R2ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ â†’ Salesforce CRM
- åˆ†æçµæœ â†’ Slacké€šçŸ¥
- é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ â†’ HubSpot ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°
- è£½å“åˆ©ç”¨çŠ¶æ³ â†’ Intercom ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆ

## ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³æ¯”è¼ƒ

| ãƒ„ãƒ¼ãƒ« | ç‰¹å¾´ | ã‚³ã‚¹ãƒˆ | Cloudflareçµ±åˆ |
|--------|------|--------|---------------|
| **Hightouch** | GUIã€å¤šæ•°ã®ã‚³ãƒã‚¯ã‚¿ã€SQLåŒæœŸ | $$$ | R2/DuckDBå¯¾å¿œ |
| **Census** | ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºå‘ã‘ã€ãƒªãƒƒãƒUI | $$$ | ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒã‚¯ã‚¿ |
| **Airbyte** | ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã€æ‹¡å¼µæ€§é«˜ | $ (ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ) | ã‚«ã‚¹ã‚¿ãƒ ã‚½ãƒ¼ã‚¹ä½œæˆå¯ |
| **è‡ªä½œ (Workers)** | å®Œå…¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ | $ (Workerså®Ÿè¡Œè²») | âœ… ãƒã‚¤ãƒ†ã‚£ãƒ– |

## æ¨å¥¨: Workers + Airbyte ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  R2 (ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ã‚¯)                            â”‚
â”‚  Silver/Gold Layer Parquet files                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Workers Cron (ãƒªãƒãƒ¼ã‚¹ETLã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿)              â”‚
â”‚  - DuckDBã§R2ã‚’ã‚¯ã‚¨ãƒª                                        â”‚
â”‚  - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆAPIå½¢å¼ã«å¤‰æ›                                     â”‚
â”‚  - ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ»ãƒªãƒˆãƒ©ã‚¤ç®¡ç†                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                    â–¼              â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Salesforce  â”‚  â”‚  HubSpot    â”‚
          â”‚   API       â”‚  â”‚    API      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Workerså®Ÿè£…

```javascript
// workers/reverse-etl/salesforce-sync.js

import duckdb from '@duckdb/duckdb-wasm';

export default {
  async scheduled(event, env, ctx) {
    ctx.waitUntil(syncToSalesforce(env));
  }
};

async function syncToSalesforce(env) {
  // 1. DuckDBã§R2ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ã‚¨ãƒª
  const conn = await duckdb.connect(':memory:');

  await conn.execute(`
    INSTALL httpfs;
    LOAD httpfs;
    SET s3_endpoint='${env.R2_ENDPOINT}';
    SET s3_access_key_id='${env.R2_ACCESS_KEY_ID}';
    SET s3_secret_access_key='${env.R2_SECRET_ACCESS_KEY}';
  `);

  const result = await conn.query(`
    SELECT
      email,
      first_name,
      last_name,
      company,
      annual_revenue,
      last_activity_date
    FROM read_parquet('s3://data-lake-gold/crm/enriched_leads.parquet')
    WHERE sync_to_salesforce = true
      AND last_synced_at < CURRENT_TIMESTAMP - INTERVAL '1' HOUR
    LIMIT 1000
  `);

  // 2. Salesforce APIã«é€ä¿¡
  const accessToken = await getSalesforceToken(env);

  for (const row of result.toArray()) {
    try {
      await upsertSalesforceContact(accessToken, {
        Email: row.email,
        FirstName: row.first_name,
        LastName: row.last_name,
        Company: row.company,
        AnnualRevenue: row.annual_revenue,
        LastActivityDate: row.last_activity_date
      });

      // åŒæœŸå®Œäº†ã‚’ãƒãƒ¼ã‚¯
      await markAsSynced(env, row.email);

    } catch (error) {
      console.error(`Failed to sync ${row.email}:`, error);
      await logSyncError(env, row.email, error.message);
    }
  }

  conn.close();
}

async function getSalesforceToken(env) {
  const response = await fetch('https://login.salesforce.com/services/oauth2/token', {
    method: 'POST',
    headers: { 'Content-Type': 'application/x-www-form-urlencoded' },
    body: new URLSearchParams({
      grant_type: 'client_credentials',
      client_id: env.SALESFORCE_CLIENT_ID,
      client_secret: env.SALESFORCE_CLIENT_SECRET
    })
  });

  const data = await response.json();
  return data.access_token;
}

async function upsertSalesforceContact(token, contact) {
  const response = await fetch(
    `https://yourinstance.salesforce.com/services/data/v58.0/sobjects/Contact/Email/${contact.Email}`,
    {
      method: 'PATCH',
      headers: {
        'Authorization': `Bearer ${token}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(contact)
    }
  );

  if (!response.ok) {
    throw new Error(`Salesforce API error: ${response.statusText}`);
  }

  return response.json();
}
```

### Airbyte ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ

```yaml
# docker-compose.yml (Airbyte)

version: '3.8'
services:
  airbyte:
    image: airbyte/airbyte:latest
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://airbyte:airbyte@db:5432/airbyte
    volumes:
      - airbyte-data:/data

  db:
    image: postgres:14
    environment:
      POSTGRES_USER: airbyte
      POSTGRES_PASSWORD: airbyte
      POSTGRES_DB: airbyte

volumes:
  airbyte-data:
```

ã‚«ã‚¹ã‚¿ãƒ R2ã‚½ãƒ¼ã‚¹ä½œæˆ:

```python
# airbyte-integrations/connectors/source-r2-duckdb/source.py

from airbyte_cdk.sources import AbstractSource
import duckdb

class SourceR2DuckDB(AbstractSource):
    def check_connection(self, logger, config) -> Tuple[bool, any]:
        try:
            conn = duckdb.connect(':memory:')
            conn.execute(f"SELECT * FROM read_parquet('{config['s3_path']}') LIMIT 1")
            return True, None
        except Exception as e:
            return False, e

    def streams(self, config):
        return [R2Stream(config)]
```

---

# DVC (Data Version Control)

## æ¦‚è¦

**DVC**: Git-like data/model versioning for ML/Data pipelines.

**æ©Ÿèƒ½:**
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ»ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®šç¾©ãƒ»å®Ÿè¡Œ
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
- ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ (R2å¯¾å¿œ)

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. DVCã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install dvc dvc-s3
```

### 2. R2ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰è¨­å®š

```bash
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆæœŸåŒ–
dvc init

# R2ãƒªãƒ¢ãƒ¼ãƒˆè¨­å®š
dvc remote add -d r2 s3://dvc-storage/data
dvc remote modify r2 endpointurl https://xxxxx.r2.cloudflarestorage.com
dvc remote modify r2 access_key_id $R2_ACCESS_KEY_ID
dvc remote modify r2 secret_access_key $R2_SECRET_ACCESS_KEY
```

### 3. ãƒ‡ãƒ¼ã‚¿ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°

```bash
# å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’DVCã§ç®¡ç†
dvc add data/raw/large_dataset.parquet

# Git add (ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿)
git add data/raw/large_dataset.parquet.dvc .gitignore

# R2ã«ãƒ—ãƒƒã‚·ãƒ¥
dvc push

# ä»–ã®ç’°å¢ƒã§ãƒ—ãƒ«
dvc pull
```

### 4. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®šç¾©

```yaml
# dvc.yaml

stages:
  download:
    cmd: python scripts/download_data.py
    outs:
      - data/raw/api_data.json

  transform:
    cmd: python scripts/transform.py
    deps:
      - data/raw/api_data.json
      - scripts/transform.py
    outs:
      - data/processed/transformed.parquet

  train:
    cmd: python scripts/train_model.py
    deps:
      - data/processed/transformed.parquet
      - scripts/train_model.py
    params:
      - train.epochs
      - train.batch_size
    outs:
      - models/model.joblib
    metrics:
      - metrics/train_metrics.json:
          cache: false
```

```bash
# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
dvc repro

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ
dvc metrics show
dvc metrics diff
```

### 5. ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒª

```bash
# ãƒ¢ãƒ‡ãƒ«ã‚’R2ã«ä¿å­˜
dvc add models/production_model.joblib
git add models/production_model.joblib.dvc
git commit -m "Add production model v1.2.0"
git tag -a "model-v1.2.0" -m "Production model version 1.2.0"
dvc push

# Workers AIã§ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
# (R2ã‹ã‚‰èª­ã¿å–ã‚Š â†’ æ¨è«–)
```

---

# Debezium (CDC)

## æ¦‚è¦

**Debezium**: Change Data Capture (CDC) for databases.

**æ©Ÿèƒ½:**
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¤‰æ›´ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚­ãƒ£ãƒ—ãƒãƒ£
- Kafka Connectçµ±åˆ
- è¤‡æ•°DBå¯¾å¿œ (PostgreSQL, MySQL, MongoDB, etc.)

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           PostgreSQL / MySQL (æœ¬ç•ªDB)                        â”‚
â”‚  - Write-Ahead Log (WAL) / Binary Log                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼ CDC
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Debezium Connector                              â”‚
â”‚  - WAL/BinLogèª­ã¿å–ã‚Š                                        â”‚
â”‚  - å¤‰æ›´ã‚¤ãƒ™ãƒ³ãƒˆç”Ÿæˆ                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Kafka / Redpanda                                â”‚
â”‚  - ãƒˆãƒ”ãƒƒã‚¯: dbserver1.public.users                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Cloudflare Workers (Kafka Consumer)                  â”‚
â”‚  - Kafka â†’ Workers Queues                                   â”‚
â”‚  - å¤‰æ›´ã‚’R2ã«æ›¸ãè¾¼ã¿                                          â”‚
â”‚  - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              R2 (ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ã‚¯)                                â”‚
â”‚  - CDC events stored as Parquet                             â”‚
â”‚  - Iceberg table format                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. Debezium + Kafka

```yaml
# docker-compose.yml

version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092

  debezium:
    image: debezium/connect:2.5
    depends_on:
      - kafka
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: debezium_configs
      OFFSET_STORAGE_TOPIC: debezium_offsets
      STATUS_STORAGE_TOPIC: debezium_statuses
```

### 2. PostgreSQL Connectorè¨­å®š

```json
{
  "name": "postgres-connector",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "tasks.max": "1",
    "database.hostname": "postgres",
    "database.port": "5432",
    "database.user": "debezium",
    "database.password": "debezium",
    "database.dbname": "production_db",
    "database.server.name": "dbserver1",
    "table.include.list": "public.users,public.orders,public.products",
    "plugin.name": "pgoutput",
    "publication.name": "dbz_publication",
    "slot.name": "debezium"
  }
}
```

```bash
# Connectorç™»éŒ²
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d @postgres-connector.json
```

### 3. Kafka â†’ Workersçµ±åˆ

```javascript
// workers/cdc-consumer/index.js

/**
 * Kafkaã‹ã‚‰CDCã‚¤ãƒ™ãƒ³ãƒˆã‚’å—ä¿¡ã—ã¦R2ã«ä¿å­˜
 */

import { Kafka } from 'kafkajs';

export default {
  async scheduled(event, env, ctx) {
    ctx.waitUntil(consumeCDCEvents(env));
  }
};

async function consumeCDCEvents(env) {
  const kafka = new Kafka({
    clientId: 'cloudflare-workers',
    brokers: [env.KAFKA_BROKER],
    sasl: {
      mechanism: 'plain',
      username: env.KAFKA_USERNAME,
      password: env.KAFKA_PASSWORD
    }
  });

  const consumer = kafka.consumer({ groupId: 'cdc-to-r2' });

  await consumer.connect();
  await consumer.subscribe({ topics: ['dbserver1.public.users'] });

  await consumer.run({
    eachMessage: async ({ topic, partition, message }) => {
      const event = JSON.parse(message.value.toString());

      // Debeziumã‚¤ãƒ™ãƒ³ãƒˆè§£æ
      const { op, before, after, ts_ms } = event.payload;

      // R2ã«ä¿å­˜
      await saveCDCEventToR2(env, {
        operation: op,  // c=create, u=update, d=delete
        table: topic,
        before: before,
        after: after,
        timestamp: new Date(ts_ms).toISOString()
      });
    }
  });
}

async function saveCDCEventToR2(env, cdcEvent) {
  const key = `cdc/${cdcEvent.table}/${cdcEvent.timestamp.split('T')[0]}/${crypto.randomUUID()}.json`;

  await env.CDC_BUCKET.put(key, JSON.stringify(cdcEvent), {
    httpMetadata: { contentType: 'application/json' },
    customMetadata: {
      operation: cdcEvent.operation,
      table: cdcEvent.table
    }
  });

  // Analytics Engineã«è¨˜éŒ²
  await env.ANALYTICS.writeDataPoint({
    blobs: ['cdc_event', cdcEvent.operation, cdcEvent.table],
    doubles: [1],
    indexes: [cdcEvent.timestamp]
  });
}
```

### 4. CDC â†’ Iceberg Table

```python
# scripts/cdc_to_iceberg.py

from pyiceberg.catalog import load_catalog
import duckdb

# Icebergã‚«ã‚¿ãƒ­ã‚°æ¥ç¶š
catalog = load_catalog("r2", **{
    "type": "rest",
    "uri": "http://localhost:8181",
    "s3.endpoint": os.getenv("R2_ENDPOINT"),
    "s3.access-key-id": os.getenv("R2_ACCESS_KEY_ID"),
    "s3.secret-access-key": os.getenv("R2_SECRET_ACCESS_KEY")
})

# CDCã‚¤ãƒ™ãƒ³ãƒˆã‚’èª­ã¿å–ã‚Š
conn = duckdb.connect(':memory:')
conn.execute("INSTALL httpfs; LOAD httpfs;")
conn.execute(f"SET s3_endpoint='{os.getenv('R2_ENDPOINT')}';")

events = conn.execute("""
    SELECT * FROM read_json_auto('s3://cdc-bucket/cdc/dbserver1.public.users/*/*/*.json')
    WHERE timestamp >= CURRENT_TIMESTAMP - INTERVAL '1' HOUR
""").fetchall()

# Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒãƒ¼ã‚¸
table = catalog.load_table("cdc.users")

for event in events:
    if event['operation'] == 'c':  # Create
        table.append(event['after'])
    elif event['operation'] == 'u':  # Update
        table.overwrite(event['after'])
    elif event['operation'] == 'd':  # Delete
        table.delete(event['before'])
```

---

## çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¾‹

### ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰: æœ¬ç•ªDB â†’ CDC â†’ R2 â†’ dbt â†’ Evidence

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL  â”‚ (æœ¬ç•ªãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³DB)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ WAL
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Debezium   â”‚ (CDC)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Kafka
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Workers   â”‚ (Kafka Consumer)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ R2 (Bronze) â”‚ (CDC Events)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ dbt
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ R2 (Silver) â”‚ (Transformed + PII Masked)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ dbt
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ R2 (Gold)   â”‚ (Business Metrics)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Evidence
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dashboard  â”‚ (BI Reporting)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Reverse ETL
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Salesforce  â”‚ (Operational System)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## GitHub Actionsçµ±åˆ

```yaml
# .github/workflows/data-ops.yml

name: Data Operations

on:
  workflow_dispatch:
  schedule:
    - cron: '0 */6 * * *'

jobs:
  dvc-pipeline:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup DVC
        run: |
          pip install dvc dvc-s3
          dvc remote modify r2 access_key_id ${{ secrets.R2_ACCESS_KEY_ID }}
          dvc remote modify r2 secret_access_key ${{ secrets.R2_SECRET_ACCESS_KEY }}

      - name: Pull data
        run: dvc pull

      - name: Run pipeline
        run: dvc repro

      - name: Push results
        run: dvc push

  reverse-etl:
    runs-on: ubuntu-latest
    needs: dvc-pipeline
    steps:
      - name: Trigger Workers Cron
        run: |
          curl -X POST https://reverse-etl.yourcompany.workers.dev/sync \
            -H "Authorization: Bearer ${{ secrets.WORKERS_API_KEY }}"
```

---

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. ãƒ‡ãƒ¼ã‚¿å“è³ª
- **Elementary**: dbtãƒ¢ãƒ‡ãƒ«ã®å“è³ªç›£è¦–
- **Great Expectations**: Bronzeå±¤ã®ç”Ÿãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
- **OpenMetadata**: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ãƒ»ãƒªãƒãƒ¼ã‚¸

### 2. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
- **PIIæ¤œå‡º**: å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§PIIè‡ªå‹•æ¤œå‡º
- **ãƒ‡ãƒ¼ã‚¿ãƒã‚¹ã‚­ãƒ³ã‚°**: Silverå±¤ã§ãƒã‚¹ã‚­ãƒ³ã‚°
- **Cloudflare Access**: ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä¿è­·

### 3. ã‚³ã‚¹ãƒˆæœ€é©åŒ–
- **Evidence**: R2ã‚³ã‚¹ãƒˆç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
- **DVC**: å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
- **Workers Cron**: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã¨ã—ã¦ã®ã‚³ã‚¹ãƒˆå‰Šæ¸›

### 4. é‹ç”¨ç›£è¦–
- **OpenLineage**: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼å¯è¦–åŒ–
- **Analytics Engine**: ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
- **Debezium**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¤‰æ›´è¿½è·¡

---

## å‚è€ƒãƒªãƒ³ã‚¯

- [OpenMetadata Docs](https://docs.open-metadata.org/)
- [OpenLineage](https://openlineage.io/)
- [Airbyte](https://airbyte.com/)
- [DVC Documentation](https://dvc.org/doc)
- [Debezium Documentation](https://debezium.io/documentation/)

---

æœ€çµ‚æ›´æ–°: 2025-12-26
