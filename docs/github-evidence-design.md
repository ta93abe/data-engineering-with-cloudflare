# GitHub ãƒ‡ãƒ¼ã‚¿ Evidence.dev å¯è¦–åŒ–è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: è¨­è¨ˆãƒ•ã‚§ãƒ¼ã‚º
**ä½œæˆæ—¥**: 2025-01-03
**å¯¾è±¡**: Evidence.dev ã«ã‚ˆã‚‹ GitHub ãƒ‡ãƒ¼ã‚¿ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰

## ğŸ“‹ ç›®æ¬¡

1. [æ¦‚è¦](#æ¦‚è¦)
2. [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
3. [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ](#ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ)
4. [ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š](#ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š)
5. [ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆ](#ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆ)
6. [ãƒšãƒ¼ã‚¸è©³ç´°è¨­è¨ˆ](#ãƒšãƒ¼ã‚¸è©³ç´°è¨­è¨ˆ)
7. [ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª](#ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª)
8. [ãƒ‡ãƒ—ãƒ­ã‚¤æˆ¦ç•¥](#ãƒ‡ãƒ—ãƒ­ã‚¤æˆ¦ç•¥)
9. [å®Ÿè£…è¨ˆç”»](#å®Ÿè£…è¨ˆç”»)

---

## æ¦‚è¦

### Evidence.dev ã¨ã¯

Evidence.dev ã¯ **ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®BI (Business Intelligence) ãƒ„ãƒ¼ãƒ«** ã§ã™ã€‚

**ç‰¹å¾´**:
- ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ + SQL ã§ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ
- Git ã§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
- Cloudflare Workers ã§ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°å¯èƒ½
- ãƒªã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒãƒ£ãƒ¼ãƒˆãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«
- é™çš„ã‚µã‚¤ãƒˆç”Ÿæˆ (SSG) ã§é«˜é€Ÿ

### ãªãœ Evidence.dev ã‹

| é …ç›® | Evidence.dev | å¾“æ¥ã®BIãƒ„ãƒ¼ãƒ« |
|-----|-------------|-------------|
| **ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†** | Git ã§ç®¡ç† | âœ— |
| **ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼** | PR ã§ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯èƒ½ | âœ— |
| **ãƒ‡ãƒ—ãƒ­ã‚¤** | Cloudflare Workers | å°‚ç”¨ã‚µãƒ¼ãƒãƒ¼ |
| **ã‚³ã‚¹ãƒˆ** | ç„¡æ–™ (Workers) | æœˆé¡èª²é‡‘ |
| **ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º** | å®Œå…¨åˆ¶å¾¡ | åˆ¶é™ã‚ã‚Š |
| **å­¦ç¿’ã‚³ã‚¹ãƒˆ** | Markdown + SQL | å°‚ç”¨UI |

### å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼

- **é–‹ç™ºãƒãƒ¼ãƒ **: ãƒªãƒã‚¸ãƒˆãƒªæ´»å‹•ã®å¯è¦–åŒ–
- **ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼**: Issue/PR ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- **ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼**: ãƒãƒ¼ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- **çµŒå–¶å±¤**: å…¨ä½“KPI

---

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

```mermaid
graph LR
    A[GitHub API] -->|Workers/dlt| B[R2 Raw Layer]
    B -->|dbt| C[R2 Staging Layer]
    C -->|dbt| D[R2 Marts Layer]
    D -->|DuckDB| E[Evidence.dev]
    E -->|Build| F[Workers + R2]
    F -->|Deploy| G[Cloudflare Workers]

    style E fill:#9f6,color:#000
    style G fill:#f96,color:#fff
```

### æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

| ãƒ¬ã‚¤ãƒ¤ãƒ¼ | æŠ€è¡“ |
|---------|------|
| **ãƒ‡ãƒ¼ã‚¿å–å¾—** | Cloudflare Workers / dlt |
| **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸** | Cloudflare R2 (Parquet) |
| **å¤‰æ›** | dbt + DuckDB |
| **å¯è¦–åŒ–** | Evidence.dev |
| **ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°** | Cloudflare Workers |

### ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³

```typescript
// Evidence.dev â†’ DuckDB â†’ R2
DuckDB.query(`
  SELECT *
  FROM read_parquet('s3://data-lake-raw/marts/github/fct_repository_activity.parquet')
`)
```

---

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ

### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
evidence/
â”œâ”€â”€ .evidence/
â”‚   â””â”€â”€ template/           # Evidence ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
â”œâ”€â”€ sources/
â”‚   â””â”€â”€ github.md          # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š (DuckDB + R2)
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ index.md           # ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸
â”‚   â”œâ”€â”€ overview.md        # å…¨ä½“ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
â”‚   â”œâ”€â”€ repositories/
â”‚   â”‚   â”œâ”€â”€ index.md       # ãƒªãƒã‚¸ãƒˆãƒªä¸€è¦§
â”‚   â”‚   â””â”€â”€ [repo].md      # ãƒªãƒã‚¸ãƒˆãƒªè©³ç´° (å‹•çš„ãƒ«ãƒ¼ãƒˆ)
â”‚   â”œâ”€â”€ issues-prs/
â”‚   â”‚   â”œâ”€â”€ issues.md      # Issue åˆ†æ
â”‚   â”‚   â””â”€â”€ pull-requests.md  # PR åˆ†æ
â”‚   â”œâ”€â”€ contributors/
â”‚   â”‚   â”œâ”€â”€ index.md       # ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚¿ãƒ¼ä¸€è¦§
â”‚   â”‚   â””â”€â”€ [user].md      # ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚¿ãƒ¼è©³ç´°
â”‚   â”œâ”€â”€ ci-cd.md           # CI/CD ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
â”‚   â””â”€â”€ growth.md          # æˆé•·ãƒ¡ãƒˆãƒªã‚¯ã‚¹
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ KPICard.svelte     # å†åˆ©ç”¨å¯èƒ½ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”œâ”€â”€ TrendChart.svelte
â”‚   â””â”€â”€ RepositoryTable.svelte
â”œâ”€â”€ static/
â”‚   â””â”€â”€ logo.png
â”œâ”€â”€ package.json
â”œâ”€â”€ evidence.config.yaml
â””â”€â”€ README.md
```

---

## ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š

### `sources/github.md`

```yaml
---
name: github
type: duckdb
options:
  filename: ':memory:'
  extensions:
    - httpfs
  settings:
    s3_endpoint: ${R2_ENDPOINT}
    s3_access_key_id: ${R2_ACCESS_KEY_ID}
    s3_secret_access_key: ${R2_SECRET_ACCESS_KEY}
    s3_region: auto
---

# GitHub Data Source

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¯ Cloudflare R2 ã«ä¿å­˜ã•ã‚ŒãŸ GitHub ãƒ‡ãƒ¼ã‚¿ã«æ¥ç¶šã—ã¾ã™ã€‚

## Available Tables

### Marts Layer

- `fct_repository_activity`: ãƒªãƒã‚¸ãƒˆãƒªåˆ¥ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- `fct_issue_lifecycle`: Issue ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«åˆ†æ
- `fct_pr_metrics`: Pull Request ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- `dim_repositories`: ãƒªãƒã‚¸ãƒˆãƒªãƒã‚¹ã‚¿
- `dim_contributors`: ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚¿ãƒ¼ãƒã‚¹ã‚¿
- `agg_daily_metrics`: æ—¥æ¬¡é›†è¨ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹

### SQL Helper

```sql repos
CREATE OR REPLACE VIEW repos AS
SELECT * FROM read_parquet('s3://data-lake-raw/marts/github/fct_repository_activity.parquet');
```

```sql daily
CREATE OR REPLACE VIEW daily AS
SELECT * FROM read_parquet('s3://data-lake-raw/marts/github/agg_daily_metrics.parquet');
```
```

### ç’°å¢ƒå¤‰æ•°è¨­å®š

**`.env`**:
```bash
R2_ENDPOINT=https://ACCOUNT_ID.r2.cloudflarestorage.com
R2_ACCESS_KEY_ID=your_access_key
R2_SECRET_ACCESS_KEY=your_secret_key
```

---

## ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆ

### ãƒšãƒ¼ã‚¸æ§‹æˆ

| ãƒšãƒ¼ã‚¸ | ãƒ‘ã‚¹ | ç›®çš„ |
|--------|------|------|
| **ãƒ›ãƒ¼ãƒ ** | `/` | ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸ã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ |
| **å…¨ä½“æ¦‚è¦** | `/overview` | å…¨ãƒªãƒã‚¸ãƒˆãƒªã®KPIã€ãƒˆãƒ¬ãƒ³ãƒ‰ |
| **ãƒªãƒã‚¸ãƒˆãƒªä¸€è¦§** | `/repositories` | å…¨ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ†ãƒ¼ãƒ–ãƒ« |
| **ãƒªãƒã‚¸ãƒˆãƒªè©³ç´°** | `/repositories/[repo]` | å€‹åˆ¥ãƒªãƒã‚¸ãƒˆãƒªã®è©³ç´°åˆ†æ |
| **Issueåˆ†æ** | `/issues-prs/issues` | Issue ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ãƒˆãƒ¬ãƒ³ãƒ‰ |
| **PRåˆ†æ** | `/issues-prs/pull-requests` | PR ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ãƒãƒ¼ã‚¸æ™‚é–“ |
| **ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚¿ãƒ¼** | `/contributors` | è²¢çŒ®åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° |
| **CI/CD** | `/ci-cd` | ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æˆåŠŸç‡ã€å®Ÿè¡Œæ™‚é–“ |
| **æˆé•·æŒ‡æ¨™** | `/growth` | Staræˆé•·ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¨ç§» |

### ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³

**ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ¡ãƒ‹ãƒ¥ãƒ¼**:
```
ğŸ“Š Overview
ğŸ“ Repositories
  â”œâ”€ All Repositories
  â””â”€ [Dynamic: Selected Repo]
ğŸ› Issues & PRs
  â”œâ”€ Issues
  â””â”€ Pull Requests
ğŸ‘¥ Contributors
âš™ï¸ CI/CD Performance
ğŸ“ˆ Growth Metrics
```

---

## ãƒšãƒ¼ã‚¸è©³ç´°è¨­è¨ˆ

### 1. Overview Dashboard (`pages/overview.md`)

**ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ**:

```markdown
# GitHub Analytics Overview

<DatePicker />

## Key Performance Indicators

<div class="grid grid-cols-4 gap-4">

```sql kpi_repos
SELECT
  COUNT(*) as total_repositories,
  SUM(CASE WHEN activity_status = 'Active' THEN 1 ELSE 0 END) as active_repos,
  SUM(stars) as total_stars,
  SUM(forks) as total_forks
FROM read_parquet('s3://data-lake-raw/marts/github/fct_repository_activity.parquet')
```

<BigValue data={kpi_repos} value=total_repositories label="Total Repositories" />
<BigValue data={kpi_repos} value=active_repos label="Active Repositories" />
<BigValue data={kpi_repos} value=total_stars label="Total Stars" />
<BigValue data={kpi_repos} value=total_forks label="Total Forks" />

</div>

## Activity Trends

```sql daily_activity
SELECT
  metric_date,
  issues_created,
  prs_merged,
  commits_made,
  new_stars
FROM read_parquet('s3://data-lake-raw/marts/github/agg_daily_metrics.parquet')
WHERE metric_date >= current_date - interval '90 days'
ORDER BY metric_date
```

<LineChart
  data={daily_activity}
  x=metric_date
  y={['issues_created', 'prs_merged', 'commits_made']}
  title="Daily Activity (Last 90 Days)"
/>

## Top Repositories by Stars

```sql top_repos
SELECT
  repository_full_name,
  stars,
  forks,
  open_issues,
  ci_success_rate
FROM read_parquet('s3://data-lake-raw/marts/github/fct_repository_activity.parquet')
ORDER BY stars DESC
LIMIT 10
```

<DataTable data={top_repos} />

## Repository Health Distribution

```sql health_dist
SELECT
  ci_health,
  COUNT(*) as repository_count
FROM read_parquet('s3://data-lake-raw/marts/github/fct_repository_activity.parquet')
GROUP BY ci_health
```

<BarChart
  data={health_dist}
  x=ci_health
  y=repository_count
  title="Repository Health Distribution"
/>
```

### 2. Repository List (`pages/repositories/index.md`)

```markdown
# All Repositories

```sql repos
SELECT
  repository_full_name,
  primary_language,
  stars,
  forks,
  open_issues,
  total_prs,
  ci_success_rate,
  popularity_tier,
  activity_status
FROM read_parquet('s3://data-lake-raw/marts/github/fct_repository_activity.parquet')
ORDER BY stars DESC
```

<DataTable
  data={repos}
  link=repository_full_name
  rows=50
  search=true
>
  <Column id=repository_full_name title="Repository" />
  <Column id=primary_language title="Language" />
  <Column id=stars title="Stars" fmt='#,##0' />
  <Column id=forks title="Forks" fmt='#,##0' />
  <Column id=open_issues title="Open Issues" />
  <Column id=ci_success_rate title="CI Success %" fmt='0.0%' />
  <Column id=activity_status title="Status" />
</DataTable>
```

### 3. Repository Detail (`pages/repositories/[repo].md`)

```markdown
---
queries:
  - repo_detail.sql
  - repo_issues.sql
  - repo_prs.sql
  - repo_commits.sql
---

# {params.repo}

```sql repo_info
SELECT *
FROM read_parquet('s3://data-lake-raw/marts/github/fct_repository_activity.parquet')
WHERE repository_full_name = '${params.repo}'
```

<Details data={repo_info}>

## Overview

<div class="grid grid-cols-4 gap-4">

<BigValue data={repo_info} value=stars label="Stars" />
<BigValue data={repo_info} value=forks label="Forks" />
<BigValue data={repo_info} value=total_issues label="Total Issues" />
<BigValue data={repo_info} value=total_prs label="Total PRs" />

</div>

## Issue Trends

```sql issue_trends
SELECT
  DATE_TRUNC('month', created_at) as month,
  COUNT(*) as issue_count,
  AVG(hours_to_close) / 24 as avg_days_to_close
FROM read_parquet('s3://data-lake-raw/marts/github/fct_issue_lifecycle.parquet')
WHERE repository_full_name = '${params.repo}'
GROUP BY month
ORDER BY month
```

<LineChart
  data={issue_trends}
  x=month
  y=issue_count
  y2=avg_days_to_close
  title="Issue Activity"
/>

## Pull Request Metrics

```sql pr_metrics
SELECT
  pr_size,
  COUNT(*) as pr_count,
  AVG(hours_to_merge) / 24 as avg_days_to_merge
FROM read_parquet('s3://data-lake-raw/marts/github/fct_pr_metrics.parquet')
WHERE repository_full_name = '${params.repo}' AND is_merged = true
GROUP BY pr_size
ORDER BY
  CASE pr_size
    WHEN 'XS' THEN 1
    WHEN 'S' THEN 2
    WHEN 'M' THEN 3
    WHEN 'L' THEN 4
    WHEN 'XL' THEN 5
  END
```

<BarChart
  data={pr_metrics}
  x=pr_size
  y=pr_count
  title="PR Size Distribution"
/>

## Top Contributors

```sql contributors
SELECT
  c.username,
  c.total_commits,
  c.total_prs_merged,
  c.total_issues_created
FROM read_parquet('s3://data-lake-raw/marts/github/dim_contributors.parquet') c
-- Filter by repository (éœ€è¦ commits/prs ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ JOIN)
ORDER BY c.activity_score DESC
LIMIT 10
```

<DataTable data={contributors} />

</Details>
```

### 4. Issue Analysis (`pages/issues-prs/issues.md`)

```markdown
# Issue Analysis

## Issue Lifecycle

```sql issue_lifecycle
SELECT
  resolution_time_bucket,
  COUNT(*) as issue_count,
  AVG(hours_to_close) / 24 as avg_days_to_close
FROM read_parquet('s3://data-lake-raw/marts/github/fct_issue_lifecycle.parquet')
WHERE state = 'closed'
GROUP BY resolution_time_bucket
ORDER BY
  CASE resolution_time_bucket
    WHEN '< 1 Day' THEN 1
    WHEN '1-7 Days' THEN 2
    WHEN '1-4 Weeks' THEN 3
    WHEN '> 1 Month' THEN 4
  END
```

<BarChart
  data={issue_lifecycle}
  x=resolution_time_bucket
  y=issue_count
  title="Issue Resolution Time Distribution"
/>

## Issue Type Breakdown

```sql issue_types
SELECT
  issue_type,
  COUNT(*) as count,
  AVG(hours_to_close) / 24 as avg_days_to_close
FROM read_parquet('s3://data-lake-raw/marts/github/fct_issue_lifecycle.parquet')
GROUP BY issue_type
```

<BarChart
  data={issue_types}
  x=issue_type
  y=count
  title="Issue Type Distribution"
/>

## Open vs Closed Trend

```sql issue_trend
SELECT
  DATE_TRUNC('week', created_at) as week,
  SUM(CASE WHEN state = 'open' THEN 1 ELSE 0 END) as open_count,
  SUM(CASE WHEN state = 'closed' THEN 1 ELSE 0 END) as closed_count
FROM read_parquet('s3://data-lake-raw/marts/github/fct_issue_lifecycle.parquet')
GROUP BY week
ORDER BY week
```

<LineChart
  data={issue_trend}
  x=week
  y={['open_count', 'closed_count']}
  title="Issue Open/Close Trend"
/>
```

### 5. Pull Request Analysis (`pages/issues-prs/pull-requests.md`)

```markdown
# Pull Request Analysis

## PR Merge Time Distribution

```sql pr_merge_time
SELECT
  merge_time_bucket,
  COUNT(*) as pr_count
FROM read_parquet('s3://data-lake-raw/marts/github/fct_pr_metrics.parquet')
WHERE is_merged = true
GROUP BY merge_time_bucket
ORDER BY
  CASE merge_time_bucket
    WHEN '< 1 Hour' THEN 1
    WHEN '1-24 Hours' THEN 2
    WHEN '1-7 Days' THEN 3
    WHEN '> 1 Week' THEN 4
  END
```

<BarChart
  data={pr_merge_time}
  x=merge_time_bucket
  y=pr_count
  title="PR Merge Time"
/>

## PR Size vs Merge Time

```sql pr_size_time
SELECT
  pr_size,
  AVG(hours_to_merge) as avg_hours_to_merge,
  COUNT(*) as pr_count
FROM read_parquet('s3://data-lake-raw/marts/github/fct_pr_metrics.parquet')
WHERE is_merged = true
GROUP BY pr_size
```

<ScatterPlot
  data={pr_size_time}
  x=pr_size
  y=avg_hours_to_merge
  size=pr_count
  title="PR Size vs Merge Time"
/>

## Merged vs Not Merged

```sql pr_status
SELECT
  is_merged,
  COUNT(*) as count
FROM read_parquet('s3://data-lake-raw/marts/github/fct_pr_metrics.parquet')
GROUP BY is_merged
```

<BarChart
  data={pr_status}
  x=is_merged
  y=count
  title="PR Status"
/>
```

### 6. Contributors (`pages/contributors/index.md`)

```markdown
# Top Contributors

```sql contributors
SELECT
  username,
  total_commits,
  total_prs_merged,
  total_issues_created,
  activity_score
FROM read_parquet('s3://data-lake-raw/marts/github/dim_contributors.parquet')
ORDER BY activity_score DESC
LIMIT 50
```

<DataTable data={contributors} link=username>
  <Column id=username title="Username" />
  <Column id=total_commits title="Commits" fmt='#,##0' />
  <Column id=total_prs_merged title="PRs Merged" fmt='#,##0' />
  <Column id=total_issues_created title="Issues Created" fmt='#,##0' />
  <Column id=activity_score title="Activity Score" fmt='#,##0' />
</DataTable>

## Activity Distribution

```sql activity_dist
SELECT
  CASE
    WHEN activity_score >= 1000 THEN 'Highly Active (1000+)'
    WHEN activity_score >= 100 THEN 'Active (100-999)'
    WHEN activity_score >= 10 THEN 'Casual (10-99)'
    ELSE 'New (0-9)'
  END as activity_level,
  COUNT(*) as contributor_count
FROM read_parquet('s3://data-lake-raw/marts/github/dim_contributors.parquet')
GROUP BY activity_level
```

<BarChart
  data={activity_dist}
  x=activity_level
  y=contributor_count
  title="Contributor Activity Distribution"
/>
```

### 7. CI/CD Performance (`pages/ci-cd.md`)

```markdown
# CI/CD Performance

## Overall Success Rate

```sql overall_ci
SELECT
  AVG(ci_success_rate) as avg_success_rate,
  SUM(total_workflow_runs) as total_runs
FROM read_parquet('s3://data-lake-raw/marts/github/fct_repository_activity.parquet')
WHERE total_workflow_runs > 0
```

<BigValue data={overall_ci} value=avg_success_rate label="Average CI Success Rate" fmt='0.0%' />

## Repository CI Health

```sql repo_ci
SELECT
  repository_full_name,
  ci_success_rate,
  total_workflow_runs,
  ci_health
FROM read_parquet('s3://data-lake-raw/marts/github/fct_repository_activity.parquet')
WHERE total_workflow_runs > 0
ORDER BY ci_success_rate DESC
```

<DataTable data={repo_ci} />

## CI Health Distribution

```sql health_dist
SELECT
  ci_health,
  COUNT(*) as repo_count
FROM read_parquet('s3://data-lake-raw/marts/github/fct_repository_activity.parquet')
WHERE total_workflow_runs > 0
GROUP BY ci_health
```

<BarChart
  data={health_dist}
  x=ci_health
  y=repo_count
  title="CI Health Distribution"
/>
```

### 8. Growth Metrics (`pages/growth.md`)

```markdown
# Growth Metrics

## Star Growth

```sql star_growth
SELECT
  metric_date,
  SUM(new_stars) OVER (ORDER BY metric_date) as cumulative_stars
FROM read_parquet('s3://data-lake-raw/marts/github/agg_daily_metrics.parquet')
ORDER BY metric_date
```

<LineChart
  data={star_growth}
  x=metric_date
  y=cumulative_stars
  title="Cumulative Star Growth"
/>

## Activity Score Trend

```sql activity_score
SELECT
  DATE_TRUNC('week', metric_date) as week,
  SUM(prs_merged * 3 + issues_closed * 2 + commits_made * 1) as weekly_activity_score
FROM read_parquet('s3://data-lake-raw/marts/github/agg_daily_metrics.parquet')
GROUP BY week
ORDER BY week
```

<LineChart
  data={activity_score}
  x=week
  y=weekly_activity_score
  title="Weekly Activity Score"
/>

## Contributor Growth

```sql contributor_growth
SELECT
  metric_date,
  SUM(active_contributors) OVER (ORDER BY metric_date) as cumulative_contributors
FROM read_parquet('s3://data-lake-raw/marts/github/agg_daily_metrics.parquet')
ORDER BY metric_date
```

<LineChart
  data={contributor_growth}
  x=metric_date
  y=cumulative_contributors
  title="Cumulative Contributor Growth"
/>
```

---

## ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª

### å†åˆ©ç”¨å¯èƒ½ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

#### 1. KPICard.svelte

```svelte
<script>
  export let title;
  export let value;
  export let change = null;
  export let format = '#,##0';
</script>

<div class="kpi-card">
  <h3>{title}</h3>
  <div class="value">{value}</div>
  {#if change}
    <div class="change" class:positive={change > 0} class:negative={change < 0}>
      {change > 0 ? 'â†‘' : 'â†“'} {Math.abs(change)}%
    </div>
  {/if}
</div>

<style>
  .kpi-card {
    padding: 1.5rem;
    border-radius: 8px;
    background: white;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
  }
  .value {
    font-size: 2rem;
    font-weight: bold;
    margin: 0.5rem 0;
  }
  .change.positive { color: green; }
  .change.negative { color: red; }
</style>
```

#### 2. TrendChart.svelte

å†åˆ©ç”¨å¯èƒ½ãªãƒˆãƒ¬ãƒ³ãƒ‰ãƒãƒ£ãƒ¼ãƒˆ (LineChart ã®ãƒ©ãƒƒãƒ‘ãƒ¼)

#### 3. RepositoryCard.svelte

ãƒªãƒã‚¸ãƒˆãƒªã‚µãƒãƒªãƒ¼ã‚«ãƒ¼ãƒ‰

---

## ãƒ‡ãƒ—ãƒ­ã‚¤æˆ¦ç•¥

### Cloudflare Workers + R2 ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°

Evidence.devã®é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’R2ã«ä¿å­˜ã—ã€Workersã§ã‚µãƒ¼ãƒ“ã‚¹ã—ã¾ã™ã€‚

#### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
Evidence Build
    â†“
Static Files (HTML/CSS/JS)
    â†“
R2 Bucket (github-analytics-static)
    â†“
Cloudflare Workers (ãƒ—ãƒ­ã‚­ã‚·)
    â†“ (Cache API)
Users
```

#### åˆ©ç‚¹

| é …ç›® | Workers + R2 | Workers |
|-----|-------------|-------|
| **ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º** | å®Œå…¨åˆ¶å¾¡ | åˆ¶é™ã‚ã‚Š |
| **ã‚³ã‚¹ãƒˆ** | ç„¡æ–™æ å†… | ç„¡æ–™æ å†… |
| **ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°** | Cache API | è‡ªå‹•CDN |
| **èªè¨¼** | ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚¸ãƒƒã‚¯ | Cloudflare Access |
| **SSR** | å¯èƒ½ | ä¸å¯ |

### 1. R2ãƒã‚±ãƒƒãƒˆä½œæˆ

```bash
# é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒã‚±ãƒƒãƒˆä½œæˆ
wrangler r2 bucket create github-analytics-static
```

### 2. Workerså®Ÿè£…

**`workers/evidence-host/index.ts`**:

```typescript
export interface Env {
  STATIC_BUCKET: R2Bucket;
}

export default {
  async fetch(request: Request, env: Env): Promise<Response> {
    const url = new URL(request.url);
    let pathname = url.pathname;

    // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
    if (pathname === '/' || pathname.endsWith('/')) {
      pathname = pathname + 'index.html';
    }

    // Cache API ãƒã‚§ãƒƒã‚¯
    const cache = caches.default;
    let response = await cache.match(request);

    if (!response) {
      // R2ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
      const object = await env.STATIC_BUCKET.get(pathname.slice(1));

      if (!object) {
        // 404: index.htmlã‚’è¿”ã™ (SPAå¯¾å¿œ)
        const indexObject = await env.STATIC_BUCKET.get('index.html');
        if (!indexObject) {
          return new Response('Not Found', { status: 404 });
        }

        response = new Response(indexObject.body, {
          headers: {
            'Content-Type': 'text/html',
            'Cache-Control': 'public, max-age=3600',
          },
        });
      } else {
        // Content-Typeè¨­å®š
        const contentType = getContentType(pathname);

        response = new Response(object.body, {
          headers: {
            'Content-Type': contentType,
            'Cache-Control': getCacheControl(pathname),
            'ETag': object.httpEtag,
          },
        });
      }

      // Cache API ã«ä¿å­˜
      await cache.put(request, response.clone());
    }

    return response;
  },
};

function getContentType(pathname: string): string {
  const ext = pathname.split('.').pop();
  const types: Record<string, string> = {
    html: 'text/html',
    css: 'text/css',
    js: 'application/javascript',
    json: 'application/json',
    png: 'image/png',
    jpg: 'image/jpeg',
    svg: 'image/svg+xml',
    woff: 'font/woff',
    woff2: 'font/woff2',
  };
  return types[ext || ''] || 'application/octet-stream';
}

function getCacheControl(pathname: string): string {
  // é™çš„ã‚¢ã‚»ãƒƒãƒˆ: é•·æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥
  if (pathname.includes('/_app/')) {
    return 'public, max-age=31536000, immutable';
  }
  // HTML: çŸ­æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥
  if (pathname.endsWith('.html')) {
    return 'public, max-age=3600';
  }
  // ãã®ä»–: ä¸­æœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥
  return 'public, max-age=86400';
}
```

### 3. wrangler.toml

**`workers/evidence-host/wrangler.toml`**:

```toml
name = "github-analytics"
main = "index.ts"
compatibility_date = "2025-01-01"

[[r2_buckets]]
binding = "STATIC_BUCKET"
bucket_name = "github-analytics-static"

[env.production]
route = "analytics.example.com/*"

[env.development]
route = "analytics-dev.example.com/*"
```

### 4. ãƒ“ãƒ«ãƒ‰ & ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**`scripts/deploy-evidence.sh`**:

```bash
#!/bin/bash
set -e

echo "ğŸ“¦ Building Evidence..."
cd evidence
npm run build

echo "ğŸ“¤ Uploading to R2..."
cd build

# R2ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (wranglerã¾ãŸã¯aws-cliä½¿ç”¨)
for file in $(find . -type f); do
  # ãƒ‘ã‚¹ã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã‚’å–å¾—
  key="${file#./}"
  
  # Content-Typeã‚’è¨­å®š
  case "$key" in
    *.html) content_type="text/html" ;;
    *.css) content_type="text/css" ;;
    *.js) content_type="application/javascript" ;;
    *.json) content_type="application/json" ;;
    *.png) content_type="image/png" ;;
    *.jpg|*.jpeg) content_type="image/jpeg" ;;
    *.svg) content_type="image/svg+xml" ;;
    *) content_type="application/octet-stream" ;;
  esac

  # wrangler r2 object put
  wrangler r2 object put github-analytics-static/"$key" \
    --file="$file" \
    --content-type="$content_type"
done

cd ../..

echo "ğŸš€ Deploying Worker..."
cd workers/evidence-host
wrangler deploy

echo "âœ… Deployment complete!"
echo "ğŸŒ Access: https://analytics.example.com"
```

### 5. GitHub Actions

**`.github/workflows/deploy-evidence.yml`**:

```yaml
name: Deploy Evidence to Workers

on:
  push:
    branches: [main]
    paths:
      - 'evidence/**'
      - 'workers/evidence-host/**'
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6

      - uses: actions/setup-node@v6
        with:
          node-version: '20'

      - name: Install Evidence dependencies
        working-directory: evidence
        run: npm ci

      - name: Build Evidence
        working-directory: evidence
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
        run: npm run build

      - name: Upload to R2
        working-directory: evidence/build
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        run: |
          npm install -g wrangler
          
          # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’R2ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
          find . -type f | while read file; do
            key="${file#./}"
            wrangler r2 object put github-analytics-static/"$key" --file="$file"
          done

      - name: Deploy Worker
        uses: cloudflare/wrangler-action@v3
        with:
          apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          workingDirectory: 'workers/evidence-host'
          command: deploy

      - name: Send Slack notification
        if: always()
        uses: slackapi/slack-github-action@v2
        with:
          webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}
          payload: |
            {
              "text": "${{ job.status == 'success' && 'âœ…' || 'âŒ' }} Evidence ãƒ‡ãƒ—ãƒ­ã‚¤ ${{ job.status }}",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Evidence Dashboard Deployment*\nStatus: ${{ job.status }}\nURL: https://analytics.example.com"
                  }
                }
              ]
            }
```

### 6. èªè¨¼ãƒ»ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡

#### Cloudflare Access

Workers ãƒ«ãƒ¼ãƒˆã« Cloudflare Access ã‚’è¨­å®š:

```bash
# Cloudflare ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§è¨­å®š
Access > Applications > Add an application
- Application type: Self-hosted
- Application domain: analytics.example.com
- Policy: Allow email domain @example.com
```

#### ã‚«ã‚¹ã‚¿ãƒ èªè¨¼ (Workerså†…)

```typescript
// workers/evidence-host/index.ts ã«è¿½åŠ 

async function authenticate(request: Request): Promise<boolean> {
  const authHeader = request.headers.get('Authorization');
  
  if (!authHeader || !authHeader.startsWith('Bearer ')) {
    return false;
  }

  const token = authHeader.substring(7);
  // ãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼ãƒ­ã‚¸ãƒƒã‚¯
  // ä¾‹: JWTãƒˆãƒ¼ã‚¯ãƒ³ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã€APIã‚­ãƒ¼ãªã©
  
  return true; // èªè¨¼æˆåŠŸ
}

export default {
  async fetch(request: Request, env: Env): Promise<Response> {
    // èªè¨¼ãƒã‚§ãƒƒã‚¯
    const isAuthenticated = await authenticate(request);
    
    if (!isAuthenticated) {
      return new Response('Unauthorized', {
        status: 401,
        headers: {
          'WWW-Authenticate': 'Bearer realm="GitHub Analytics"',
        },
      });
    }

    // é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ã‚µãƒ¼ãƒ“ã‚¹å‡¦ç†...
  },
};
```

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£

#### CORSè¨­å®š

```typescript
const corsHeaders = {
  'Access-Control-Allow-Origin': 'https://analytics.example.com',
  'Access-Control-Allow-Methods': 'GET, OPTIONS',
  'Access-Control-Allow-Headers': 'Content-Type, Authorization',
};

// OPTIONSãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†
if (request.method === 'OPTIONS') {
  return new Response(null, { headers: corsHeaders });
}
```

#### CSP (Content Security Policy)

```typescript
const cspHeaders = {
  'Content-Security-Policy': [
    "default-src 'self'",
    "script-src 'self' 'unsafe-inline'",
    "style-src 'self' 'unsafe-inline'",
    "img-src 'self' data: https:",
  ].join('; '),
};
```

---
---

## å‚è€ƒè³‡æ–™

### Evidence.dev

- [Evidence Documentation](https://docs.evidence.dev/)
- [Evidence GitHub](https://github.com/evidence-dev/evidence)
- [Component Library](https://docs.evidence.dev/components/)

### Cloudflare Workers

- [Workers Documentation](https://developers.cloudflare.com/pages/)
- [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/)

### DuckDB

- [DuckDB httpfs Extension](https://duckdb.org/docs/extensions/httpfs.html)

---

## å¤‰æ›´å±¥æ­´

| æ—¥ä»˜ | ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | å¤‰æ›´å†…å®¹ |
|-----|-----------|---------|
| 2025-01-03 | 1.0 | åˆç‰ˆä½œæˆ |

---

## æ‰¿èªãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼

- [ ] ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆãƒ¬ãƒ“ãƒ¥ãƒ¼
- [ ] UX/UIãƒ¬ãƒ“ãƒ¥ãƒ¼
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ“ãƒ¥ãƒ¼
- [ ] å®Ÿè£…é–‹å§‹æ‰¿èª
